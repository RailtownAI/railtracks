{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introducing RailTracks","text":"<p>RailTracks is a streamlined agentic workflow creation tool that allows users to quickly prototype, test, and  deploy agentic workflows. The foundational principles of RailTracks were designed to make the process of creating agents an exercise in writing code, not writing a configuration file.</p> <p> Quick Start Guides Examples API Reference </p>"},{"location":"#why-railtracks","title":"Why RailTracks?","text":"<p>The space of agentic AI frameworks is vast and diverse, with many frameworks offering many features it can be hard to decide which one to use.  RailTracks offers a unique approach that focuses on simplicity and developer experience. (# TODO improve this)</p> Visualizable <p>Easy to create visualizations that are perfect to show off the internals of the agent you created.</p> Code Defined Flows <p>Write your agentic workflows in Python code, no need to learn an API, basic Python skills is all it takes.</p> Modularity <p>RailTracks is modular to its core. Your tools and components should be reusable for any system.</p> Logging <p>RailTracks comes out of the box with logging that will allow you to know what is going on in your system.</p>"},{"location":"#learn-about-railtracks","title":"Learn about RailTracks","text":"Agent Building <p>Building agents can be complicated work. RailTracks streamlines the process.</p> Building Agents Multi Agent Flows <p>For the majority of applications, 1 agent is not going to get the job done. RailTracks provides a simple interface for more complicated flows</p> Defining Agentic Flows Tool Ecosystem <p>RailTracks has rich ecosystem of tools that allow you to plug them in directly into your agents.</p> Browse Tools Visualizing Your Flows <p>It is critical to understand how your agents accomplished the task. RailTracks provides a beautiful UI to interact with your flows.</p> Visualizer"},{"location":"advanced_usage/context/","title":"\ud83c\udf0d Global Context","text":"<p>RailTracks includes a concept of global context, letting you store and retrieve shared information across the lifecycle of a run. This makes it easy to coordinate data like config settings, environment flags, or shared resources.</p>"},{"location":"advanced_usage/context/#what-is-global-context","title":"\ud83e\udde0 What is Global Context?","text":"<p>The context system gives you a simple and clear API for interacting with shared values. It's scoped to the duration of a run, so everything is neatly contained.</p>"},{"location":"advanced_usage/context/#core-functions","title":"\ud83e\uddf0 Core Functions","text":"<p>You can use the context with just two main functions:</p> <ul> <li><code>rt.context.get(key, default=None)</code></li> <li><code>rt.context.put(key, value)</code></li> </ul>"},{"location":"advanced_usage/context/#quick-example","title":"\ud83d\ude80 Quick Example","text":"<p>Here\u2019s how you can use context during a run:</p> <pre><code>import railtracks as rt\n\n# Set up some context data\ndata = {\"var_1\": \"value_1\"}\n\nwith rt.Session(context=data):\n    rt.context.get(\"var_1\")  # \u27a1\ufe0f Outputs: value_1\n    rt.context.get(\"var_2\", \"default_value\")  # \u27a1\ufe0f Outputs: default_value\n\n    rt.context.put(\"var_2\", \"value_2\")  # Sets var_2 to value_2\n    rt.context.put(\"var_1\", \"new_value_1\")  # Replaces var_1 with new_value_1\n</code></pre> <p>Tip</p> <p>You can also use context inside nodes:</p> <pre><code>import railtracks as rt\n\n@rt.to_node\ndef some_node():\n    return rt.context.get(\"var_1\")\n\nwith rt.Runner(context={\"var_1\": \"value_1\"}):\n    rt.call_sync(some_node)\n</code></pre> <p>Warning</p> <p>The context only exists while the run is active. After that, it's gone.</p>"},{"location":"advanced_usage/context/#real-world-example","title":"\ud83e\uddea Real-World Example","text":"<p>Say multiple parts of your code need access to something like an <code>API_KEY</code>. You can stash it in the context and reuse it without passing it around explicitly:</p> <pre><code>import railtracks as rt\nimport os\n\napi_key = os.environ[\"API_KEY\"]\n\n\ndef api_call_1():\n    key = rt.context.get(\"api_key\")\n    # Use the key...\n\n\ndef api_call_2():\n    key = rt.context.get(\"api_key\")\n    # Use the key...\n\n\nwith rt.Session(context={\"api_key\": api_key}):\n    rt.call_sync(api_call_1)\n    rt.call_sync(api_call_2)\n</code></pre> <p>This approach reduces repetitive code and keeps sensitive info out of LLM inputs.</p> <p>Note</p> <p>You could use global variables, but the context system gives you a safer and clearer way to manage shared values. It makes your runs more predictable and your data easier to reason about. \u2705</p>"},{"location":"advanced_usage/prompt_injection/","title":"\u270f\ufe0f Prompt Injection","text":"<p>Passing prompt details up the chain can be expensive in both tokens and latency. In many cases, it\u2019s more efficient to inject values directly into a prompt using our context system.</p>"},{"location":"advanced_usage/prompt_injection/#what-is-prompt-injection","title":"\u2753 What is Prompt Injection?","text":"<p>Prompt injection refers to the practice of dynamically inserting values into a prompt template. This is especially useful when your prompt is waiting for input that isn't known ahead of time.</p> <p>For example:</p> <pre><code>\"Find the capital of {country}.\"\n</code></pre> <p>Here, the <code>{country}</code> placeholder will be replaced with a specific country name at runtime.</p> <p>Tip</p> <p>Prompt injection is helpful when you want to adapt your prompt to the current execution context or when you only have certain data available at runtime.</p> <p>\ud83d\udcda For more on how to use prompt injection effectively, check out the Prompts documentation.</p>"},{"location":"api_reference/components/api_provider_wrappers/","title":"API Provider Wrappers","text":"<p>The API Provider Wrappers component provides a unified interface for interacting with different language model providers. It ensures compatibility and error handling for model interactions, abstracting the complexities of each provider's API.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/api_provider_wrappers/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/api_provider_wrappers/#1-purpose","title":"1. Purpose","text":"<p>The primary purpose of the API Provider Wrappers is to facilitate seamless interaction with various language model providers such as Anthropic, Gemini, and OpenAI. This component abstracts the provider-specific details and provides a consistent interface for model operations.</p>"},{"location":"api_reference/components/api_provider_wrappers/#11-interacting-with-a-model","title":"1.1 Interacting with a Model","text":"<p>This use case demonstrates how to initialize a model wrapper and perform operations such as sending messages to the model.</p> <p>python from railtracks.llm.models.api_providers.openai import OpenAILLM</p>"},{"location":"api_reference/components/api_provider_wrappers/#initialize-the-openai-model-wrapper","title":"Initialize the OpenAI model wrapper","text":"<p>model = OpenAILLM(model_name=\"gpt-3.5-turbo\")</p>"},{"location":"api_reference/components/api_provider_wrappers/#interact-with-the-model","title":"Interact with the model","text":"<p>response = model.chat_with_tools(messages=[\"Hello, how are you?\"], tools=[]) print(response)</p>"},{"location":"api_reference/components/api_provider_wrappers/#12-handling-model-errors","title":"1.2 Handling Model Errors","text":"<p>This use case shows how to handle errors when interacting with a model, such as when a model is not found.</p> <p>python from railtracks.llm.models.api_providers.openai import OpenAILLM from railtracks.llm.models.api_providers._provider_wrapper import ModelNotFoundError</p> <p>try:     model = OpenAILLM(model_name=\"nonexistent-model\") except ModelNotFoundError as e:     print(f\"Error: {e}\")</p>"},{"location":"api_reference/components/api_provider_wrappers/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/api_provider_wrappers/#3-architectural-design","title":"3. Architectural Design","text":"<p>The API Provider Wrappers component is designed to abstract the complexities of interacting with different language model providers. It uses a base class, <code>ProviderLLMWrapper</code>, which defines the common interface and behavior for all provider-specific wrappers.</p>"},{"location":"api_reference/components/api_provider_wrappers/#31-providerllmwrapper","title":"3.1 ProviderLLMWrapper","text":"<ul> <li>Design Consideration: The <code>ProviderLLMWrapper</code> class inherits from <code>LiteLLMWrapper</code> and <code>ABC</code> (Abstract Base Class), ensuring that all provider-specific wrappers implement the <code>model_type</code> method.</li> <li>Logic Flow: The constructor verifies the model's provider using <code>get_llm_provider</code> and raises a <code>ModelNotFoundError</code> if the model is not recognized.</li> <li>Trade-offs: The design prioritizes a consistent interface over provider-specific optimizations, which may lead to some performance overhead.</li> </ul>"},{"location":"api_reference/components/api_provider_wrappers/#32-provider-specific-wrappers","title":"3.2 Provider-Specific Wrappers","text":"<ul> <li>AnthropicLLM: Implements the <code>model_type</code> method to return \"Anthropic\".</li> <li>GeminiLLM: Overrides <code>full_model_name</code> to accommodate the \"gemini/{model_name}\" format.</li> <li>OpenAILLM: Provides access to the OpenAI API and implements the <code>model_type</code> method to return \"OpenAI\".</li> </ul>"},{"location":"api_reference/components/api_provider_wrappers/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/api_provider_wrappers/#41-error-handling","title":"4.1 Error Handling","text":"<ul> <li>ModelNotFoundError: Raised when a model is not found. It includes helpful debugging notes.</li> <li>FunctionCallingNotSupportedError: Raised if the model does not support function calling.</li> </ul>"},{"location":"api_reference/components/api_provider_wrappers/#42-dependencies","title":"4.2 Dependencies","text":"<ul> <li>litellm: The component relies on the <code>litellm</code> library for provider logic and function calling support.</li> </ul>"},{"location":"api_reference/components/api_provider_wrappers/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/api_provider_wrappers/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>_provider_wrapper.py</code>: Contains the base class for provider wrappers.</li> <li><code>anthropic.py</code>: Implements the Anthropic provider wrapper.</li> <li><code>gemini.py</code>: Implements the Gemini provider wrapper.</li> <li><code>openai.py</code>: Implements the OpenAI provider wrapper.</li> </ul>"},{"location":"api_reference/components/api_provider_wrappers/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>model_interaction.md</code>: Details on how models interact within the system.</li> <li><code>model_error_handling.md</code>: Describes error handling strategies for model interactions.</li> </ul>"},{"location":"api_reference/components/api_provider_wrappers/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>llm_integration.md</code>: Discusses the integration of language models into the broader system.</li> </ul>"},{"location":"api_reference/components/api_provider_wrappers/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/chat_tool_call_node/","title":"Chat Tool Call Node","text":"<p>The <code>ChatToolCallLLM</code> component is designed to facilitate interactions with Language Learning Models (LLMs) using a chat interface. It supports tool calls and user inputs, enabling dynamic and interactive communication with LLMs.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/chat_tool_call_node/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/chat_tool_call_node/#1-purpose","title":"1. Purpose","text":"<p>The <code>ChatToolCallLLM</code> component is primarily used to manage and execute conversations with LLMs, allowing for both user-driven and tool-driven interactions. This component is essential for applications that require real-time, interactive dialogue with LLMs, such as virtual assistants or automated customer support systems.</p>"},{"location":"api_reference/components/chat_tool_call_node/#11-interactive-chat-with-llm","title":"1.1 Interactive Chat with LLM","text":"<p>This use case involves initiating a chat session with an LLM, where the user can input messages and receive responses from the model. The component handles the flow of messages and ensures that the conversation is coherent and contextually relevant.</p> <p>python async def start_chat():     chat_node = ChatToolCallLLM()     await chat_node.invoke()</p>"},{"location":"api_reference/components/chat_tool_call_node/#12-tool-call-execution","title":"1.2 Tool Call Execution","text":"<p>In this scenario, the component allows the LLM to execute predefined tool calls based on the conversation context. This is particularly useful for tasks that require external data fetching or processing.</p> <p>python async def execute_tool_calls():     chat_node = ChatToolCallLLM()     await chat_node.invoke()</p>"},{"location":"api_reference/components/chat_tool_call_node/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/chat_tool_call_node/#3-architectural-design","title":"3. Architectural Design","text":""},{"location":"api_reference/components/chat_tool_call_node/#31-design-considerations","title":"3.1 Design Considerations","text":"<ul> <li>Asynchronous Communication: The component is designed to handle asynchronous operations, allowing for non-blocking interactions with the LLM and tools.</li> <li>Tool Call Management: It manages the execution of tool calls, ensuring that the number of tool calls does not exceed the specified limit (<code>max_tool_calls</code>).</li> <li>Error Handling: Implements robust error handling to manage unexpected message types or tool execution failures.</li> </ul>"},{"location":"api_reference/components/chat_tool_call_node/#32-logic-flow","title":"3.2 Logic Flow","text":"<p>The component follows a structured flow: 1. User Input Handling: Waits for user input if the last message is not from the user. 2. Tool Call Execution: Executes tool calls if the LLM response includes them, ensuring the number of calls is within the allowed limit. 3. Message Handling: Processes messages from the LLM, updating the chat UI and message history accordingly.</p>"},{"location":"api_reference/components/chat_tool_call_node/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/chat_tool_call_node/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>Environment: Ensure that the environment supports asynchronous operations and has access to the necessary LLM models and tools.</li> <li>Configuration: The <code>max_tool_calls</code> parameter should be configured based on the application's requirements to prevent excessive tool usage.</li> </ul>"},{"location":"api_reference/components/chat_tool_call_node/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>Concurrency: The component is designed to handle concurrent tool calls, but the performance may vary based on the complexity and number of tools.</li> <li>Error Handling: Proper error handling is crucial to manage exceptions during tool execution and message processing.</li> </ul>"},{"location":"api_reference/components/chat_tool_call_node/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/chat_tool_call_node/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>chat_tool_call_llm.py</code>: Implements the <code>ChatToolCallLLM</code> class for managing chat interactions with LLMs.</li> </ul>"},{"location":"api_reference/components/chat_tool_call_node/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>tool_call_node_base.md</code>: Provides foundational information on tool call nodes, which <code>ChatToolCallLLM</code> extends.</li> </ul>"},{"location":"api_reference/components/chat_tool_call_node/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>node_management.md</code>: Discusses node management features, including the integration of <code>ChatToolCallLLM</code>.</li> </ul>"},{"location":"api_reference/components/chat_tool_call_node/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/chat_ui/","title":"Chat UI","text":"<p>The Chat UI component provides a simple interface for chatbot interaction with a web-based UI, supporting message sending and user input. It is designed to facilitate real-time communication between a chatbot and users through a web interface.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/chat_ui/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/chat_ui/#1-purpose","title":"1. Purpose","text":"<p>The Chat UI component is primarily used to enable real-time interaction between a chatbot and users via a web interface. It supports two main functionalities:</p>"},{"location":"api_reference/components/chat_ui/#11-sending-messages-to-the-ui","title":"1.1 Sending Messages to the UI","text":"<p>This use case involves sending messages from the chatbot to the user interface, allowing the chatbot to communicate responses or information to the user.</p> <p>python async def send_message(content: str) -&gt; None:     \"\"\"     Send an assistant message to the chat interface.</p> <pre><code>Args:\n    content: The message content to send\n\"\"\"\nmessage = {\n    \"type\": \"assistant_response\",\n    \"data\": content,\n    \"timestamp\": datetime.now().strftime(\"%H:%M:%S\"),\n}\nawait self.sse_queue.put(message)\n</code></pre>"},{"location":"api_reference/components/chat_ui/#12-waiting-for-user-input","title":"1.2 Waiting for User Input","text":"<p>This use case involves waiting for user input from the chat interface, allowing the chatbot to receive and process user messages.</p> <p>python async def wait_for_user_input(timeout: Optional[float] = None) -&gt; Optional[str]:     \"\"\"     Wait for user input from the chat interface.</p> <pre><code>Args:\n    timeout: Maximum time to wait for input (None = wait indefinitely)\n\nReturns:\n    User input string, or None if timeout/window closed\n\"\"\"\ntry:\n    if timeout:\n        user_msg = await asyncio.wait_for(\n            self.user_input_queue.get(), timeout=timeout\n        )\n    else:\n        user_msg = await self.user_input_queue.get()\n\n    return user_msg.get(\"message\") if user_msg else None\n\nexcept asyncio.TimeoutError:\n    return None\n</code></pre>"},{"location":"api_reference/components/chat_ui/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/chat_ui/#3-architectural-design","title":"3. Architectural Design","text":""},{"location":"api_reference/components/chat_ui/#31-core-design","title":"3.1 Core Design","text":"<ul> <li>FastAPI Server: The component uses FastAPI to create a server that handles HTTP requests for sending messages and receiving user input.</li> <li>Asynchronous Communication: Utilizes asynchronous queues (<code>asyncio.Queue</code>) for handling messages and user inputs, ensuring non-blocking operations.</li> <li>Static File Serving: Static files such as HTML, CSS, and JavaScript are served to render the chat interface in the browser.</li> </ul>"},{"location":"api_reference/components/chat_ui/#32-data-flow","title":"3.2 Data Flow","text":"<ul> <li>Message Flow: Messages from the chatbot are placed in an SSE (Server-Sent Events) queue and streamed to the client.</li> <li>User Input Flow: User inputs are received via HTTP POST requests and placed in a queue for the chatbot to process.</li> </ul>"},{"location":"api_reference/components/chat_ui/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/chat_ui/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>FastAPI and Uvicorn: Ensure these are installed as they are critical for running the server.</li> <li>Static Files: The component relies on static files (<code>chat.html</code>, <code>chat.css</code>, <code>chat.js</code>) located in the <code>railtracks.visuals.browser</code> package.</li> </ul>"},{"location":"api_reference/components/chat_ui/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>Concurrency: The component is designed to handle multiple simultaneous connections using asynchronous programming.</li> <li>Timeouts: Proper handling of timeouts is crucial to avoid blocking operations.</li> </ul>"},{"location":"api_reference/components/chat_ui/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/chat_ui/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>chat_ui.py</code>: Main implementation of the Chat UI component.</li> </ul>"},{"location":"api_reference/components/chat_ui/#52-related-feature-files","title":"5.2 Related Feature Files","text":"<ul> <li><code>llm_integration.md</code>: Documentation on how the Chat UI integrates with LLMs (Language Model Models).</li> </ul>"},{"location":"api_reference/components/chat_ui/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/chunking_service/","title":"Chunking Service","text":"<p>The Chunking Service is a component designed to process text by dividing it into manageable chunks. It supports various chunking strategies, including character-based and token-based chunking, and is intended to facilitate text processing tasks such as summarization and analysis.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/chunking_service/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/chunking_service/#1-purpose","title":"1. Purpose","text":"<p>The Chunking Service is primarily used to divide large text bodies into smaller, more manageable pieces. This is particularly useful in scenarios where text needs to be processed in parts, such as in natural language processing tasks or when dealing with large documents that exceed processing limits.</p>"},{"location":"api_reference/components/chunking_service/#11-character-based-chunking","title":"1.1 Character-Based Chunking","text":"<p>Character-based chunking splits text into chunks of a specified number of characters, allowing for overlap between chunks to ensure continuity.</p> <p>python chunker = TextChunkingService(chunk_size=1000, chunk_overlap=100) chunks = chunker.chunk_by_char(\"This is a long text that needs to be chunked...\")</p>"},{"location":"api_reference/components/chunking_service/#12-token-based-chunking","title":"1.2 Token-Based Chunking","text":"<p>Token-based chunking divides text into chunks based on tokens, which are typically words or subwords, using a specified model for tokenization.</p> <p>python chunker = TextChunkingService(chunk_size=100, chunk_overlap=10, model=\"gpt-3.5-turbo\") chunks = chunker.chunk_by_token(\"This is a long text that needs to be chunked...\")</p>"},{"location":"api_reference/components/chunking_service/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/chunking_service/#class-basechunkingservice","title":"<code>class BaseChunkingService</code>","text":"<p>Base class for media chunking services.</p> <p>Args:     chunk_size (int): Size of each chunk in bytes/tokens/characters.     chunk_overlap (int): Overlap between chunks.     strategy (Optional[Callable]): Callable for the chunking strategy.</p>"},{"location":"api_reference/components/chunking_service/#__init__self-chunk_size-chunk_overlap-strategy-other_configs-other_kwargs","title":"<code>.__init__(self, chunk_size, chunk_overlap, strategy, *other_configs, **other_kwargs)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/chunking_service/#chunkself-content-args-kwargs","title":"<code>.chunk(self, content, *args, **kwargs)</code>","text":"<p>Invoke the assigned chunking strategy on content.</p> <p>Args:     content: The media/text to chunk.     args, *kwargs: Passed to the chunking strategy. Returns:     Chunked content as per the strategy.</p>"},{"location":"api_reference/components/chunking_service/#set_strategyself-new_strategy","title":"<code>.set_strategy(self, new_strategy)</code>","text":"<p>Set a new chunking strategy, binding it if needed.</p>"},{"location":"api_reference/components/chunking_service/#chunk_fileself-file_path","title":"<code>.chunk_file(self, file_path)</code>","text":"<p>Split file into chunks based on strategy.</p>"},{"location":"api_reference/components/chunking_service/#class-textchunkingservicebasechunkingservice","title":"<code>class TextChunkingService(BaseChunkingService)</code>","text":"<p>Processor for text operations.</p>"},{"location":"api_reference/components/chunking_service/#__init__self-chunk_size-chunk_overlap-model-strategy-other_configs-other_kwargs","title":"<code>.__init__(self, chunk_size, chunk_overlap, model, strategy, *other_configs, **other_kwargs)</code>","text":"<p>Initialize the text chunker.</p> <p>Args:     chunk_size: Size of each chunk in characters     chunk_overlap: Overlap between chunks in characters</p>"},{"location":"api_reference/components/chunking_service/#chunk_by_charself-content","title":"<code>.chunk_by_char(self, content)</code>","text":"<p>Split text into chunks</p>"},{"location":"api_reference/components/chunking_service/#chunk_by_tokenself-content","title":"<code>.chunk_by_token(self, content)</code>","text":"<p>Split text into chunks by token.</p> <p>TODO: use LLM to do this</p>"},{"location":"api_reference/components/chunking_service/#chunk_smartself-content","title":"<code>.chunk_smart(self, content)</code>","text":"<p>Smart chunking using LLM to determine optimal chunk size.</p> <p>Args:     content: Text content to be chunked     model: Model to use for smart chunking</p> <p>Returns:     List of text chunks</p>"},{"location":"api_reference/components/chunking_service/#3-architectural-design","title":"3. Architectural Design","text":""},{"location":"api_reference/components/chunking_service/#31-design-principles","title":"3.1 Design Principles","text":"<ul> <li>Modularity: The service is designed to be modular, allowing different chunking strategies to be implemented and swapped easily.</li> <li>Extensibility: New chunking strategies can be added by subclassing <code>BaseChunkingService</code> and implementing the <code>chunk_file</code> method.</li> <li>Flexibility: The service supports both character-based and token-based chunking, with the potential for more sophisticated strategies like smart chunking.</li> </ul>"},{"location":"api_reference/components/chunking_service/#32-data-flow","title":"3.2 Data Flow","text":"<ul> <li>Input: Text content is provided to the service, either as a string or a file path.</li> <li>Processing: The content is processed using the specified chunking strategy, which divides it into chunks.</li> <li>Output: The resulting chunks are returned as a list of strings.</li> </ul>"},{"location":"api_reference/components/chunking_service/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/chunking_service/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>Tokenizer: The <code>TextChunkingService</code> relies on a <code>Tokenizer</code> class for token-based chunking. Ensure that the appropriate model is specified and available.</li> <li>Logging: The service uses Python's logging module to report errors and warnings.</li> </ul>"},{"location":"api_reference/components/chunking_service/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>Chunk Overlap: Ensure that <code>chunk_overlap</code> is less than or equal to <code>chunk_size</code> to avoid errors.</li> <li>Model Dependency: Token-based chunking requires a specified model for tokenization. If no model is provided, an error will be raised.</li> </ul>"},{"location":"api_reference/components/chunking_service/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/chunking_service/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>chunking_service.py</code>: Contains the implementation of the Chunking Service.</li> </ul>"},{"location":"api_reference/components/chunking_service/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li>Tokenizer Documentation: Provides details on the <code>Tokenizer</code> class used for token-based chunking.</li> </ul>"},{"location":"api_reference/components/chunking_service/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (2023-10-01) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/cli_entry_point/","title":"CLI Entry Point","text":"<p>The CLI Entry Point serves as the entry point for the railtracks command-line interface (CLI) when executed as a module. It is a crucial component that initializes the CLI application, allowing users to interact with the railtracks system through command-line commands.</p> <p>Version: 0.0.1</p> <p>Component Contact: @github_username</p>"},{"location":"api_reference/components/cli_entry_point/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/cli_entry_point/#1-purpose","title":"1. Purpose","text":"<p>The primary purpose of the CLI Entry Point is to serve as the main execution point for the railtracks CLI. It allows users to run the CLI by executing the module directly, which in turn calls the main function from the <code>railtracks_cli</code> package.</p>"},{"location":"api_reference/components/cli_entry_point/#11-executing-the-cli","title":"1.1 Executing the CLI","text":"<p>The CLI Entry Point is executed when the module is run directly. This is typically done using the command:</p> <p>bash python -m railtracks_cli</p> <p>This command triggers the <code>__main__.py</code> file, which imports and calls the <code>main</code> function from the <code>railtracks_cli</code> package.</p>"},{"location":"api_reference/components/cli_entry_point/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/cli_entry_point/#3-architectural-design","title":"3. Architectural Design","text":"<p>The design of the CLI Entry Point is straightforward, focusing on simplicity and ease of use. The primary responsibility of this component is to act as a bridge between the command-line execution and the CLI logic defined in the <code>railtracks_cli</code> package.</p>"},{"location":"api_reference/components/cli_entry_point/#31-design-considerations","title":"3.1 Design Considerations","text":"<ul> <li>Simplicity: The entry point is designed to be minimal, with the sole purpose of invoking the <code>main</code> function.</li> <li>Modularity: By delegating the CLI logic to the <code>railtracks_cli</code> package, the entry point remains clean and focused on its primary task.</li> <li>Ease of Use: Users can easily execute the CLI by running the module, without needing to know the internal structure of the package.</li> </ul>"},{"location":"api_reference/components/cli_entry_point/#4-important-considerations","title":"4. Important Considerations","text":"<ul> <li>Dependencies: The CLI Entry Point depends on the <code>railtracks_cli</code> package, specifically the <code>main</code> function. Ensure that this package is correctly installed and accessible in the Python environment.</li> <li>Execution Context: The entry point should be executed in an environment where the <code>railtracks_cli</code> package is available. This is typically the case when the package is installed in the Python environment.</li> </ul>"},{"location":"api_reference/components/cli_entry_point/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/cli_entry_point/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>__main__.py</code>: The entry point script for the railtracks CLI.</li> </ul>"},{"location":"api_reference/components/cli_entry_point/#52-related-feature-files","title":"5.2 Related Feature Files","text":"<ul> <li><code>cli_interface.md</code>: Documentation for the CLI interface, detailing the commands and options available to users.</li> </ul>"},{"location":"api_reference/components/cli_entry_point/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/cloud_model_wrappers/","title":"Azure AI LLM Wrapper","text":"<p>The Azure AI LLM Wrapper is a component designed to interface with Azure's cloud-based language models, providing a seamless integration for leveraging Azure AI's capabilities within the larger project.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/cloud_model_wrappers/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/cloud_model_wrappers/#1-purpose","title":"1. Purpose","text":"<p>The Azure AI LLM Wrapper is primarily used to facilitate interactions with Azure's language models. It abstracts the complexities involved in communicating with Azure AI, allowing developers to focus on building applications without worrying about the underlying API intricacies.</p>"},{"location":"api_reference/components/cloud_model_wrappers/#11-chat-with-azure-ai","title":"1.1 Chat with Azure AI","text":"<p>This use case involves sending a series of messages to an Azure AI model and receiving a response. It is crucial for applications that require natural language processing capabilities.</p> <p>python from azureai import AzureAILLM</p>"},{"location":"api_reference/components/cloud_model_wrappers/#initialize-the-azure-ai-llm-with-a-specific-model","title":"Initialize the Azure AI LLM with a specific model","text":"<p>azure_ai = AzureAILLM(model_name=\"text-davinci-003\")</p>"},{"location":"api_reference/components/cloud_model_wrappers/#send-a-chat-message","title":"Send a chat message","text":"<p>response = azure_ai.chat(messages=[\"Hello, how can I assist you today?\"]) print(response)</p>"},{"location":"api_reference/components/cloud_model_wrappers/#12-chat-with-tools","title":"1.2 Chat with Tools","text":"<p>This use case extends the basic chat functionality by allowing the integration of external tools, enhancing the model's capabilities to perform specific tasks.</p> <p>python from azureai import AzureAILLM</p>"},{"location":"api_reference/components/cloud_model_wrappers/#initialize-the-azure-ai-llm-with-a-specific-model_1","title":"Initialize the Azure AI LLM with a specific model","text":"<p>azure_ai = AzureAILLM(model_name=\"text-davinci-003\")</p>"},{"location":"api_reference/components/cloud_model_wrappers/#define-tools-to-be-used","title":"Define tools to be used","text":"<p>tools = [\"calculator\", \"translator\"]</p>"},{"location":"api_reference/components/cloud_model_wrappers/#send-a-chat-message-with-tools","title":"Send a chat message with tools","text":"<p>response = azure_ai.chat_with_tools(messages=[\"Translate 'Hello' to French.\"], tools=tools) print(response)</p>"},{"location":"api_reference/components/cloud_model_wrappers/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/cloud_model_wrappers/#class-azureaillmlitellmwrapper","title":"<code>class AzureAILLM(LiteLLMWrapper)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/cloud_model_wrappers/#model_typecls","title":"<code>.model_type(cls)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/cloud_model_wrappers/#__init__self-model_name-kwargs","title":"<code>.__init__(self, model_name, **kwargs)</code>","text":"<p>Initialize an Azure AI LLM instance.</p> <p>Args:     model_name (str): Name of the Azure AI model to use.     **kwargs: Additional arguments passed to the parent LiteLLMWrapper.</p> <p>Raises:     AzureAIError: If the specified model is not available or if there are issues with the Azure AI service.</p>"},{"location":"api_reference/components/cloud_model_wrappers/#chatself-messages-kwargs","title":"<code>.chat(self, messages, **kwargs)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/cloud_model_wrappers/#chat_with_toolsself-messages-tools-kwargs","title":"<code>.chat_with_tools(self, messages, tools, **kwargs)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/cloud_model_wrappers/#3-architectural-design","title":"3. Architectural Design","text":""},{"location":"api_reference/components/cloud_model_wrappers/#31-azureaillm-class","title":"3.1 AzureAILLM Class","text":"<ul> <li>Inheritance from LiteLLMWrapper:</li> <li> <p>The <code>AzureAILLM</code> class inherits from <code>LiteLLMWrapper</code>, which provides a base implementation for language model interactions. This design choice allows for code reuse and consistency across different model wrappers.</p> </li> <li> <p>Error Handling:</p> </li> <li> <p>Custom exceptions like <code>AzureAIError</code> and <code>FunctionCallingNotSupportedError</code> are used to handle specific error scenarios, ensuring that issues are communicated clearly to the developer.</p> </li> <li> <p>Model Availability:</p> </li> <li> <p>The <code>_is_model_available</code> method checks if the specified model is available in Azure AI, preventing runtime errors due to unavailable models.</p> </li> <li> <p>Tool Calling Support:</p> </li> <li>The <code>_tool_calling_supported</code> method determines if the model supports tool calling, which is essential for integrating external functionalities.</li> </ul>"},{"location":"api_reference/components/cloud_model_wrappers/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/cloud_model_wrappers/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>Logging:</li> <li> <p>The component uses a custom logger obtained via <code>get_rt_logger(LOGGER_NAME)</code>. Ensure that the logging configuration is set up correctly to capture logs from this component.</p> </li> <li> <p>Model Case Sensitivity:</p> </li> <li>Model names are case-sensitive when matching against available Azure models. Ensure that the correct casing is used to avoid errors.</li> </ul>"},{"location":"api_reference/components/cloud_model_wrappers/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>Error Handling:</li> <li>The component raises <code>AzureAIError</code> for internal server errors from Azure AI, which should be handled gracefully in the application to ensure a smooth user experience.</li> </ul>"},{"location":"api_reference/components/cloud_model_wrappers/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/cloud_model_wrappers/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>../_litellm_wrapper.py</code>: Provides the base class <code>LiteLLMWrapper</code> that <code>AzureAILLM</code> extends.</li> <li><code>../_model_exception_base.py</code>: Contains the base exception classes used for error handling in the Azure AI wrapper.</li> </ul>"},{"location":"api_reference/components/cloud_model_wrappers/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>../api_provider_wrappers.md</code>: Documents other API provider wrappers that integrate with different cloud services.</li> </ul>"},{"location":"api_reference/components/cloud_model_wrappers/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>../llm_integration.md</code>: Describes the integration of language models within the project, including Azure AI.</li> </ul>"},{"location":"api_reference/components/cloud_model_wrappers/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/context_management/","title":"Context Management","text":"<p>The Context Management component is responsible for managing context variables and configurations within a thread or execution environment, providing utilities for context-specific data handling.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/context_management/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/context_management/#1-purpose","title":"1. Purpose","text":"<p>The Context Management component is designed to handle context variables and configurations within a thread or execution environment. It provides utilities for managing context-specific data, ensuring that each thread or execution environment can maintain its own set of context variables without interference from others.</p>"},{"location":"api_reference/components/context_management/#11-context-variable-management","title":"1.1 Context Variable Management","text":"<p>This use case involves managing context variables that are scoped within the context of a single runner. It is crucial for maintaining thread-specific data integrity.</p> <p>python from railtracks.context.central import register_globals, get_runner_id</p>"},{"location":"api_reference/components/context_management/#register-global-variables-for-the-current-thread","title":"Register global variables for the current thread","text":"<p>register_globals(     runner_id=\"runner_123\",     rt_publisher=None,     parent_id=None,     executor_config=ExecutorConfig(),     global_context_vars={} )</p>"},{"location":"api_reference/components/context_management/#retrieve-the-runner-id-for-the-current-thread","title":"Retrieve the runner ID for the current thread","text":"<p>runner_id = get_runner_id() print(runner_id)  # Output: runner_123</p>"},{"location":"api_reference/components/context_management/#12-executor-configuration-management","title":"1.2 Executor Configuration Management","text":"<p>This use case demonstrates how to manage executor configurations within the context, allowing for customization of execution behavior.</p> <p>python from railtracks.context.central import set_global_config, get_global_config</p>"},{"location":"api_reference/components/context_management/#set-a-new-global-executor-configuration","title":"Set a new global executor configuration","text":"<p>set_global_config(ExecutorConfig(timeout=200.0, end_on_error=True))</p>"},{"location":"api_reference/components/context_management/#retrieve-the-current-global-executor-configuration","title":"Retrieve the current global executor configuration","text":"<p>config = get_global_config() print(config.timeout)  # Output: 200.0</p>"},{"location":"api_reference/components/context_management/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/context_management/#def-safe_get_runner_context","title":"<code>def safe_get_runner_context()</code>","text":"<p>Safely get the runner context for the current thread.</p> <pre><code>Returns:\n    RunnerContextVars: The runner context associated with the current thread.\n\nRaises:\n    RuntimeError: If the global variables have not been registered.\n</code></pre>"},{"location":"api_reference/components/context_management/#def-is_context_present","title":"<code>def is_context_present()</code>","text":"<p>Returns true if a context exists.</p>"},{"location":"api_reference/components/context_management/#def-is_context_active","title":"<code>def is_context_active()</code>","text":"<p>Check if the global variables for the current thread are active.</p> <p>Returns:     bool: True if the global variables are active, False otherwise.</p>"},{"location":"api_reference/components/context_management/#def-get_publisher","title":"<code>def get_publisher()</code>","text":"<p>Get the publisher for the current thread's global variables.</p> <p>Returns:     RTPublisher: The publisher associated with the current thread's global variables.</p> <p>Raises:     RuntimeError: If the global variables have not been registered.</p>"},{"location":"api_reference/components/context_management/#def-get_runner_id","title":"<code>def get_runner_id()</code>","text":"<p>Get the runner ID of the current thread's global variables.</p> <p>Returns:     str: The runner ID associated with the current thread's global variables.</p> <p>Raises:     RuntimeError: If the global variables have not been registered.</p>"},{"location":"api_reference/components/context_management/#def-get_parent_id","title":"<code>def get_parent_id()</code>","text":"<p>Get the parent ID of the current thread's global variables.</p> <p>Returns:     str | None: The parent ID associated with the current thread's global variables, or None if not set.</p> <p>Raises:     RuntimeError: If the global variables have not been registered.</p>"},{"location":"api_reference/components/context_management/#def-register_globals","title":"<code>def register_globals()</code>","text":"<p>Register the global variables for the current thread.</p>"},{"location":"api_reference/components/context_management/#def-activate_publisher","title":"<code>def activate_publisher()</code>","text":"<p>Activate the publisher for the current thread's global variables.</p> <p>This function should be called to ensure that the publisher is running and can be used to publish messages.</p>"},{"location":"api_reference/components/context_management/#def-shutdown_publisher","title":"<code>def shutdown_publisher()</code>","text":"<p>Shutdown the publisher for the current thread's global variables.</p> <p>This function should be called to stop the publisher and clean up resources.</p>"},{"location":"api_reference/components/context_management/#def-get_global_config","title":"<code>def get_global_config()</code>","text":"<p>Get the executor configuration for the current thread's global variables.</p> <p>Returns:     ExecutorConfig: The executor configuration associated with the current thread's global variables, or None if not set.</p>"},{"location":"api_reference/components/context_management/#def-set_local_configexecutor_config","title":"<code>def set_local_config(executor_config)</code>","text":"<p>Set the executor configuration for the current thread's global variables.</p> <p>Args:     executor_config (ExecutorConfig): The executor configuration to set.</p>"},{"location":"api_reference/components/context_management/#def-set_global_configexecutor_config","title":"<code>def set_global_config(executor_config)</code>","text":"<p>Set the executor configuration for the current thread's global variables.</p> <p>Args:     executor_config (ExecutorConfig): The executor configuration to set.</p>"},{"location":"api_reference/components/context_management/#def-update_parent_idnew_parent_id","title":"<code>def update_parent_id(new_parent_id)</code>","text":"<p>Update the parent ID of the current thread's global variables.</p>"},{"location":"api_reference/components/context_management/#def-delete_globals","title":"<code>def delete_globals()</code>","text":"<p>Resets the globals to None.</p>"},{"location":"api_reference/components/context_management/#def-getdefault","title":"<code>def get(default)</code>","text":"<p>Get a value from context</p> <p>Args:     key (str): The key to retrieve.     default (Any | None): The default value to return if the key does not exist. If set to None and the key does not exist, a KeyError will be raised. Returns:     Any: The value associated with the key, or the default value if the key does not exist.</p> <p>Raises:     KeyError: If the key does not exist and no default value is provided.</p>"},{"location":"api_reference/components/context_management/#def-putkey-value","title":"<code>def put(key, value)</code>","text":"<p>Set a value in the context.</p> <p>Args:     key (str): The key to set.     value (Any): The value to set.</p>"},{"location":"api_reference/components/context_management/#def-updatedata","title":"<code>def update(data)</code>","text":"<p>Sets the values in the context. If the context already has values, this will overwrite them, but it will not delete any existing keys.</p> <p>Args:     data (dict[str, Any]): The data to update the context with.</p>"},{"location":"api_reference/components/context_management/#def-deletekey","title":"<code>def delete(key)</code>","text":"<p>Delete a key from the context.</p> <p>Args:     key (str): The key to delete.</p> <p>Raises:     KeyError: If the key does not exist.</p>"},{"location":"api_reference/components/context_management/#def-set_config","title":"<code>def set_config()</code>","text":"<p>Sets the global configuration for the executor. This will be propagated to all new runners created after this call.</p> <ul> <li>If you call this function after the runner has been created, it will not affect the current runner.</li> <li>This function will only overwrite the values that are provided, leaving the rest unchanged.</li> </ul>"},{"location":"api_reference/components/context_management/#3-architectural-design","title":"3. Architectural Design","text":""},{"location":"api_reference/components/context_management/#31-context-management-design","title":"3.1 Context Management Design","text":"<ul> <li>Core Philosophy &amp; Design Principles: The component is designed around the principle of thread-local storage, ensuring that each thread or execution environment can maintain its own set of context variables.</li> <li>High-Level Architecture &amp; Data Flow: The component uses Python's <code>contextvars</code> to manage context variables. The <code>RunnerContextVars</code> class encapsulates the context for a single runner, including internal and external contexts.</li> <li>Key Design Decisions &amp; Trade-offs: The use of <code>contextvars</code> allows for efficient context management without the overhead of manual context passing. However, it requires careful management of context activation and deactivation.</li> <li>Component Boundaries &amp; Responsibilities: This component is responsible for managing context variables and executor configurations. It is not responsible for the actual execution of tasks or handling of messages.</li> <li>Rejected Alternatives: Using global variables was considered but rejected due to potential conflicts in multi-threaded environments.</li> </ul>"},{"location":"api_reference/components/context_management/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/context_management/#41-context-activation-and-deactivation","title":"4.1 Context Activation and Deactivation","text":"<ul> <li>Dependencies &amp; Setup: The component relies on <code>contextvars</code> for managing context variables. Ensure that the <code>contextvars</code> module is available in your Python environment.</li> <li>Performance &amp; Limitations: The component is designed for use in multi-threaded environments. It may not perform optimally in single-threaded applications.</li> <li>State Management &amp; Concurrency: The component is thread-safe, but care must be taken to activate and deactivate contexts correctly to avoid data leakage between threads.</li> <li>Security Considerations: Ensure that context variables do not contain sensitive information that could be exposed through logging or debugging.</li> <li>Configuration &amp; Feature Flags: The component supports configuration through the <code>ExecutorConfig</code> class, allowing for customization of execution behavior.</li> </ul>"},{"location":"api_reference/components/context_management/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/context_management/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>central.py</code>: Contains the main logic for managing context variables and configurations.</li> <li><code>external.py</code>: Defines the <code>ExternalContext</code> and <code>MutableExternalContext</code> classes for managing external context variables.</li> <li><code>internal.py</code>: Defines the <code>InternalContext</code> class for managing internal context variables.</li> </ul>"},{"location":"api_reference/components/context_management/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>../utils/config.py</code>: Contains the <code>ExecutorConfig</code> class used for managing executor configurations.</li> </ul>"},{"location":"api_reference/components/context_management/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>../features/context_management.md</code>: Documentation for the Context Management feature.</li> </ul>"},{"location":"api_reference/components/context_management/#54-external-dependencies","title":"5.4 External Dependencies","text":"<ul> <li><code>https://docs.python.org/3/library/contextvars.html</code>: Python's <code>contextvars</code> module documentation.</li> </ul>"},{"location":"api_reference/components/context_management/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/embedding_service/","title":"Embedding Service","text":"<p>The Embedding Service is designed to generate embeddings for text data using the <code>litellm</code> library. It provides a convenient interface for embedding tasks, primarily focusing on text processing within the larger project.</p> <p>Version: 0.0.1</p> <p>Component Contact: @github_username</p>"},{"location":"api_reference/components/embedding_service/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/embedding_service/#1-purpose","title":"1. Purpose","text":"<p>The Embedding Service is used to convert text data into numerical embeddings, which are essential for various natural language processing tasks such as similarity search, clustering, and classification.</p>"},{"location":"api_reference/components/embedding_service/#11-text-embedding","title":"1.1 Text Embedding","text":"<p>The primary use case of the Embedding Service is to generate embeddings for a batch of text inputs. This is crucial for applications that require text data to be represented in a numerical format for further processing or analysis.</p> <p>python from railtracks.rag.embedding_service import EmbeddingService</p>"},{"location":"api_reference/components/embedding_service/#initialize-the-embedding-service","title":"Initialize the embedding service","text":"<p>embedding_service = EmbeddingService()</p>"},{"location":"api_reference/components/embedding_service/#example-texts-to-embed","title":"Example texts to embed","text":"<p>texts = [\"Hello world\", \"Embedding service example\"]</p>"},{"location":"api_reference/components/embedding_service/#generate-embeddings","title":"Generate embeddings","text":"<p>embeddings = embedding_service.embed(texts) print(embeddings)</p>"},{"location":"api_reference/components/embedding_service/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/embedding_service/#class-embeddingservicebaseembeddingservice","title":"<code>class EmbeddingService(BaseEmbeddingService)</code>","text":"<p>Embedding service that uses litellm to perform embedding tasks.</p>"},{"location":"api_reference/components/embedding_service/#__init__self-model-litellm_extra","title":"<code>.__init__(self, model, **litellm_extra)</code>","text":"<p>Initialize the embedding service.</p> <p>Args:     model: Model name (OpenAI, TogetherAI, etc.)     api_key: If None, taken from OPENAI_API_KEY env var.     base_url: Override OpenAI base URL if using gateway / proxy.     timeout: Per-request timeout in seconds.     **litellm_extra: Any other args passed straight to litellm.embedding                     (e.g. headers, organization, etc.)</p>"},{"location":"api_reference/components/embedding_service/#embedself-texts","title":"<code>.embed(self, texts)</code>","text":"<p>Convenience wrapper to embed many short texts in one go.</p>"},{"location":"api_reference/components/embedding_service/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Embedding Service is built around the <code>litellm</code> library, which provides the core functionality for generating embeddings. The service is designed to be flexible and configurable, allowing users to specify different models and parameters.</p>"},{"location":"api_reference/components/embedding_service/#31-design-considerations","title":"3.1 Design Considerations","text":"<ul> <li>Model Selection: The service uses a default model (<code>text-embedding-3-small</code>) but allows customization through the constructor.</li> <li>Batch Processing: Texts are processed in batches to optimize performance and manage resource usage effectively.</li> <li>Extensibility: The service is designed to be extended for different embedding models and configurations.</li> </ul>"},{"location":"api_reference/components/embedding_service/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/embedding_service/#41-configuration-and-dependencies","title":"4.1 Configuration and Dependencies","text":"<ul> <li>API Key: The service requires an API key for authentication, which can be provided directly or through the <code>OPENAI_API_KEY</code> environment variable.</li> <li>Timeouts: The service includes a configurable timeout for requests to prevent long-running operations from blocking the system.</li> </ul>"},{"location":"api_reference/components/embedding_service/#42-performance","title":"4.2 Performance","text":"<ul> <li>Batch Size: The batch size can be adjusted to balance between performance and resource consumption. Larger batch sizes may improve throughput but require more memory.</li> </ul>"},{"location":"api_reference/components/embedding_service/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/embedding_service/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>embedding_service.py</code>: Contains the implementation of the Embedding Service.</li> </ul>"},{"location":"api_reference/components/embedding_service/#52-related-feature-files","title":"5.2 Related Feature Files","text":"<ul> <li><code>rag_system.md</code>: Provides an overview of the RAG system, which utilizes the Embedding Service for text processing tasks.</li> </ul>"},{"location":"api_reference/components/embedding_service/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/exception_handling/","title":"Exception Handling","text":"<p>The Exception Handling component is designed to define custom exceptions and error handling mechanisms for runtime errors, node execution, and context issues within the Railtracks project.</p> <p>Version: 0.0.1</p> <p>Component Contact: @github_username</p>"},{"location":"api_reference/components/exception_handling/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/exception_handling/#1-purpose","title":"1. Purpose","text":"<p>This component provides a structured way to handle exceptions that occur during the execution of nodes, context management, and other runtime operations. It ensures that errors are communicated clearly and consistently, with helpful debugging information when available.</p>"},{"location":"api_reference/components/exception_handling/#11-node-execution-errors","title":"1.1 Node Execution Errors","text":"<p>Node execution errors are critical as they can halt the entire process flow. This component provides specific exceptions like <code>NodeInvocationError</code> and <code>NodeCreationError</code> to handle such scenarios.</p> <p>python try:     # Node execution logic except NodeInvocationError as e:     print(e)</p>"},{"location":"api_reference/components/exception_handling/#12-context-management-errors","title":"1.2 Context Management Errors","text":"<p>Errors in context management can lead to incorrect data processing. The <code>ContextError</code> class is used to handle such issues.</p> <p>python try:     # Context management logic except ContextError as e:     print(e)</p>"},{"location":"api_reference/components/exception_handling/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/exception_handling/#class-nodeinvocationerrorrterror","title":"<code>class NodeInvocationError(RTError)</code>","text":"<p>Raised during node for execution problems in graph, including node or orchestration failures. For example, bad config, missing required parameters, or structural errors.</p>"},{"location":"api_reference/components/exception_handling/#__init__self-message-notes-fatal","title":"<code>.__init__(self, message, notes, fatal)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/exception_handling/#class-nodecreationerrorrterror","title":"<code>class NodeCreationError(RTError)</code>","text":"<p>Raised during node creation/validation before any execution begins. For example, bad config, missing required parameters, or structural errors.</p>"},{"location":"api_reference/components/exception_handling/#__init__self-message-notes","title":"<code>.__init__(self, message, notes)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/exception_handling/#class-llmerrorrterror","title":"<code>class LLMError(RTError)</code>","text":"<p>Raised when an error occurs during LLM invocation or completion.</p>"},{"location":"api_reference/components/exception_handling/#__init__self-reason-message_history","title":"<code>.__init__(self, reason, message_history)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/exception_handling/#class-globaltimeouterrorrterror","title":"<code>class GlobalTimeOutError(RTError)</code>","text":"<p>Raised on global timeout for whole execution.</p>"},{"location":"api_reference/components/exception_handling/#__init__self-timeout","title":"<code>.__init__(self, timeout)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/exception_handling/#class-contexterrorrterror","title":"<code>class ContextError(RTError)</code>","text":"<p>Raised when there is an error with the context.</p>"},{"location":"api_reference/components/exception_handling/#__init__self-message-notes_1","title":"<code>.__init__(self, message, notes)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/exception_handling/#class-fatalerrorrterror","title":"<code>class FatalError(RTError)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/exception_handling/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Exception Handling component is designed to provide a robust framework for managing errors across the Railtracks project. It leverages a base class, <code>RTError</code>, to ensure consistency in error reporting and handling.</p>"},{"location":"api_reference/components/exception_handling/#31-core-philosophy-design-principles","title":"3.1 Core Philosophy &amp; Design Principles","text":"<ul> <li>Consistency: All exceptions inherit from <code>RTError</code>, ensuring a uniform interface for error handling.</li> <li>Clarity: Error messages are color-coded for better readability in terminal outputs.</li> <li>Extensibility: New exceptions can be easily added by extending <code>RTError</code>.</li> </ul>"},{"location":"api_reference/components/exception_handling/#32-high-level-architecture-data-flow","title":"3.2 High-Level Architecture &amp; Data Flow","text":"<p>The component is structured into base classes and specific error classes. The base class, <code>RTError</code>, provides common functionality like color-coded messages. Specific error classes like <code>NodeInvocationError</code> and <code>LLMError</code> extend this base class to provide detailed error handling for different scenarios.</p>"},{"location":"api_reference/components/exception_handling/#33-key-design-decisions-trade-offs","title":"3.3 Key Design Decisions &amp; Trade-offs","text":"<ul> <li>Color-Coding: Chosen for better visibility in terminal outputs, which may not be suitable for non-terminal environments.</li> <li>Message Externalization: Error messages and notes are externalized in a YAML file for easy updates and localization.</li> </ul>"},{"location":"api_reference/components/exception_handling/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/exception_handling/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>YAML Configuration: The component relies on a YAML file (<code>exception_messages.yaml</code>) for storing error messages and notes. Ensure this file is accessible and correctly formatted.</li> </ul>"},{"location":"api_reference/components/exception_handling/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>Terminal Output: The color-coding feature is designed for terminal outputs and may not render correctly in other environments.</li> </ul>"},{"location":"api_reference/components/exception_handling/#43-debugging-observability","title":"4.3 Debugging &amp; Observability","text":"<ul> <li>Error Messages: Use the <code>get_message</code> and <code>get_notes</code> functions to retrieve detailed error messages and debugging notes.</li> </ul>"},{"location":"api_reference/components/exception_handling/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/exception_handling/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>_base.py</code>: Defines the base class <code>RTError</code> for all custom exceptions.</li> <li><code>errors.py</code>: Contains specific exception classes for node execution and context errors.</li> <li><code>exception_messages.py</code>: Manages the retrieval of error messages and notes.</li> <li><code>exception_messages.yaml</code>: Stores error messages and notes in a structured format.</li> </ul>"},{"location":"api_reference/components/exception_handling/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/execution_info/","title":"Execution Info","text":"<p>The <code>ExecutionInfo</code> component captures the state of a system run at any given point, providing a snapshot for display or graphical representation. It is designed to be used as a snapshot of state that can be used to display the state of the run or to create a graphical representation of the system.</p> <p>Version: 0.0.1</p> <p>Component Contact: @github_username</p>"},{"location":"api_reference/components/execution_info/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/execution_info/#1-purpose","title":"1. Purpose","text":"<p>The <code>ExecutionInfo</code> component is primarily used to capture and represent the state of a system's execution at any given point in time. This is crucial for debugging, monitoring, and visualizing the flow of data and control within the system.</p>"},{"location":"api_reference/components/execution_info/#11-capturing-execution-state","title":"1.1 Capturing Execution State","text":"<p>The <code>ExecutionInfo</code> class provides a comprehensive snapshot of the system's state, including nodes, requests, and timestamps.</p> <p>python from railtracks.state.info import ExecutionInfo</p>"},{"location":"api_reference/components/execution_info/#create-a-new-execution-info-instance","title":"Create a new execution info instance","text":"<p>execution_info = ExecutionInfo.create_new()</p>"},{"location":"api_reference/components/execution_info/#access-the-answer-of-the-run","title":"Access the answer of the run","text":"<p>answer = execution_info.answer</p>"},{"location":"api_reference/components/execution_info/#get-all-stamps","title":"Get all stamps","text":"<p>stamps = execution_info.all_stamps</p>"},{"location":"api_reference/components/execution_info/#12-graphical-representation","title":"1.2 Graphical Representation","text":"<p>The component can convert the current state into a graph representation, which can be serialized into JSON for visualization purposes.</p> <p>python</p>"},{"location":"api_reference/components/execution_info/#convert-to-graph-representation","title":"Convert to graph representation","text":"<p>vertices, edges = execution_info._to_graph()</p>"},{"location":"api_reference/components/execution_info/#serialize-to-json","title":"Serialize to JSON","text":"<p>graph_json = execution_info.graph_serialization()</p>"},{"location":"api_reference/components/execution_info/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/execution_info/#class-executioninfo","title":"<code>class ExecutionInfo</code>","text":"<p>A class that contains the full details of the state of a run at any given point in time.</p> <p>The class is designed to be used as a snapshot of state that can be used to display the state of the run, or to create a graphical representation of the system.</p>"},{"location":"api_reference/components/execution_info/#__init__self-request_forest-node_forest-stamper","title":"<code>.__init__(self, request_forest, node_forest, stamper)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/execution_info/#defaultcls","title":"<code>.default(cls)</code>","text":"<p>Creates a new \"empty\" instance of the ExecutionInfo class with the default values.</p>"},{"location":"api_reference/components/execution_info/#create_newcls","title":"<code>.create_new(cls)</code>","text":"<p>Creates a new empty instance of state variables with the provided executor configuration.</p>"},{"location":"api_reference/components/execution_info/#answerself","title":"<code>.answer(self)</code>","text":"<p>Convenience method to access the answer of the run.</p>"},{"location":"api_reference/components/execution_info/#all_stampsself","title":"<code>.all_stamps(self)</code>","text":"<p>Convenience method to access all the stamps of the run.</p>"},{"location":"api_reference/components/execution_info/#insertion_requestsself","title":"<code>.insertion_requests(self)</code>","text":"<p>A convenience method to access all the insertion requests of the run.</p>"},{"location":"api_reference/components/execution_info/#graph_serializationself","title":"<code>.graph_serialization(self)</code>","text":"<pre><code>    Creates a string (JSON) representation of this info object designed to be used to construct a graph for this\n    info object.\n\n    Some important notes about its structure are outlined below:\n    - The `nodes` key contains a list of all the nodes in the graph, represented as `Vertex` objects.\n    - The `edges` key contains a list of all the edges in the graph, represented as `Edge` objects.\n    - The `stamps` key contains an ease of use list of all the stamps associated with the run, represented as `Stamp` objects.\n\n    - The \"nodes\" and \"requests\" key will be outlined with normal graph details like connections and identifiers in addition to a loose details object.\n    - However, both will carry an addition param called \"stamp\" which is a timestamp style object.\n    - They also will carry a \"parent\" param which is a recursive structure that allows you to traverse the graph in time.\n\n    The current output_schema looks something like the following.\n    json\n</code></pre> <p>{   \"nodes\": [     {       \"identifier\": str,       \"node_type\": str,       \"stamp\": {          \"step\": int,          \"time\": float,          \"identifier\": str       }       \"details\": {          \"internals\": {             \"latency\": float,                    }       \"parent\":    ]   \"edges\": [     {       \"source\": str | null,       \"target\": str,       \"indentifier\": str,       \"stamp\": {         \"step\": int,         \"time\": float,         \"identifier\": str       }       \"details\": {          \"input_args\": [],          \"input_kwargs\": {},          \"output\": Any       }       \"parent\":      }   ],   \"stamps\": [     {        \"step\": int,        \"time\": float,        \"identifier: str     }   ] }"},{"location":"api_reference/components/execution_info/#3-architectural-design","title":"3. Architectural Design","text":""},{"location":"api_reference/components/execution_info/#31-core-design","title":"3.1 Core Design","text":"<ul> <li>State Management: The <code>ExecutionInfo</code> class manages the state using <code>RequestForest</code> and <code>NodeForest</code> to track requests and nodes, respectively. The <code>StampManager</code> is used to handle timestamps.</li> <li>Graph Representation: The component uses <code>Vertex</code> and <code>Edge</code> classes to represent nodes and connections in a graph structure. This allows for a clear visualization of the system's execution flow.</li> <li>Serialization: The <code>RTJSONEncoder</code> is used to serialize the graph representation into JSON, supporting various custom types like <code>Vertex</code>, <code>Edge</code>, and <code>Stamp</code>.</li> </ul>"},{"location":"api_reference/components/execution_info/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/execution_info/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>Profiling and Serialization: The component relies on <code>Stamp</code> and <code>StampManager</code> from <code>railtracks.utils.profiling</code> and <code>Vertex</code> and <code>Edge</code> from <code>railtracks.utils.serialization.graph</code>.</li> <li>Concurrency: The <code>StampManager</code> uses locks to ensure thread-safe operations when creating stamps.</li> </ul>"},{"location":"api_reference/components/execution_info/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>Graph Complexity: The complexity of the graph representation can grow with the number of nodes and requests, potentially impacting performance for very large datasets.</li> </ul>"},{"location":"api_reference/components/execution_info/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/execution_info/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>node.py</code>: Defines the <code>NodeForest</code> and <code>LinkedNode</code> classes used for managing node states.</li> <li><code>request.py</code>: Contains the <code>RequestForest</code> and <code>RequestTemplate</code> classes for handling request states.</li> <li><code>serialize.py</code>: Provides serialization utilities, including the <code>RTJSONEncoder</code>.</li> <li><code>utils.py</code>: Includes utility functions like <code>create_sub_state_info</code> for managing subsets of state information.</li> <li><code>profiling.py</code>: Contains the <code>Stamp</code> and <code>StampManager</code> classes for timestamp management.</li> <li><code>graph.py</code>: Defines the <code>Vertex</code> and <code>Edge</code> classes for graph representation.</li> </ul>"},{"location":"api_reference/components/execution_info/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>state_management.md</code>: Provides an overview of state management components, including <code>ExecutionInfo</code>.</li> </ul>"},{"location":"api_reference/components/execution_info/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>state_management.md</code>: Discusses the state management feature, which includes the <code>ExecutionInfo</code> component.</li> </ul>"},{"location":"api_reference/components/execution_info/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/executor_configuration/","title":"Executor Configuration","text":"<p>The <code>ExecutorConfig</code> class is a configuration object designed to allow customization of the executor in the RT (RailTracks) system. It provides various settings to control the behavior of the executor, such as timeout, error handling, logging, and more.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/executor_configuration/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/executor_configuration/#1-purpose","title":"1. Purpose","text":"<p>The <code>ExecutorConfig</code> class is primarily used to configure the behavior of an executor within the RT system. It allows developers to specify settings such as timeout duration, error handling preferences, logging configurations, and more. This flexibility is crucial for tailoring the executor's behavior to specific use cases and operational requirements.</p>"},{"location":"api_reference/components/executor_configuration/#11-configuring-timeout-and-error-handling","title":"1.1 Configuring Timeout and Error Handling","text":"<p>The <code>ExecutorConfig</code> allows setting a timeout for the executor's operations and determining whether the executor should stop on encountering an error.</p> <p>python config = ExecutorConfig(timeout=120.0, end_on_error=True)</p>"},{"location":"api_reference/components/executor_configuration/#12-customizing-logging","title":"1.2 Customizing Logging","text":"<p>Developers can specify the logging level and the log file location to capture the executor's activities.</p> <p>python config = ExecutorConfig(logging_setting=\"DEBUG\", log_file=\"executor.log\")</p>"},{"location":"api_reference/components/executor_configuration/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/executor_configuration/#class-executorconfig","title":"<code>class ExecutorConfig</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/executor_configuration/#__init__self","title":"<code>.__init__(self)</code>","text":"<p>ExecutorConfig is special configuration object designed to allow customization of the executor in the RT system.</p> <p>Args:     timeout (float): The maximum number of seconds to wait for a response to your top level request     end_on_error (bool): If true, the executor will stop execution when an exception is encountered.     logging_setting (allowable_log_levels): The setting for the level of logging you would like to have.     log_file (str | os.PathLike | None): The file to which the logs will be written. If None, no file will be created.     run_identifier (str | None): You can specify a run identifier to be used for this run. If None, a random UUID will be generated.     broadcast_callback (Callable or Coroutine): A function or coroutine that will handle streaming messages.     prompt_injection (bool): If true, prompts can be injected with global context     save_state (bool): If true, the state of the executor will be saved to disk.</p>"},{"location":"api_reference/components/executor_configuration/#precedence_overwrittenself","title":"<code>.precedence_overwritten(self)</code>","text":"<p>If any of the parameters are provided (not None), it will create a new update the current instance with the new values and return a deep copied reference to it.</p>"},{"location":"api_reference/components/executor_configuration/#3-architectural-design","title":"3. Architectural Design","text":"<p>The <code>ExecutorConfig</code> class is designed with flexibility and extensibility in mind. It encapsulates various configuration options that can be easily adjusted to meet different operational needs.</p>"},{"location":"api_reference/components/executor_configuration/#31-design-considerations","title":"3.1 Design Considerations","text":"<ul> <li>Flexibility: The class provides a wide range of configuration options, allowing for fine-tuned control over the executor's behavior.</li> <li>Extensibility: New configuration options can be added with minimal impact on existing functionality.</li> <li>Ease of Use: The class is designed to be intuitive, with sensible defaults and clear parameter descriptions.</li> </ul>"},{"location":"api_reference/components/executor_configuration/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/executor_configuration/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>The <code>ExecutorConfig</code> class relies on the <code>allowable_log_levels</code> from the <code>railtracks.utils.logging.config</code> module to determine valid logging levels.</li> </ul>"},{"location":"api_reference/components/executor_configuration/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>The <code>timeout</code> parameter should be set considering the expected duration of operations to avoid premature termination.</li> <li>The <code>end_on_error</code> setting should be used judiciously, as stopping on errors might not be desirable in all scenarios.</li> </ul>"},{"location":"api_reference/components/executor_configuration/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/executor_configuration/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>../utils/config.py</code>: Contains the implementation of the <code>ExecutorConfig</code> class.</li> </ul>"},{"location":"api_reference/components/executor_configuration/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>../logging_profiling.md</code>: Discusses logging and profiling features related to the executor configuration.</li> </ul>"},{"location":"api_reference/components/executor_configuration/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/function_node_base/","title":"Function Node Base","text":"<p>The Function Node Base component provides a framework for creating dynamic function nodes, supporting both synchronous and asynchronous functions. It is designed to facilitate the conversion of function parameters to the required values and to manage the execution of these functions within a node-based architecture.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/function_node_base/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/function_node_base/#1-purpose","title":"1. Purpose","text":"<p>The Function Node Base component is primarily used to create nodes that encapsulate function execution, allowing for both synchronous and asynchronous operations. This is particularly useful in scenarios where functions need to be dynamically integrated into a node-based processing pipeline.</p>"},{"location":"api_reference/components/function_node_base/#11-synchronous-function-node","title":"1.1 Synchronous Function Node","text":"<p>The synchronous function node is used when the function to be executed is synchronous. This is important for operations that do not involve I/O-bound tasks or do not require concurrency.</p> <p>python class MySyncNode(SyncDynamicFunctionNode):     @classmethod     def func(cls, args, *kwargs):         return sum(args)</p> <p>node = MySyncNode(1, 2, 3) result = node.invoke()</p>"},{"location":"api_reference/components/function_node_base/#12-asynchronous-function-node","title":"1.2 Asynchronous Function Node","text":"<p>The asynchronous function node is used for functions that are I/O-bound or require concurrency, such as network requests or file I/O operations.</p> <p>python class MyAsyncNode(AsyncDynamicFunctionNode):     @classmethod     async def func(cls, args, *kwargs):         await asyncio.sleep(1)         return sum(args)</p> <p>node = MyAsyncNode(1, 2, 3) result = await node.invoke()</p>"},{"location":"api_reference/components/function_node_base/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/function_node_base/#class-dynamicfunctionnodenode_toutput-abc-generic_p-_toutput","title":"<code>class DynamicFunctionNode(Node[_TOutput], ABC, Generic[_P, _TOutput])</code>","text":"<p>A base class which contains logic around converting function parameters to the required value given by the function. It also contains the framework for functionality of function nodes that can be built using the <code>from_function</code> method.</p> <p>NOTE: This class is not designed to be worked with directly. The classes SyncDynamicFunctionNode and AsyncDynamicFunctionNode are the ones designed for consumption.</p>"},{"location":"api_reference/components/function_node_base/#__init__self-args-kwargs","title":"<code>.__init__(self, *args, **kwargs)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/function_node_base/#funccls-args-kwargs","title":"<code>.func(cls, *args, **kwargs)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/function_node_base/#namecls","title":"<code>.name(cls)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/function_node_base/#tool_infocls","title":"<code>.tool_info(cls)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/function_node_base/#type_mappercls","title":"<code>.type_mapper(cls)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/function_node_base/#prepare_toolcls-tool_parameters","title":"<code>.prepare_tool(cls, tool_parameters)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/function_node_base/#class-syncdynamicfunctionnodedynamicfunctionnode_p-_toutput-abc","title":"<code>class SyncDynamicFunctionNode(DynamicFunctionNode[_P, _TOutput], ABC)</code>","text":"<p>A nearly complete class that expects a synchronous function to be provided in the <code>func</code> method.</p> <p>The class' internals will handle the creation of the rest of the internals required for a node to operate.</p> <p>You can override methods like name and tool_info to provide custom names and tool information. However, do note that these overrides can cause unexpected behavior if not done according to what is expected in the parent class as it uses a lot of the structures in its implementation of other functions.</p>"},{"location":"api_reference/components/function_node_base/#funccls-args-kwargs_1","title":"<code>.func(cls, *args, **kwargs)</code>","text":"<p>The function that this node will call. This function should be synchronous.</p>"},{"location":"api_reference/components/function_node_base/#invokeself","title":"<code>.invoke(self)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/function_node_base/#class-asyncdynamicfunctionnodedynamicfunctionnode_p-_toutput-abc","title":"<code>class AsyncDynamicFunctionNode(DynamicFunctionNode[_P, _TOutput], ABC)</code>","text":"<p>A nearly complete class that expects an async function to be provided in the <code>func</code> method.</p> <p>The class' internals will handle the creation of the rest of the internals required for a node to operate.</p> <p>You can override methods like name and tool_info to provide custom names and tool information. However, do note that these overrides can cause unexpected behavior if not done according to what is expected in the parent class as it uses a lot of the structures in its implementation of other functions.</p>"},{"location":"api_reference/components/function_node_base/#funccls-args-kwargs_2","title":"<code>.func(cls, *args, **kwargs)</code>","text":"<p>The async function that this node will call.</p>"},{"location":"api_reference/components/function_node_base/#invokeself_1","title":"<code>.invoke(self)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/function_node_base/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Function Node Base component is designed with flexibility and extensibility in mind, allowing developers to create nodes that can execute both synchronous and asynchronous functions. The design leverages Python's type system and abstract base classes to enforce the implementation of required methods.</p>"},{"location":"api_reference/components/function_node_base/#31-dynamic-function-node","title":"3.1 Dynamic Function Node","text":"<ul> <li>DynamicFunctionNode: Serves as the base class for all function nodes. It provides the framework for parameter conversion and function execution.</li> <li>Design Consideration: The class is abstract and not intended for direct instantiation. It requires subclasses to implement the <code>func</code>, <code>tool_info</code>, and <code>type_mapper</code> methods.</li> </ul>"},{"location":"api_reference/components/function_node_base/#32-syncdynamicfunctionnode","title":"3.2 SyncDynamicFunctionNode","text":"<ul> <li>SyncDynamicFunctionNode: Inherits from <code>DynamicFunctionNode</code> and is tailored for synchronous functions.</li> <li>Design Consideration: Ensures that the function provided is synchronous and raises an error if a coroutine is returned.</li> </ul>"},{"location":"api_reference/components/function_node_base/#33-asyncdynamicfunctionnode","title":"3.3 AsyncDynamicFunctionNode","text":"<ul> <li>AsyncDynamicFunctionNode: Inherits from <code>DynamicFunctionNode</code> and is tailored for asynchronous functions.</li> <li>Design Consideration: Designed to handle asynchronous execution, leveraging Python's <code>asyncio</code> for concurrency.</li> </ul>"},{"location":"api_reference/components/function_node_base/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/function_node_base/#41-parameter-conversion","title":"4.1 Parameter Conversion","text":"<ul> <li>The <code>DynamicFunctionNode</code> uses a <code>TypeMapper</code> to convert function parameters to the appropriate types. This is crucial for ensuring that the function receives the correct input types.</li> </ul>"},{"location":"api_reference/components/function_node_base/#42-error-handling","title":"4.2 Error Handling","text":"<ul> <li>The <code>SyncDynamicFunctionNode</code> includes error handling to ensure that synchronous functions do not inadvertently return coroutines, which would lead to runtime errors.</li> </ul>"},{"location":"api_reference/components/function_node_base/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/function_node_base/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>function_base.py</code>: Contains the implementation of the <code>DynamicFunctionNode</code>, <code>SyncDynamicFunctionNode</code>, and <code>AsyncDynamicFunctionNode</code>.</li> </ul>"},{"location":"api_reference/components/function_node_base/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>node_building.md</code>: Provides documentation on building nodes within the system.</li> </ul>"},{"location":"api_reference/components/function_node_base/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>node_management.md</code>: Discusses the management and orchestration of nodes within the system.</li> </ul>"},{"location":"api_reference/components/function_node_base/#54-external-dependencies","title":"5.4 External Dependencies","text":"<ul> <li><code>typing_extensions</code>: Used for advanced type hinting features.</li> </ul>"},{"location":"api_reference/components/function_node_base/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/graph_serialization/","title":"Graph Serialization","text":"<p>The Graph Serialization component defines classes for representing edges and vertices in a graph structure, intended for use in a request system. This component is crucial for modeling relationships and dependencies within the system, allowing for efficient data serialization and deserialization.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/graph_serialization/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/graph_serialization/#1-purpose","title":"1. Purpose","text":"<p>The Graph Serialization component is designed to facilitate the representation of graph structures through the <code>Edge</code> and <code>Vertex</code> classes. These classes are used to model the relationships and properties of nodes and edges within a graph, which is essential for request handling and data processing in the system.</p>"},{"location":"api_reference/components/graph_serialization/#11-representing-edges","title":"1.1 Representing Edges","text":"<p>The <code>Edge</code> class models a connection between two vertices in a graph. It is crucial for defining the relationships and dependencies between different nodes.</p> <p>python from railtracks.utils.serialization.graph import Edge</p> <p>edge = Edge(     identifier=\"edge_1\",     source=\"vertex_1\",     target=\"vertex_2\",     stamp=some_stamp_instance,     details={\"weight\": 5},     parent=None )</p>"},{"location":"api_reference/components/graph_serialization/#12-representing-vertices","title":"1.2 Representing Vertices","text":"<p>The <code>Vertex</code> class encapsulates the properties of a node in a graph, allowing for detailed representation of each vertex's attributes.</p> <p>python from railtracks.utils.serialization.graph import Vertex</p> <p>vertex = Vertex(     identifier=\"vertex_1\",     node_type=\"type_a\",     stamp=some_stamp_instance,     details={\"attribute\": \"value\"},     parent=None )</p>"},{"location":"api_reference/components/graph_serialization/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/graph_serialization/#class-edge","title":"<code>class Edge</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/graph_serialization/#__init__self","title":"<code>.__init__(self)</code>","text":"<p>A simple representation of an edge in a graph structure.</p> <p>This type is designed to be used as a request in the system and should not be extended for other uses.</p> <p>Args:     identifier (str | None): The unique identifier for the edge.     source (str | None): The source vertex of the edge. None can be expected if the input does not have a source     target (str): The target (sink) vertex of the edge.     stamp (Stamp): A timestamp that is attached to this edge.     details (dict[str, Any]): Additional details about the edge, which can include any relevant information.     parent (Edge | None): An optional parent edge, this item should represent the temporal parent of the edge.</p>"},{"location":"api_reference/components/graph_serialization/#class-vertex","title":"<code>class Vertex</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/graph_serialization/#__init__self_1","title":"<code>.__init__(self)</code>","text":"<p>The Vertex class represents a single vertex in a graph structure.</p> <p>This class is designed to encapsulate the properties that a node would have and should not be extended for use cases outside of that</p> <p>Args:     identifier (str): The unique identifier for the vertex.     node_type (str): The type of the node, which can be used to differentiate between different kinds of nodes.     stamp (Stamp): A timestamp that represents a timestamp attached this vertex.     details (dict[str, Any]): Additional details about the vertex, which can include any relevant information.     Often times this should contain     parent (Vertex | None): An optional parent vertex, this item should represent the temporal parent of the vertex.</p>"},{"location":"api_reference/components/graph_serialization/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Graph Serialization component is designed with simplicity and efficiency in mind, focusing on the core functionality of graph representation without unnecessary complexity.</p>"},{"location":"api_reference/components/graph_serialization/#31-design-considerations","title":"3.1 Design Considerations","text":"<ul> <li>Edge Class:</li> <li>Designed to represent a directed connection between two vertices.</li> <li>Includes attributes for identifier, source, target, timestamp, and additional details.</li> <li> <p>Ensures consistency by asserting that the parent edge, if present, matches the current edge's identifier, source, and target.</p> </li> <li> <p>Vertex Class:</p> </li> <li>Represents a single node in the graph with attributes for identifier, node type, timestamp, and additional details.</li> <li>Ensures consistency by asserting that the parent vertex, if present, matches the current vertex's identifier.</li> </ul>"},{"location":"api_reference/components/graph_serialization/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/graph_serialization/#41-implementation-details","title":"4.1 Implementation Details","text":"<ul> <li>Stamp Dependency: The <code>Edge</code> and <code>Vertex</code> classes rely on a <code>Stamp</code> object from the <code>railtracks.utils.profiling</code> module, which must be correctly instantiated and passed to these classes.</li> <li>Parent Consistency: Both classes enforce consistency checks on parent-child relationships to maintain graph integrity.</li> </ul>"},{"location":"api_reference/components/graph_serialization/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/graph_serialization/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>graph.py</code>: Contains the implementation of the <code>Edge</code> and <code>Vertex</code> classes.</li> </ul>"},{"location":"api_reference/components/graph_serialization/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li>Serialization Component: Provides an overview of serialization strategies used across the system.</li> </ul>"},{"location":"api_reference/components/graph_serialization/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li>State Management Feature: Discusses how graph serialization integrates with state management within the system.</li> </ul>"},{"location":"api_reference/components/graph_serialization/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/in_memory_vector_store/","title":"In-Memory Vector Store","text":"<p>The In-Memory Vector Store is a component designed to store and manage feature vectors and their associated metadata entirely in memory. It provides a fast and efficient way to handle vector data for applications that require quick access and manipulation of vectors, such as machine learning and data analysis tasks.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/in_memory_vector_store/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/in_memory_vector_store/#1-purpose","title":"1. Purpose","text":"<p>The In-Memory Vector Store is primarily used for storing and managing feature vectors and their metadata in a fast and efficient manner. It is particularly useful in scenarios where quick access to vector data is crucial, such as in real-time data processing or machine learning model inference.</p>"},{"location":"api_reference/components/in_memory_vector_store/#11-adding-vectors","title":"1.1 Adding Vectors","text":"<p>The component allows adding vectors or raw text, which can be embedded into vectors using an optional embedding service.</p> <p>python store = InMemoryVectorStore(embedding_service=my_embedding_service) ids = store.add([\"text1\", \"text2\"], embed=True)</p>"},{"location":"api_reference/components/in_memory_vector_store/#12-searching-vectors","title":"1.2 Searching Vectors","text":"<p>It supports searching for the most similar vectors to a given query, using various similarity metrics.</p> <p>python results = store.search(\"query text\", top_k=3, embed=True)</p>"},{"location":"api_reference/components/in_memory_vector_store/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/in_memory_vector_store/#3-architectural-design","title":"3. Architectural Design","text":"<p>The In-Memory Vector Store is designed to be a lightweight and efficient solution for vector storage and retrieval. It leverages Python's built-in data structures to maintain a mapping of vector IDs to vectors and their associated metadata.</p>"},{"location":"api_reference/components/in_memory_vector_store/#31-core-design-principles","title":"3.1 Core Design Principles","text":"<ul> <li>Simplicity and Efficiency: The component is designed to be simple and efficient, using Python dictionaries to store vectors and metadata.</li> <li>Flexibility: Supports various similarity metrics and optional vector normalization.</li> <li>Extensibility: Can be extended with an embedding service to convert raw text into vectors.</li> </ul>"},{"location":"api_reference/components/in_memory_vector_store/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/in_memory_vector_store/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>Embedding Service: An optional embedding service can be provided to convert text into vectors. This service must implement the <code>BaseEmbeddingService</code> interface.</li> </ul>"},{"location":"api_reference/components/in_memory_vector_store/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>Memory Usage: As an in-memory store, it is limited by the available system memory.</li> <li>Concurrency: The component is not thread-safe and should be used in a single-threaded context or with external synchronization.</li> </ul>"},{"location":"api_reference/components/in_memory_vector_store/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/in_memory_vector_store/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>in_memory.py</code>: Contains the implementation of the In-Memory Vector Store.</li> </ul>"},{"location":"api_reference/components/in_memory_vector_store/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>vector_store_base.md</code>: Describes the base class and interfaces for vector stores.</li> </ul>"},{"location":"api_reference/components/in_memory_vector_store/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>rag_system.md</code>: Provides an overview of the RAG system, which utilizes the vector store.</li> </ul>"},{"location":"api_reference/components/in_memory_vector_store/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/jupyter_compatibility/","title":"Jupyter Compatibility","text":"<p>This component provides compatibility patches for MCP tools to function correctly in Jupyter notebooks, addressing I/O stream limitations.</p> <p>Version: 0.0.1</p> <p>Component Contact: @github_username</p>"},{"location":"api_reference/components/jupyter_compatibility/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/jupyter_compatibility/#1-purpose","title":"1. Purpose","text":"<p>The Jupyter Compatibility component is designed to ensure that MCP tools, which typically rely on standard I/O streams, can function seamlessly within Jupyter notebook environments. This is achieved by providing monkey patches for subprocess creation functions that are incompatible with Jupyter's custom I/O streams.</p>"},{"location":"api_reference/components/jupyter_compatibility/#11-subprocess-creation-in-jupyter","title":"1.1 Subprocess Creation in Jupyter","text":"<p>In Jupyter notebooks, the standard I/O streams are replaced with custom implementations that do not support the <code>fileno()</code> method. This component provides patched versions of subprocess creation functions to handle this limitation.</p> <p>python from railtracks.rt_mcp.jupyter_compat import apply_patches</p> <p>apply_patches()</p>"},{"location":"api_reference/components/jupyter_compatibility/#now-you-can-use-mcp-tools-that-rely-on-subprocess-creation-in-jupyter-notebooks","title":"Now you can use MCP tools that rely on subprocess creation in Jupyter notebooks.","text":""},{"location":"api_reference/components/jupyter_compatibility/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/jupyter_compatibility/#def-is_jupyter","title":"<code>def is_jupyter()</code>","text":"<p>Check if we're running in a Jupyter notebook environment.</p> <p>Returns:     bool: True if running in a Jupyter notebook, False otherwise.</p>"},{"location":"api_reference/components/jupyter_compatibility/#def-patched_create_windows_processcommand-args-env-errlog-cwd","title":"<code>def patched_create_windows_process(command, args, env, errlog, cwd)</code>","text":"<p>Patched version of create_windows_process that works in Jupyter notebooks.</p> <p>This function wraps the original create_windows_process function and handles the case where errlog doesn't support fileno() in Jupyter notebooks.</p>"},{"location":"api_reference/components/jupyter_compatibility/#def-patched_create_windows_fallback_processcommand-args-env-errlog-cwd","title":"<code>def patched_create_windows_fallback_process(command, args, env, errlog, cwd)</code>","text":"<p>Patched version of _create_windows_fallback_process that works in Jupyter notebooks.</p> <p>This function reimplements the original _create_windows_fallback_process function to handle the case where errlog doesn't support fileno() in Jupyter notebooks.</p>"},{"location":"api_reference/components/jupyter_compatibility/#def-apply_patches","title":"<code>def apply_patches()</code>","text":"<p>Apply the monkey patches to make MCP work in Jupyter notebooks.</p> <p>This function patches the create_windows_process and _create_windows_fallback_process functions in the mcp.os.win32.utilities module to make them work in Jupyter notebooks.</p> <p>The patches are only applied if we're in a Jupyter environment.</p>"},{"location":"api_reference/components/jupyter_compatibility/#3-architectural-design","title":"3. Architectural Design","text":""},{"location":"api_reference/components/jupyter_compatibility/#31-design-considerations","title":"3.1 Design Considerations","text":"<ul> <li>Monkey Patching: The component uses monkey patching to replace the original subprocess creation functions with versions that are compatible with Jupyter's I/O streams.</li> <li>Environment Detection: The <code>is_jupyter()</code> function is used to detect if the code is running within a Jupyter environment, ensuring that patches are only applied when necessary.</li> <li>Error Handling: The <code>_safe_stderr_for_jupyter()</code> function ensures that error logs are handled correctly, even when the standard error stream does not support <code>fileno()</code>.</li> </ul>"},{"location":"api_reference/components/jupyter_compatibility/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/jupyter_compatibility/#41-implementation-details","title":"4.1 Implementation Details","text":"<ul> <li>Platform Specificity: The patches are only applied on Windows platforms, as indicated by the <code>sys.platform.startswith(\"win\")</code> check.</li> <li>Single Application: The patches are applied only once per session to prevent redundant operations, controlled by the <code>_patched</code> flag.</li> <li>Fallback Mechanism: If the patched subprocess creation with <code>creationflags</code> fails, a fallback mechanism without these flags is used to ensure compatibility.</li> </ul>"},{"location":"api_reference/components/jupyter_compatibility/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/jupyter_compatibility/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>../packages/railtracks/src/railtracks/rt_mcp/jupyter_compat.py</code>: Contains the implementation of the Jupyter compatibility patches.</li> </ul>"},{"location":"api_reference/components/jupyter_compatibility/#52-related-feature-files","title":"5.2 Related Feature Files","text":"<ul> <li><code>../features/mcp_integration.md</code>: Documents the integration of MCP tools with other systems, including Jupyter compatibility.</li> </ul>"},{"location":"api_reference/components/jupyter_compatibility/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/llm_messaging/","title":"LLM Messaging","text":"<p>The LLM Messaging component is designed to handle messaging for language models, including managing message history, content, and structured interactions.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/llm_messaging/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/llm_messaging/#1-purpose","title":"1. Purpose","text":"<p>The LLM Messaging component is essential for managing interactions with language models. It provides structures for message content, maintains message history, and supports different roles in messaging, such as user, assistant, system, and tool. This component is crucial for applications that require structured communication with language models.</p>"},{"location":"api_reference/components/llm_messaging/#11-message-creation-and-management","title":"1.1 Message Creation and Management","text":"<p>This use case involves creating and managing messages with different roles and content types.</p> <p>python from railtracks.llm.message import UserMessage, SystemMessage, AssistantMessage, ToolMessage from railtracks.llm.content import ToolResponse</p>"},{"location":"api_reference/components/llm_messaging/#create-a-user-message","title":"Create a user message","text":"<p>user_msg = UserMessage(content=\"Hello, how can I assist you today?\")</p>"},{"location":"api_reference/components/llm_messaging/#create-a-system-message","title":"Create a system message","text":"<p>system_msg = SystemMessage(content=\"System maintenance scheduled at midnight.\")</p>"},{"location":"api_reference/components/llm_messaging/#create-an-assistant-message","title":"Create an assistant message","text":"<p>assistant_msg = AssistantMessage(content=\"Sure, I can help with that.\")</p>"},{"location":"api_reference/components/llm_messaging/#create-a-tool-message-with-a-tool-response","title":"Create a tool message with a tool response","text":"<p>tool_response = ToolResponse(identifier=\"123\", name=\"WeatherTool\", result=\"Sunny\") tool_msg = ToolMessage(content=tool_response)</p>"},{"location":"api_reference/components/llm_messaging/#12-message-history-management","title":"1.2 Message History Management","text":"<p>This use case demonstrates how to manage and manipulate message history.</p> <p>python from railtracks.llm.history import MessageHistory from railtracks.llm.message import UserMessage, SystemMessage</p>"},{"location":"api_reference/components/llm_messaging/#create-a-message-history","title":"Create a message history","text":"<p>history = MessageHistory()</p>"},{"location":"api_reference/components/llm_messaging/#append-messages-to-history","title":"Append messages to history","text":"<p>history.append(UserMessage(content=\"User message\")) history.append(SystemMessage(content=\"System message\"))</p>"},{"location":"api_reference/components/llm_messaging/#remove-system-messages-from-history","title":"Remove system messages from history","text":"<p>filtered_history = history.removed_system_messages()</p>"},{"location":"api_reference/components/llm_messaging/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/llm_messaging/#class-toolcallbasemodel","title":"<code>class ToolCall(BaseModel)</code>","text":"<p>A simple model object that represents a tool call.</p> <p>This simple model represents a moment when a tool is called.</p>"},{"location":"api_reference/components/llm_messaging/#class-toolresponsebasemodel","title":"<code>class ToolResponse(BaseModel)</code>","text":"<p>A simple model object that represents a tool response.</p> <p>This simple model should be used when adding a response to a tool.</p>"},{"location":"api_reference/components/llm_messaging/#class-messagehistorylistmessage","title":"<code>class MessageHistory(List[Message])</code>","text":"<p>A basic object that represents a history of messages. The object has all the same capability as a list such as <code>.remove()</code>, <code>.append()</code>, etc.</p>"},{"location":"api_reference/components/llm_messaging/#removed_system_messagesself","title":"<code>.removed_system_messages(self)</code>","text":"<p>Returns a new MessageHistory object with all SystemMessages removed.</p>"},{"location":"api_reference/components/llm_messaging/#class-usermessage_stringonlycontent","title":"<code>class UserMessage(_StringOnlyContent)</code>","text":"<p>Note that we only support string input</p> <p>Args:     content (str): The content of the user message.     inject_prompt (bool, optional): Whether to inject prompt with context variables. Defaults to True.</p>"},{"location":"api_reference/components/llm_messaging/#__init__self-content-inject_prompt","title":"<code>.__init__(self, content, inject_prompt)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/llm_messaging/#class-systemmessage_stringonlycontent","title":"<code>class SystemMessage(_StringOnlyContent)</code>","text":"<p>A simple class that represents a system message.</p> <p>Args:     content (str): The content of the system message.     inject_prompt (bool, optional): Whether to inject prompt with context  variables. Defaults to True.</p>"},{"location":"api_reference/components/llm_messaging/#__init__self-content-inject_prompt_1","title":"<code>.__init__(self, content, inject_prompt)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/llm_messaging/#class-assistantmessagemessage_t-generic_t","title":"<code>class AssistantMessage(Message[_T], Generic[_T])</code>","text":"<p>A simple class that represents a message from the assistant.</p> <p>Args:     content (_T): The content of the assistant message.     inject_prompt (bool, optional): Whether to inject prompt with context  variables. Defaults to True.</p>"},{"location":"api_reference/components/llm_messaging/#__init__self-content-inject_prompt_2","title":"<code>.__init__(self, content, inject_prompt)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/llm_messaging/#class-toolmessagemessagetoolresponse","title":"<code>class ToolMessage(Message[ToolResponse])</code>","text":"<p>A simple class that represents a message that is a tool call answer.</p> <p>Args:     content (ToolResponse): The tool response content for the message.</p>"},{"location":"api_reference/components/llm_messaging/#__init__self-content","title":"<code>.__init__(self, content)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/llm_messaging/#3-architectural-design","title":"3. Architectural Design","text":""},{"location":"api_reference/components/llm_messaging/#31-message-structure-and-roles","title":"3.1 Message Structure and Roles","text":"<ul> <li>Message Class: The <code>Message</code> class is a generic base class that represents a message with content and a role. It supports various content types, including strings, tool calls, and tool responses.</li> <li>Role Enum: The <code>Role</code> enum defines the possible roles a message can have, such as <code>assistant</code>, <code>user</code>, <code>system</code>, and <code>tool</code>.</li> <li>Design Considerations: The design allows for flexibility in message content and role assignment, supporting diverse interaction scenarios with language models.</li> </ul>"},{"location":"api_reference/components/llm_messaging/#32-content-management","title":"3.2 Content Management","text":"<ul> <li>ToolCall and ToolResponse: These classes represent tool interactions, encapsulating the call and response data. They are used to structure messages that involve tool operations.</li> <li>Content Union: The <code>Content</code> type is a union of possible content types, ensuring that messages can handle various data structures.</li> </ul>"},{"location":"api_reference/components/llm_messaging/#33-message-history","title":"3.3 Message History","text":"<ul> <li>MessageHistory Class: Inherits from Python's list to manage a sequence of messages. It provides additional functionality, such as filtering out system messages.</li> <li>Design Considerations: The use of list inheritance allows for natural list operations while extending functionality specific to message history management.</li> </ul>"},{"location":"api_reference/components/llm_messaging/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/llm_messaging/#41-content-validation","title":"4.1 Content Validation","text":"<ul> <li>Content Validation: The <code>Message</code> class includes a <code>validate_content</code> method to ensure that the content type is appropriate for the message type. This is crucial for maintaining data integrity.</li> </ul>"},{"location":"api_reference/components/llm_messaging/#42-role-assignment","title":"4.2 Role Assignment","text":"<ul> <li>Role Assignment: The role of a message is critical for its processing and interpretation. Ensure that roles are correctly assigned to avoid miscommunication in interactions.</li> </ul>"},{"location":"api_reference/components/llm_messaging/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/llm_messaging/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>content.py</code>: Defines data structures for tool calls and responses.</li> <li><code>history.py</code>: Manages message history, including filtering capabilities.</li> <li><code>message.py</code>: Contains the core message classes and role definitions.</li> </ul>"},{"location":"api_reference/components/llm_messaging/#52-related-feature-files","title":"5.2 Related Feature Files","text":"<ul> <li><code>llm_integration.md</code>: Documents the integration of LLM messaging within the broader system.</li> </ul>"},{"location":"api_reference/components/llm_messaging/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/llm_node_base/","title":"LLM Node Base","text":"<p>The LLM Node Base component provides a foundational framework for interacting with Large Language Models (LLMs), managing message histories, and handling structured outputs. It serves as a base class for creating nodes that can communicate with LLMs, encapsulating the logic for attaching models and managing message histories.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/llm_node_base/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/llm_node_base/#1-purpose","title":"1. Purpose","text":"<p>The LLM Node Base component is designed to facilitate the integration and interaction with LLMs by providing a structured way to manage message histories and model interactions. It is primarily used to:</p> <ul> <li>Attach LLM models and manage message histories.</li> <li>Provide hooks for pre-processing and post-processing of messages.</li> <li>Handle structured and unstructured outputs from LLMs.</li> </ul>"},{"location":"api_reference/components/llm_node_base/#11-attaching-llm-models","title":"1.1 Attaching LLM Models","text":"<p>The component allows for the attachment of LLM models to nodes, enabling the processing of message histories and the execution of LLM-based tasks.</p> <p>python from railtracks.nodes.concrete._llm_base import LLMBase from railtracks.llm import MessageHistory, ModelBase</p> <p>llm_node = LLMBase(user_input=MessageHistory(), llm_model=ModelBase())</p>"},{"location":"api_reference/components/llm_node_base/#12-managing-message-histories","title":"1.2 Managing Message Histories","text":"<p>The component provides mechanisms to manage and manipulate message histories, ensuring that messages are correctly formatted and processed.</p> <p>python message_history = llm_node.prepare_tool_message_history(tool_parameters={\"param1\": \"value1\"})</p>"},{"location":"api_reference/components/llm_node_base/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/llm_node_base/#class-structuredllmstructuredoutputmixin_toutput-llmbase_toutput-abc-generic_toutput","title":"<code>class StructuredLLM(StructuredOutputMixIn[_TOutput], LLMBase[_TOutput], ABC, Generic[_TOutput])</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/llm_node_base/#__init__self-user_input-llm_model","title":"<code>.__init__(self, user_input, llm_model)</code>","text":"<p>Creates a new instance of the StructuredlLLM class</p> <p>Args:     user_input (MessageHistory): The message history to use for the LLM.     llm_model (ModelBase | None, optional): The LLM model to use. Defaults to None.</p>"},{"location":"api_reference/components/llm_node_base/#namecls","title":"<code>.name(cls)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/llm_node_base/#invokeself","title":"<code>.invoke(self)</code>","text":"<p>Makes a call containing the inputted message and system prompt to the llm model and returns the response</p> <p>Returns:     (StructuredlLLM.Output): The response message from the llm model</p>"},{"location":"api_reference/components/llm_node_base/#class-llmbasenode_t-abc-generic_t","title":"<code>class LLMBase(Node[_T], ABC, Generic[_T])</code>","text":"<p>A basic LLM base class that encapsulates the attaching of an LLM model and message history object.</p> <p>The main functionality of the class is contained within the attachment of pre and post hooks to the model so we can store debugging details that will allow us to determine token usage.</p> <p>Args:     user_input: The message history to use. Can be a MessageHistory object, a UserMessage object, or a string.         If a string is provided, it will be converted to a MessageHistory with a UserMessage.         If a UserMessage is provided, it will be converted to a MessageHistory.         llm_model: The LLM model to use. If None, the default model will be used.</p>"},{"location":"api_reference/components/llm_node_base/#__init__self-user_input-llm_model_1","title":"<code>.__init__(self, user_input, llm_model)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/llm_node_base/#prepare_tool_message_historycls-tool_parameters-tool_params","title":"<code>.prepare_tool_message_history(cls, tool_parameters, tool_params)</code>","text":"<p>Prepare a message history for a tool call with the given parameters.</p> <p>This method creates a coherent instruction message from tool parameters instead of multiple separate messages.</p> <p>Args:     tool_parameters: Dictionary of parameter names to values     tool_params: Iterable of Parameter objects defining the tool parameters</p> <p>Returns:     MessageHistory object with a single UserMessage containing the formatted parameters</p>"},{"location":"api_reference/components/llm_node_base/#return_outputself","title":"<code>.return_output(self)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/llm_node_base/#get_llm_modelcls","title":"<code>.get_llm_model(cls)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/llm_node_base/#system_messagecls","title":"<code>.system_message(cls)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/llm_node_base/#return_intoself","title":"<code>.return_into(self)</code>","text":"<p>Return the name of the variable to return the result into. This method can be overridden by subclasses to customize the return variable name. By default, it returns None.</p>"},{"location":"api_reference/components/llm_node_base/#returns","title":"Returns","text":"<p>str     The name of the variable to return the result into.</p>"},{"location":"api_reference/components/llm_node_base/#format_for_returnself-result","title":"<code>.format_for_return(self, result)</code>","text":"<p>Format the result for return when return_into is provided. This method can be overridden by subclasses to customize the return format. By default, it returns None.</p> <p>Args:     result (Any): The result to format.</p> <p>Returns:     Any: The formatted result.</p>"},{"location":"api_reference/components/llm_node_base/#format_for_contextself-result","title":"<code>.format_for_context(self, result)</code>","text":"<p>Format the result for context when return_into is provided. This method can be overridden by subclasses to customize the context format. By default, it returns the result as is.</p> <p>Args:     result (Any): The result to format.</p> <p>Returns:     Any: The formatted result.</p>"},{"location":"api_reference/components/llm_node_base/#safe_copyself","title":"<code>.safe_copy(self)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/llm_node_base/#3-architectural-design","title":"3. Architectural Design","text":"<p>The LLM Node Base component is designed with flexibility and extensibility in mind, allowing developers to create custom nodes that interact with LLMs. The design focuses on:</p> <ul> <li>Core Philosophy &amp; Design Principles: The component is built around the principles of modularity and reusability, allowing for easy integration with different LLM models.</li> <li>High-Level Architecture &amp; Data Flow: The component manages the flow of data between the user input, message history, and the LLM model, ensuring that messages are processed and responses are handled appropriately.</li> <li>Key Design Decisions &amp; Trade-offs: The component uses a hook-based system for pre-processing and post-processing messages, allowing for customization and extensibility.</li> <li>Component Boundaries &amp; Responsibilities: The component is responsible for managing message histories and interacting with LLM models. It is not responsible for the implementation of specific LLM models or the handling of external dependencies.</li> </ul>"},{"location":"api_reference/components/llm_node_base/#31-message-history-management","title":"3.1 Message History Management","text":"<ul> <li>Function: <code>prepare_tool_message_history</code></li> <li>Design Consideration: Ensures that message histories are correctly formatted and processed before being sent to the LLM model.</li> <li>Design Consideration: Provides a coherent instruction message from tool parameters.</li> </ul>"},{"location":"api_reference/components/llm_node_base/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/llm_node_base/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>The component relies on the <code>railtracks.llm</code> package for message and model management.</li> <li>Ensure that the LLM models used are compatible with the <code>ModelBase</code> class.</li> </ul>"},{"location":"api_reference/components/llm_node_base/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>The component is designed to handle typical LLM interactions but may require optimization for large-scale deployments.</li> <li>Known bottlenecks include the processing of large message histories and the handling of complex model interactions.</li> </ul>"},{"location":"api_reference/components/llm_node_base/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/llm_node_base/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>_llm_base.py</code>: Contains the implementation of the LLMBase class and related functionalities.</li> <li><code>structured_llm_base.py</code>: Contains the implementation of the StructuredLLM class, extending the LLMBase for structured outputs.</li> </ul>"},{"location":"api_reference/components/llm_node_base/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li>LLM Messaging Component: Provides additional context and functionalities related to LLM messaging.</li> </ul>"},{"location":"api_reference/components/llm_node_base/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li>Node Management Feature: Describes the management and orchestration of nodes within the system.</li> </ul>"},{"location":"api_reference/components/llm_node_base/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/local_model_wrappers/","title":"Local Model Wrappers","text":"<p>The Local Model Wrappers component provides a wrapper for local language model interactions, specifically designed for Ollama server models. It facilitates seamless communication with local instances of language models hosted on an Ollama server, ensuring efficient and reliable model utilization.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/local_model_wrappers/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/local_model_wrappers/#1-purpose","title":"1. Purpose","text":"<p>The Local Model Wrappers component is primarily used to interact with language models hosted on a local Ollama server. It abstracts the complexities involved in setting up and managing connections to the server, allowing developers to focus on model interactions.</p>"},{"location":"api_reference/components/local_model_wrappers/#11-initialize-ollama-model","title":"1.1 Initialize Ollama Model","text":"<p>The primary use case is to initialize a connection to an Ollama model hosted locally. This is crucial for applications that require local processing of language models without relying on cloud services.</p> <p>python from railtracks.llm.models.local.ollama import OllamaLLM</p> <p>ollama_model = OllamaLLM(model_name=\"my_model\", domain=\"default\")</p>"},{"location":"api_reference/components/local_model_wrappers/#12-chat-with-tools","title":"1.2 Chat with Tools","text":"<p>Another use case is to facilitate conversations with the model using tools, which is essential for applications that require interactive model responses.</p> <p>python response = ollama_model.chat_with_tools(messages=[\"Hello, model!\"], tools=my_tools)</p>"},{"location":"api_reference/components/local_model_wrappers/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/local_model_wrappers/#class-ollamallmlitellmwrapper","title":"<code>class OllamaLLM(LiteLLMWrapper)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/local_model_wrappers/#__init__self-model_name-domain-custom_domain-kwargs","title":"<code>.__init__(self, model_name, domain, custom_domain, **kwargs)</code>","text":"<p>Initialize an Ollama LLM instance.</p> <p>Args:     model_name (str): Name of the Ollama model to use.     domain (Literal[\"default\", \"auto\", \"custom\"], optional): The domain configuration mode.         - \"default\": Uses the default localhost domain (http://localhost:11434)         - \"auto\": Uses the OLLAMA_HOST environment variable, raises OllamaError if not set         - \"custom\": Uses the provided custom_domain parameter, raises OllamaError if not provided         Defaults to \"default\".     custom_domain (str | None, optional): Custom domain URL to use when domain is set to \"custom\".         Must be provided if domain=\"custom\". Defaults to None.     **kwargs: Additional arguments passed to the parent LiteLLMWrapper.</p> <p>Raises:     OllamaError: If:         - domain is \"auto\" and OLLAMA_HOST environment variable is not set         - domain is \"custom\" and custom_domain is not provided         - specified model is not available on the server     RequestException: If connection to Ollama server fails</p>"},{"location":"api_reference/components/local_model_wrappers/#chat_with_toolsself-messages-tools-kwargs","title":"<code>.chat_with_tools(self, messages, tools, **kwargs)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/local_model_wrappers/#model_typecls","title":"<code>.model_type(cls)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/local_model_wrappers/#3-architectural-design","title":"3. Architectural Design","text":"<p>The design of the Local Model Wrappers component is centered around providing a robust and flexible interface for interacting with local language models.</p>"},{"location":"api_reference/components/local_model_wrappers/#31-ollamallm-class","title":"3.1 OllamaLLM Class","text":"<ul> <li>Domain Configuration: The <code>OllamaLLM</code> class allows for flexible domain configuration through the <code>domain</code> parameter, supporting \"default\", \"auto\", and \"custom\" modes.</li> <li>Error Handling: The component includes comprehensive error handling, raising <code>OllamaError</code> for domain configuration issues and <code>requests.exceptions.RequestException</code> for server connection failures.</li> <li>Logging: Utilizes a dedicated logger (<code>OLLAMA</code>) to track critical operations and errors, aiding in debugging and monitoring.</li> </ul>"},{"location":"api_reference/components/local_model_wrappers/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/local_model_wrappers/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>Environment Variables: The <code>OLLAMA_HOST</code> environment variable must be set when using the \"auto\" domain configuration.</li> <li>Server Availability: The Ollama server must be running and accessible at the specified domain for successful model interactions.</li> </ul>"},{"location":"api_reference/components/local_model_wrappers/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>Model Availability: Only models available on the Ollama server can be utilized. The component checks for model availability during initialization.</li> </ul>"},{"location":"api_reference/components/local_model_wrappers/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/local_model_wrappers/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>ollama.py</code>: Contains the implementation of the <code>OllamaLLM</code> class for interacting with local Ollama models.</li> </ul>"},{"location":"api_reference/components/local_model_wrappers/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>api_provider_wrappers.md</code>: Documents the API provider wrappers related to this component.</li> </ul>"},{"location":"api_reference/components/local_model_wrappers/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>llm_integration.md</code>: Details the integration of language models within the broader system architecture.</li> </ul>"},{"location":"api_reference/components/local_model_wrappers/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/logger_creation/","title":"Logger Creation","text":"<p>The Logger Creation component provides a utility function to obtain a logger, either a specific one or the root RT logger, for logging purposes. This component is essential for managing logging configurations and ensuring consistent logging behavior across the application.</p> <p>Version: 0.0.1</p> <p>Component Contact: @github_username</p>"},{"location":"api_reference/components/logger_creation/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/logger_creation/#1-purpose","title":"1. Purpose","text":"<p>The Logger Creation component is primarily used to manage and configure loggers within the application. It allows developers to obtain a logger by name, which is a wrapper around the standard Python <code>logging</code> module, with additional functionality to reference the RT root logger. This ensures that all logging is centralized and can be easily managed.</p>"},{"location":"api_reference/components/logger_creation/#11-obtaining-a-logger","title":"1.1 Obtaining a Logger","text":"<p>The primary use case for this component is to obtain a logger for logging messages. This is crucial for debugging and monitoring the application.</p> <p>python from railtracks.utils.logging.create import get_rt_logger</p>"},{"location":"api_reference/components/logger_creation/#obtain-the-root-rt-logger","title":"Obtain the root RT logger","text":"<p>logger = get_rt_logger()</p>"},{"location":"api_reference/components/logger_creation/#obtain-a-specific-logger","title":"Obtain a specific logger","text":"<p>specific_logger = get_rt_logger(\"specific_name\")</p>"},{"location":"api_reference/components/logger_creation/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/logger_creation/#def-get_rt_loggername","title":"<code>def get_rt_logger(name)</code>","text":"<p>A method used to get a logger of the provided name.</p> <p>The method is essentially a wrapper of the <code>logging</code> method to collect the logger, but it will add a reference to the RT root logger.</p> <p>If the name is not provided it returns the root RT logger.</p>"},{"location":"api_reference/components/logger_creation/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Logger Creation component is designed to provide a simple interface for obtaining loggers while ensuring that all loggers are part of the RT logging hierarchy. This design choice allows for centralized logging configuration and management.</p>"},{"location":"api_reference/components/logger_creation/#31-logger-hierarchy","title":"3.1 Logger Hierarchy","text":"<ul> <li>RT Root Logger: The root logger for the application, identified by the name <code>RT</code>. All other loggers are children of this root logger.</li> <li>Logger Naming: Loggers are named using the format <code>RT.&lt;name&gt;</code>, where <code>&lt;name&gt;</code> is the specific logger name provided by the user.</li> </ul>"},{"location":"api_reference/components/logger_creation/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/logger_creation/#41-logger-configuration","title":"4.1 Logger Configuration","text":"<ul> <li>Default Logger Name: The default logger name is <code>RT</code>, as defined in the config.py file.</li> <li>Colorful Logging: The component supports colorful logging using the <code>ColorfulFormatter</code> class, which applies colors based on log levels and specific keywords.</li> </ul>"},{"location":"api_reference/components/logger_creation/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/logger_creation/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>create.py</code>: Contains the <code>get_rt_logger</code> function for obtaining loggers.</li> <li><code>config.py</code>: Defines the RT logger name and provides various logger configurations.</li> </ul>"},{"location":"api_reference/components/logger_creation/#52-related-feature-files","title":"5.2 Related Feature Files","text":"<ul> <li><code>logging_profiling.md</code>: (Not found) Expected to contain documentation related to logging and profiling features.</li> </ul>"},{"location":"api_reference/components/logger_creation/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/logging_actions/","title":"Logging Actions","text":"<p>The Logging Actions component provides a framework for logging actions related to request creation and completion in a node-based system. It defines a set of classes that encapsulate different types of logging actions, such as request creation, successful completion, and failure, allowing for structured and consistent logging across the system.</p> <p>Version: 0.0.1</p> <p>Component Contact: @github_username</p>"},{"location":"api_reference/components/logging_actions/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/logging_actions/#1-purpose","title":"1. Purpose","text":"<p>The Logging Actions component is designed to facilitate structured logging of actions within a node-based system. It provides abstractions for logging the creation of requests, their successful completion, and any failures that occur. This component is essential for monitoring and debugging the flow of requests through the system.</p>"},{"location":"api_reference/components/logging_actions/#11-logging-request-creation","title":"1.1 Logging Request Creation","text":"<p>The <code>RequestCreationAction</code> class is used to log the creation of a request from a parent node to a child node. This is important for tracking the initiation of processes within the system.</p> <p>python from railtracks.utils.logging.action import RequestCreationAction</p> <p>action = RequestCreationAction(     parent_node_name=\"ParentNode\",     child_node_name=\"ChildNode\",     input_args=(arg1, arg2),     input_kwargs={\"key\": \"value\"} ) log_message = action.to_logging_msg() print(log_message)  # Output: \"ParentNode CREATED ChildNode\"</p>"},{"location":"api_reference/components/logging_actions/#12-logging-request-completion","title":"1.2 Logging Request Completion","text":"<p>The <code>RequestSuccessAction</code> and <code>RequestFailureAction</code> classes are used to log the completion of requests, whether successful or failed. This is crucial for understanding the outcomes of processes and handling errors appropriately.</p> <p>python from railtracks.utils.logging.action import RequestSuccessAction, RequestFailureAction</p>"},{"location":"api_reference/components/logging_actions/#for-a-successful-request","title":"For a successful request","text":"<p>success_action = RequestSuccessAction(node_name=\"ChildNode\", output=\"result\") success_log_message = success_action.to_logging_msg() print(success_log_message)  # Output: \"ChildNode DONE\"</p>"},{"location":"api_reference/components/logging_actions/#for-a-failed-request","title":"For a failed request","text":"<p>failure_action = RequestFailureAction(node_name=\"ChildNode\", exception=Exception(\"Error\")) failure_log_message = failure_action.to_logging_msg() print(failure_log_message)  # Output: \"ChildNode FAILED\"</p>"},{"location":"api_reference/components/logging_actions/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/logging_actions/#class-requestcreationactionrtaction","title":"<code>class RequestCreationAction(RTAction)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/logging_actions/#__init__self-parent_node_name-child_node_name-input_args-input_kwargs","title":"<code>.__init__(self, parent_node_name, child_node_name, input_args, input_kwargs)</code>","text":"<p>A simple object that encapsulates a Request Creation.</p> <p>Args:     parent_node_name (str): The name of the parent node that created this request.     child_node_name (str): The name of the child node that is being created.     input_args (Tuple[Any, ...]): The input arguments passed to the child node.     input_kwargs (Dict[str, Any]): The input keyword arguments passed to the child node.</p>"},{"location":"api_reference/components/logging_actions/#to_logging_msgself","title":"<code>.to_logging_msg(self)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/logging_actions/#class-requestsuccessactionrequestcompletionbase","title":"<code>class RequestSuccessAction(RequestCompletionBase)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/logging_actions/#__init__self-node_name-output","title":"<code>.__init__(self, node_name, output)</code>","text":"<p>\" A simple abstraction of a message when a request is successfully completed.</p> <p>Args:     node_name (str): The name of the child node that completed successfully.     output (Any): The output produced by the child node.</p>"},{"location":"api_reference/components/logging_actions/#to_logging_msgself_1","title":"<code>.to_logging_msg(self)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/logging_actions/#class-requestfailureactionrequestcompletionbase","title":"<code>class RequestFailureAction(RequestCompletionBase)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/logging_actions/#__init__self-node_name-exception","title":"<code>.__init__(self, node_name, exception)</code>","text":"<p>A simple abstraction of a message when a request fails.         Args:     node_name (str): The name of the child node that failed.     exception (Exception): The exception that was raised during the request.</p>"},{"location":"api_reference/components/logging_actions/#to_logging_msgself_2","title":"<code>.to_logging_msg(self)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/logging_actions/#def-arg_kwarg_logging_strargs-kwargs","title":"<code>def arg_kwarg_logging_str(args, kwargs)</code>","text":"<p>A helper function which converts the input args and kwargs into a string for pretty logging.</p>"},{"location":"api_reference/components/logging_actions/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Logging Actions component is designed with a focus on extensibility and clarity. It uses an abstract base class <code>RTAction</code> to define a common interface for all logging actions, ensuring that each action can be converted into a log message. The component is structured to handle different stages of a request's lifecycle, from creation to completion, with specific classes for success and failure scenarios.</p>"},{"location":"api_reference/components/logging_actions/#31-core-design-principles","title":"3.1 Core Design Principles","text":"<ul> <li>Abstraction and Extensibility: The use of an abstract base class (<code>RTAction</code>) allows for easy extension of logging actions. New types of actions can be added by subclassing <code>RTAction</code> and implementing the <code>to_logging_msg</code> method.</li> <li>Separation of Concerns: Each class is responsible for a specific type of logging action, ensuring that the logic for creating log messages is encapsulated within the relevant class.</li> <li>Simplicity and Clarity: The design prioritizes simplicity, with each class having a clear and singular purpose. This makes the component easy to understand and maintain.</li> </ul>"},{"location":"api_reference/components/logging_actions/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/logging_actions/#41-implementation-details","title":"4.1 Implementation Details","text":"<ul> <li>Logging Format: The <code>to_logging_msg</code> method in each class defines the format of the log message. This format should be consistent across different actions to facilitate easy parsing and analysis of logs.</li> <li>Error Handling: The <code>RequestFailureAction</code> class captures exceptions that occur during request processing. It is important to ensure that exceptions are meaningful and provide enough context for debugging.</li> </ul>"},{"location":"api_reference/components/logging_actions/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/logging_actions/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>action.py</code>: Contains the implementation of the Logging Actions component.</li> </ul>"},{"location":"api_reference/components/logging_actions/#52-related-feature-files","title":"5.2 Related Feature Files","text":"<ul> <li><code>logging_profiling.md</code>: Provides additional context and documentation related to logging and profiling features within the system.</li> </ul>"},{"location":"api_reference/components/logging_actions/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/logging_configuration/","title":"Logging Configuration","text":"<p>The Logging Configuration component is responsible for setting up and managing logging for the Railtracks application, providing different logging levels and formats to facilitate effective monitoring and debugging.</p> <p>Version: 0.0.1</p> <p>Component Contact: @railtracks-dev</p>"},{"location":"api_reference/components/logging_configuration/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/logging_configuration/#1-purpose","title":"1. Purpose","text":"<p>The Logging Configuration component is designed to provide a flexible and comprehensive logging system for the Railtracks application. It allows developers to configure logging levels, formats, and handlers to suit different environments and use cases, such as development, testing, and production.</p>"},{"location":"api_reference/components/logging_configuration/#11-configuring-logging-levels","title":"1.1 Configuring Logging Levels","text":"<p>The component supports four logging levels:</p> <ul> <li>VERBOSE: Logs all messages, including debug information.</li> <li>REGULAR: Logs informational messages and above.</li> <li>QUIET: Logs warnings and above.</li> <li>NONE: Disables all logging.</li> </ul> <p>python import railtracks as rt</p>"},{"location":"api_reference/components/logging_configuration/#set-logging-to-verbose","title":"Set logging to verbose","text":"<p>rt.ExecutorConfig(logging_setting=\"VERBOSE\")</p>"},{"location":"api_reference/components/logging_configuration/#set-logging-to-regular","title":"Set logging to regular","text":"<p>rt.ExecutorConfig(logging_setting=\"REGULAR\")</p>"},{"location":"api_reference/components/logging_configuration/#set-logging-to-quiet","title":"Set logging to quiet","text":"<p>rt.ExecutorConfig(logging_setting=\"QUIET\")</p>"},{"location":"api_reference/components/logging_configuration/#disable-logging","title":"Disable logging","text":"<p>rt.ExecutorConfig(logging_setting=\"NONE\")</p>"},{"location":"api_reference/components/logging_configuration/#12-file-logging","title":"1.2 File Logging","text":"<p>Logs can be saved to a file by specifying a file path in the configuration.</p> <p>python import railtracks as rt</p>"},{"location":"api_reference/components/logging_configuration/#save-logs-to-a-file","title":"Save logs to a file","text":"<p>rt.ExecutorConfig(log_file=\"my_logs.log\")</p>"},{"location":"api_reference/components/logging_configuration/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/logging_configuration/#class-colorfulformatterloggingformatter","title":"<code>class ColorfulFormatter(logging.Formatter)</code>","text":"<p>A simple formatter that can be used to format log messages with colours based on the log level and specific keywords.</p>"},{"location":"api_reference/components/logging_configuration/#__init__self-fmt-datefmt","title":"<code>.__init__(self, fmt, datefmt)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/logging_configuration/#formatself-record","title":"<code>.format(self, record)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/logging_configuration/#def-setup_verbose_logger_config","title":"<code>def setup_verbose_logger_config()</code>","text":"<p>Sets up the logger configuration in verbose mode.</p> <p>Specifically that means: - The console will log all messages (including debug)</p>"},{"location":"api_reference/components/logging_configuration/#def-setup_regular_logger_config","title":"<code>def setup_regular_logger_config()</code>","text":"<p>Setups the logger in the regular mode. This mode will print all messages except debug messages to the console.</p>"},{"location":"api_reference/components/logging_configuration/#def-setup_quiet_logger_config","title":"<code>def setup_quiet_logger_config()</code>","text":"<p>Set up the logger to only log warning and above messages.</p>"},{"location":"api_reference/components/logging_configuration/#def-setup_none_logger_config","title":"<code>def setup_none_logger_config()</code>","text":"<p>Set up the logger to print nothing. This can be a useful optimization technique.</p>"},{"location":"api_reference/components/logging_configuration/#def-setup_file_handler","title":"<code>def setup_file_handler()</code>","text":"<p>Setups a logger file handler that will log messages to a file with the given name and logging level.</p>"},{"location":"api_reference/components/logging_configuration/#def-prepare_logger","title":"<code>def prepare_logger()</code>","text":"<p>Prepares the logger based on the setting and optionally sets up the file handler if a path is provided.</p>"},{"location":"api_reference/components/logging_configuration/#def-detach_logging_handlers","title":"<code>def detach_logging_handlers()</code>","text":"<p>Shuts down the logging system and detaches all logging handlers.</p>"},{"location":"api_reference/components/logging_configuration/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Logging Configuration component is built around the Python <code>logging</code> module, with enhancements for color-coded output and flexible configuration. It uses a custom <code>ColorfulFormatter</code> to apply colors to log messages based on their level and specific keywords.</p>"},{"location":"api_reference/components/logging_configuration/#31-design-considerations","title":"3.1 Design Considerations","text":"<ul> <li>ColorfulFormatter: This class extends <code>logging.Formatter</code> to add color to log messages. It uses the <code>colorama</code> library to apply colors based on log levels and keywords.</li> <li>Logging Levels: The component provides functions to set up different logging configurations (<code>VERBOSE</code>, <code>REGULAR</code>, <code>QUIET</code>, <code>NONE</code>) using stream handlers.</li> <li>File Logging: The <code>setup_file_handler</code> function allows logs to be written to a file, with a default format that includes timestamps.</li> <li>Custom Handlers: Users can attach custom handlers to the logger for additional functionality, such as forwarding logs to external services.</li> </ul>"},{"location":"api_reference/components/logging_configuration/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/logging_configuration/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>Colorama: The <code>colorama</code> library is used for coloring log messages. It is initialized with <code>autoreset=True</code> to ensure colors do not bleed into other outputs.</li> <li>Logging Configuration: The logging configuration should be set up at the start of the application to ensure all components log correctly.</li> </ul>"},{"location":"api_reference/components/logging_configuration/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>Logging Overhead: Enabling verbose logging can introduce performance overhead due to the large volume of log messages. It is recommended to use this level only during development or troubleshooting.</li> </ul>"},{"location":"api_reference/components/logging_configuration/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/logging_configuration/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>config.py</code>: Contains the implementation of the logging configuration component.</li> </ul>"},{"location":"api_reference/components/logging_configuration/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>logging.md</code>: Provides additional context and examples for configuring and using logging in Railtracks.</li> </ul>"},{"location":"api_reference/components/logging_configuration/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (2023-10-01) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/mcp_client_server/","title":"MCP Client and Server","text":"<p>The MCP Client and Server component implements an asynchronous client and server for interacting with an MCP server using stdio or HTTP streaming. It facilitates seamless communication with Model Context Protocol (MCP) servers, enabling the integration of external tools and services into the RailTracks framework.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/mcp_client_server/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/mcp_client_server/#1-purpose","title":"1. Purpose","text":"<p>The MCP Client and Server component is designed to facilitate the integration of external tools and services into the RailTracks framework using the Model Context Protocol (MCP). It provides an asynchronous client and server setup to communicate with MCP servers via stdio or HTTP streaming, allowing developers to leverage a standardized interface for tool integration.</p>"},{"location":"api_reference/components/mcp_client_server/#11-connecting-to-an-mcp-server","title":"1.1 Connecting to an MCP Server","text":"<p>This use case demonstrates how to connect to an MCP server using the <code>MCPAsyncClient</code> class. It is crucial for initializing communication with the server and accessing available tools.</p> <p>python from railtracks.rt_mcp import MCPHttpParams, MCPAsyncClient</p> <p>config = MCPHttpParams(url=\"https://remote.mcpservers.org/fetch/mcp\") client = MCPAsyncClient(config=config) await client.connect()</p>"},{"location":"api_reference/components/mcp_client_server/#12-listing-available-tools","title":"1.2 Listing Available Tools","text":"<p>Once connected to an MCP server, you can list all available tools using the <code>list_tools</code> method. This is important for understanding the capabilities provided by the server.</p> <p>python tools = await client.list_tools()</p>"},{"location":"api_reference/components/mcp_client_server/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/mcp_client_server/#3-architectural-design","title":"3. Architectural Design","text":"<p>The MCP Client and Server component is designed to provide a flexible and efficient way to interact with MCP servers. It leverages asynchronous programming to handle communication, ensuring non-blocking operations and efficient resource utilization.</p>"},{"location":"api_reference/components/mcp_client_server/#31-core-design-principles","title":"3.1 Core Design Principles","text":"<ul> <li>Asynchronous Communication: Utilizes Python's <code>asyncio</code> library to manage asynchronous operations, allowing for non-blocking communication with MCP servers.</li> <li>Standardized Integration: Follows the Model Context Protocol (MCP) to ensure compatibility with a wide range of tools and services.</li> <li>Modular Design: Separates client and server functionalities into distinct classes (<code>MCPAsyncClient</code> and <code>MCPServer</code>) to promote modularity and ease of maintenance.</li> </ul>"},{"location":"api_reference/components/mcp_client_server/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/mcp_client_server/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>Ensure that the MCP server URL is correctly configured in the <code>MCPHttpParams</code>.</li> <li>The component relies on the <code>mcp</code> package for client session management and communication protocols.</li> </ul>"},{"location":"api_reference/components/mcp_client_server/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>The component is designed for asynchronous operations; ensure that the event loop is properly managed to avoid blocking.</li> <li>The <code>MCPAsyncClient</code> caches tool information to reduce redundant requests, improving performance.</li> </ul>"},{"location":"api_reference/components/mcp_client_server/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/mcp_client_server/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>main.py</code>: Contains the implementation of the MCP Client and Server component.</li> </ul>"},{"location":"api_reference/components/mcp_client_server/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li>MCP Documentation: Provides an overview of the Model Context Protocol and its integration with RailTracks.</li> </ul>"},{"location":"api_reference/components/mcp_client_server/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/mcp_tool_connection/","title":"MCP Tool Connection","text":"<p>The MCP Tool Connection component provides a function to establish a connection to an MCP server using specified configuration parameters. It is a crucial part of the system that facilitates communication with the MCP server, enabling the retrieval and management of tools.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/mcp_tool_connection/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/mcp_tool_connection/#1-purpose","title":"1. Purpose","text":"<p>The primary purpose of the MCP Tool Connection component is to establish and manage a connection to an MCP server. This connection allows for the fetching and utilization of tools hosted on the server, which are essential for various operations within the system.</p>"},{"location":"api_reference/components/mcp_tool_connection/#11-establishing-a-connection","title":"1.1 Establishing a Connection","text":"<p>The component provides a straightforward interface to connect to an MCP server using either standard input/output parameters or HTTP parameters. This flexibility is important for accommodating different server configurations and communication protocols.</p> <p>python from mcp_tool import connect_mcp from mcp import ClientSession from main import MCPHttpParams, MCPStdioParams</p>"},{"location":"api_reference/components/mcp_tool_connection/#example-of-connecting-using-http-parameters","title":"Example of connecting using HTTP parameters","text":"<p>http_config = MCPHttpParams(url=\"http://example.com\", token=\"your_token\") server = connect_mcp(config=http_config)</p>"},{"location":"api_reference/components/mcp_tool_connection/#example-of-connecting-using-stdio-parameters","title":"Example of connecting using Stdio parameters","text":"<p>stdio_config = MCPStdioParams(stdin=\"input\", stdout=\"output\") server = connect_mcp(config=stdio_config)</p>"},{"location":"api_reference/components/mcp_tool_connection/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/mcp_tool_connection/#def-connect_mcpconfig-client_session","title":"<code>def connect_mcp(config, client_session)</code>","text":"<p>Returns an MCPServer class. On creation, it will connect to the MCP server and fetch the tools. The connection will remain open until the server is closed with <code>close()</code>.</p> <p>Args:     config: Configuration for the MCP server, either as StdioServerParameters or MCPHttpParams.     client_session: Optional ClientSession to use for the MCP server connection. If not provided, a new session will be created.</p> <p>Returns:     MCPServer: An instance of the MCPServer class.</p>"},{"location":"api_reference/components/mcp_tool_connection/#3-architectural-design","title":"3. Architectural Design","text":"<p>The design of the MCP Tool Connection component is centered around flexibility and ease of use. It leverages the <code>MCPServer</code> class to manage the server connection and tool retrieval process.</p>"},{"location":"api_reference/components/mcp_tool_connection/#31-connection-management","title":"3.1 Connection Management","text":"<ul> <li>MCPServer Class: This class is responsible for maintaining the connection to the MCP server. It is designed to keep the connection open until explicitly closed by the user, ensuring persistent communication.</li> <li>Configuration Flexibility: The component supports both <code>MCPStdioParams</code> and <code>MCPHttpParams</code> for configuration, allowing it to adapt to different server setups and communication needs.</li> </ul>"},{"location":"api_reference/components/mcp_tool_connection/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/mcp_tool_connection/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>ClientSession: The component optionally uses a <code>ClientSession</code> from the <code>mcp</code> module to manage HTTP connections. If not provided, a new session is created internally.</li> <li>Jupyter Compatibility: The component applies Jupyter compatibility patches using the <code>apply_patches</code> function, ensuring smooth operation within Jupyter environments.</li> </ul>"},{"location":"api_reference/components/mcp_tool_connection/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>Persistent Connection: The connection remains open until the <code>MCPServer</code> is closed, which may have implications for resource usage and should be managed appropriately.</li> </ul>"},{"location":"api_reference/components/mcp_tool_connection/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/mcp_tool_connection/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>mcp_tool.py</code>: Contains the implementation of the <code>connect_mcp</code> function and related logic.</li> </ul>"},{"location":"api_reference/components/mcp_tool_connection/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>mcp_client_server.md</code>: Provides documentation on the MCP client-server architecture and its integration with this component.</li> </ul>"},{"location":"api_reference/components/mcp_tool_connection/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>mcp_integration.md</code>: Details the integration of MCP tools within the broader system, including usage scenarios and configuration.</li> </ul>"},{"location":"api_reference/components/mcp_tool_connection/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/model_error_handling/","title":"Model Error Handling","text":"<p>This component defines custom error classes for handling exceptions related to language models, including unsupported features.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/model_error_handling/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/model_error_handling/#1-purpose","title":"1. Purpose","text":"<p>The primary purpose of this component is to provide a structured way to handle errors specific to Large Language Models (LLMs). It includes custom exceptions that can be used to signal specific issues, such as unsupported features, in a clear and consistent manner.</p>"},{"location":"api_reference/components/model_error_handling/#11-handling-general-model-errors","title":"1.1 Handling General Model Errors","text":"<p>The <code>ModelError</code> class is used to represent any error related to LLMs. It provides a mechanism to include a reason for the error and optionally, a history of messages leading up to the error.</p> <p>python from railtracks.llm.models._model_exception_base import ModelError from railtracks.llm.history import MessageHistory</p>"},{"location":"api_reference/components/model_error_handling/#example-usage","title":"Example usage","text":"<p>try:     raise ModelError(\"An unexpected error occurred\", MessageHistory()) except ModelError as e:     print(e)</p>"},{"location":"api_reference/components/model_error_handling/#12-handling-unsupported-function-calling","title":"1.2 Handling Unsupported Function Calling","text":"<p>The <code>FunctionCallingNotSupportedError</code> is a specific type of <code>ModelError</code> that is raised when a model does not support function calling, which is crucial for certain operations.</p> <p>python from railtracks.llm.models._model_exception_base import FunctionCallingNotSupportedError</p>"},{"location":"api_reference/components/model_error_handling/#example-usage_1","title":"Example usage","text":"<p>try:     raise FunctionCallingNotSupportedError(\"ExampleModel\") except FunctionCallingNotSupportedError as e:     print(e)</p>"},{"location":"api_reference/components/model_error_handling/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/model_error_handling/#class-modelerrorrtllmerror","title":"<code>class ModelError(RTLLMError)</code>","text":"<p>Any Large Language Model (LLM) error.</p>"},{"location":"api_reference/components/model_error_handling/#__init__self-reason-message_history","title":"<code>.__init__(self, reason, message_history)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/model_error_handling/#class-functioncallingnotsupportederrormodelerror","title":"<code>class FunctionCallingNotSupportedError(ModelError)</code>","text":"<p>Error raised when a model does not support function calling.</p>"},{"location":"api_reference/components/model_error_handling/#__init__self-model_name","title":"<code>.__init__(self, model_name)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/model_error_handling/#3-architectural-design","title":"3. Architectural Design","text":""},{"location":"api_reference/components/model_error_handling/#31-error-handling-design","title":"3.1 Error Handling Design","text":"<ul> <li>ModelError Class:</li> <li>Inherits from <code>RTLLMError</code>, a base class for all LLM-related exceptions.</li> <li>Designed to encapsulate a reason for the error and optionally, a <code>MessageHistory</code> object.</li> <li> <p>Utilizes ANSI color codes for enhanced terminal output readability.</p> </li> <li> <p>FunctionCallingNotSupportedError Class:</p> </li> <li>Inherits from <code>ModelError</code>.</li> <li>Specifically used to indicate that a model does not support function calling.</li> <li>Provides a clear error message indicating the model's limitation.</li> </ul>"},{"location":"api_reference/components/model_error_handling/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/model_error_handling/#41-implementation-details","title":"4.1 Implementation Details","text":"<ul> <li>Colorized Output:</li> <li> <p>The <code>RTLLMError</code> class provides a <code>_color</code> method to apply ANSI color codes to text, enhancing readability in terminal outputs.</p> </li> <li> <p>Message History:</p> </li> <li>The <code>MessageHistory</code> class, which is a list of <code>Message</code> objects, can be used to track the sequence of messages leading to an error. This is useful for debugging and understanding the context of errors.</li> </ul>"},{"location":"api_reference/components/model_error_handling/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/model_error_handling/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>../_model_exception_base.py</code>: Defines the <code>ModelError</code> and <code>FunctionCallingNotSupportedError</code> classes.</li> <li><code>../_exception_base.py</code>: Contains the <code>RTLLMError</code> base class, which provides foundational error handling capabilities.</li> <li><code>../history.py</code>: Implements the <code>MessageHistory</code> class, used for tracking message sequences.</li> </ul>"},{"location":"api_reference/components/model_error_handling/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>../components/exception_handling.md</code>: Provides broader context on exception handling strategies within the project.</li> </ul>"},{"location":"api_reference/components/model_error_handling/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>../features/llm_integration.md</code>: Discusses the integration of LLMs and how error handling is applied in that context.</li> </ul>"},{"location":"api_reference/components/model_error_handling/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/model_interaction/","title":"Model Interaction","text":"<p>The Model Interaction component defines interfaces and hooks for interacting with models, supporting chat, structured interactions, and streaming. It provides a flexible framework for integrating various model types and handling interactions in a structured manner.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/model_interaction/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/model_interaction/#1-purpose","title":"1. Purpose","text":"<p>The Model Interaction component is designed to facilitate communication with language models through various interaction types such as chat, structured interactions, and streaming. It allows developers to insert hooks for pre-processing messages, post-processing responses, and handling exceptions, providing a customizable and extensible interface for model interactions.</p>"},{"location":"api_reference/components/model_interaction/#11-chat-interaction","title":"1.1 Chat Interaction","text":"<p>Chat interaction allows for real-time communication with a model using a sequence of messages.</p> <p>python from railtracks.llm.model import ModelBase from railtracks.llm.history import MessageHistory</p> <p>class MyModel(ModelBase):     def model_name(self) -&gt; str:         return \"MyModel\"</p> <pre><code>@classmethod\ndef model_type(cls) -&gt; str:\n    return \"CustomModel\"\n\ndef _chat(self, messages: MessageHistory, **kwargs):\n    # Implement chat logic here\n    pass\n</code></pre>"},{"location":"api_reference/components/model_interaction/#usage","title":"Usage","text":"<p>model = MyModel() response = model.chat(messages=MessageHistory())</p>"},{"location":"api_reference/components/model_interaction/#12-structured-interaction","title":"1.2 Structured Interaction","text":"<p>Structured interaction involves using a predefined schema to interact with the model, ensuring that the output adheres to a specific format.</p> <p>python from pydantic import BaseModel</p> <p>class OutputSchema(BaseModel):     field1: str     field2: int</p> <p>response = model.structured(messages=MessageHistory(), schema=OutputSchema)</p>"},{"location":"api_reference/components/model_interaction/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/model_interaction/#class-modelbaseabc","title":"<code>class ModelBase(ABC)</code>","text":"<p>A simple base that represents the behavior of a model that can be used for chat, structured interactions, and streaming.</p> <p>The base class allows for the insertion of hooks that can modify the messages before they are sent to the model, response after they are received, and map exceptions that may occur during the interaction.</p> <p>All the hooks are optional and can be added or removed as needed.</p>"},{"location":"api_reference/components/model_interaction/#__init__self-pre_hook-post_hook-exception_hook","title":"<code>.__init__(self, pre_hook, post_hook, exception_hook)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/model_interaction/#add_pre_hookself-hook","title":"<code>.add_pre_hook(self, hook)</code>","text":"<p>Adds a pre-hook to modify messages before sending them to the model.</p>"},{"location":"api_reference/components/model_interaction/#add_post_hookself-hook","title":"<code>.add_post_hook(self, hook)</code>","text":"<p>Adds a post-hook to modify the response after receiving it from the model.</p>"},{"location":"api_reference/components/model_interaction/#add_exception_hookself-hook","title":"<code>.add_exception_hook(self, hook)</code>","text":"<p>Adds an exception hook to handle exceptions during model interactions.</p>"},{"location":"api_reference/components/model_interaction/#remove_pre_hooksself","title":"<code>.remove_pre_hooks(self)</code>","text":"<p>Removes all of the hooks that modify messages before sending them to the model.</p>"},{"location":"api_reference/components/model_interaction/#remove_post_hooksself","title":"<code>.remove_post_hooks(self)</code>","text":"<p>Removes all of the hooks that modify the response after receiving it from the model.</p>"},{"location":"api_reference/components/model_interaction/#remove_exception_hooksself","title":"<code>.remove_exception_hooks(self)</code>","text":"<p>Removes all of the hooks that handle exceptions during model interactions.</p>"},{"location":"api_reference/components/model_interaction/#model_nameself","title":"<code>.model_name(self)</code>","text":"<p>Returns the name of the model being used.</p> <p>It can be treated as unique identifier for the model when paired with the <code>model_type</code>.</p>"},{"location":"api_reference/components/model_interaction/#model_typecls","title":"<code>.model_type(cls)</code>","text":"<p>The name of the provider of this model or the model type.</p>"},{"location":"api_reference/components/model_interaction/#chatself-messages-kwargs","title":"<code>.chat(self, messages, **kwargs)</code>","text":"<p>Chat with the model using the provided messages.</p>"},{"location":"api_reference/components/model_interaction/#achatself-messages-kwargs","title":"<code>.achat(self, messages, **kwargs)</code>","text":"<p>Asynchronous chat with the model using the provided messages.</p>"},{"location":"api_reference/components/model_interaction/#structuredself-messages-schema-kwargs","title":"<code>.structured(self, messages, schema, **kwargs)</code>","text":"<p>Structured interaction with the model using the provided messages and output_schema.</p>"},{"location":"api_reference/components/model_interaction/#astructuredself-messages-schema-kwargs","title":"<code>.astructured(self, messages, schema, **kwargs)</code>","text":"<p>Asynchronous structured interaction with the model using the provided messages and output_schema.</p>"},{"location":"api_reference/components/model_interaction/#stream_chatself-messages-kwargs","title":"<code>.stream_chat(self, messages, **kwargs)</code>","text":"<p>Stream chat with the model using the provided messages.</p>"},{"location":"api_reference/components/model_interaction/#astream_chatself-messages-kwargs","title":"<code>.astream_chat(self, messages, **kwargs)</code>","text":"<p>Asynchronous stream chat with the model using the provided messages.</p>"},{"location":"api_reference/components/model_interaction/#chat_with_toolsself-messages-tools-kwargs","title":"<code>.chat_with_tools(self, messages, tools, **kwargs)</code>","text":"<p>Chat with the model using the provided messages and tools.</p>"},{"location":"api_reference/components/model_interaction/#achat_with_toolsself-messages-tools-kwargs","title":"<code>.achat_with_tools(self, messages, tools, **kwargs)</code>","text":"<p>Asynchronous chat with the model using the provided messages and tools.</p>"},{"location":"api_reference/components/model_interaction/#class-response","title":"<code>class Response</code>","text":"<p>A simple object that represents a response from a model. It includes specific detail about the returned message and any other additional information from the model.</p>"},{"location":"api_reference/components/model_interaction/#__init__self-message-streamer-message_info","title":"<code>.__init__(self, message, streamer, message_info)</code>","text":"<p>Creates a new instance of a response object.</p> <p>Args:     message: The message that was returned as part of this.     streamer: A generator that streams the response as a collection of chunked strings.     message_info: Additional information about the message, such as input/output tokens and latency.</p>"},{"location":"api_reference/components/model_interaction/#messageself","title":"<code>.message(self)</code>","text":"<p>Gets the message that was returned as part of this response.</p> <p>If none exists, this will return None.</p>"},{"location":"api_reference/components/model_interaction/#streamerself","title":"<code>.streamer(self)</code>","text":"<p>Gets the streamer that was returned as part of this response.</p> <p>This object will only be filled in the case when you asked for a streamed response.</p> <p>If none exists, this will return None.</p>"},{"location":"api_reference/components/model_interaction/#message_infoself","title":"<code>.message_info(self)</code>","text":"<p>Gets the message info that was returned as part of this response.</p> <p>This object contains additional information about the message, such as input/output tokens and latency.</p>"},{"location":"api_reference/components/model_interaction/#class-typemapper","title":"<code>class TypeMapper</code>","text":"<p>A simple type that will provide functionality to convert a dictionary representation of kwargs into the appropriate types based on the function signature</p> <p>Use the method <code>convert_kwargs_to_appropriate_types</code> to convert the kwargs dictionary.</p>"},{"location":"api_reference/components/model_interaction/#__init__self-function","title":"<code>.__init__(self, function)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/model_interaction/#convert_kwargs_to_appropriate_typesself-kwargs","title":"<code>.convert_kwargs_to_appropriate_types(self, kwargs)</code>","text":"<p>Convert kwargs to appropriate types based on function signature.</p>"},{"location":"api_reference/components/model_interaction/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Model Interaction component is built around the <code>ModelBase</code> class, which serves as an abstract base class for implementing various model interaction types. It provides methods for synchronous and asynchronous interactions, including chat, structured interactions, and streaming.</p>"},{"location":"api_reference/components/model_interaction/#31-core-philosophy-design-principles","title":"3.1 Core Philosophy &amp; Design Principles","text":"<ul> <li>Extensibility: The component is designed to be easily extended with custom models by subclassing <code>ModelBase</code>.</li> <li>Hook Mechanism: Pre-hooks, post-hooks, and exception hooks allow for flexible customization of the interaction process.</li> </ul>"},{"location":"api_reference/components/model_interaction/#32-high-level-architecture-data-flow","title":"3.2 High-Level Architecture &amp; Data Flow","text":"<p>The component uses a hook-based architecture to process messages and responses. Hooks can be added or removed dynamically, allowing for pre-processing of messages, post-processing of responses, and handling of exceptions.</p>"},{"location":"api_reference/components/model_interaction/#33-key-design-decisions-trade-offs","title":"3.3 Key Design Decisions &amp; Trade-offs","text":"<ul> <li>Hook Flexibility: The use of hooks provides flexibility but may introduce complexity in managing the order and interaction of hooks.</li> <li>Abstract Methods: The use of abstract methods in <code>ModelBase</code> enforces implementation of core interaction logic in subclasses.</li> </ul>"},{"location":"api_reference/components/model_interaction/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/model_interaction/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>Pydantic: Used for schema validation in structured interactions.</li> <li>MessageHistory: Custom object representing the sequence of messages in an interaction.</li> </ul>"},{"location":"api_reference/components/model_interaction/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>Concurrency: Asynchronous methods are provided for non-blocking interactions, but care must be taken to manage concurrency and state.</li> </ul>"},{"location":"api_reference/components/model_interaction/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/model_interaction/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>model.py</code>: Defines the <code>ModelBase</code> class and interaction methods.</li> <li><code>response.py</code>: Defines the <code>Response</code> class for handling model responses.</li> <li><code>type_mapping.py</code>: Provides functionality for converting dictionary representations to appropriate types.</li> </ul>"},{"location":"api_reference/components/model_interaction/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>llm_messaging.md</code>: Documentation for the messaging component related to model interactions.</li> </ul>"},{"location":"api_reference/components/model_interaction/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/node_building/","title":"Node Building","text":"<p>The Node Building component is a flexible utility designed to facilitate the dynamic creation of node subclasses with custom configurations. It supports interactions with Language Learning Models (LLMs) and tools, allowing developers to programmatically construct new node classes with specific overrides and configurations.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/node_building/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/node_building/#1-purpose","title":"1. Purpose","text":"<p>The Node Building component is primarily used to create customized node subclasses within the railtracks framework. This is particularly useful for scenarios where small modifications to existing classes like ToolCalling, Structured, or Terminal LLMs are needed. The component allows for the overriding of methods and attributes such as pretty names, tool details, parameters, and LLM configurations.</p>"},{"location":"api_reference/components/node_building/#11-dynamic-node-creation","title":"1.1 Dynamic Node Creation","text":"<p>The primary use case for the Node Building component is the dynamic creation of node subclasses. This is important for developers who need to extend the functionality of existing node classes without modifying the original class definitions.</p> <p>python from railtracks.nodes import NodeBuilder, Node</p> <p>class CustomNode(Node):     pass</p> <p>builder = NodeBuilder(CustomNode, name=\"Custom Node\", class_name=\"DynamicCustomNode\") custom_node_class = builder.build()</p>"},{"location":"api_reference/components/node_building/#12-llm-configuration","title":"1.2 LLM Configuration","text":"<p>Another significant use case is configuring nodes to interact with specific LLM models and system messages. This is crucial for applications that require tailored LLM interactions.</p> <p>python from railtracks.llm import ModelBase, SystemMessage</p> <p>builder.llm_base(llm_model=ModelBase(), system_message=\"Custom system message\")</p>"},{"location":"api_reference/components/node_building/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/node_building/#3-architectural-design","title":"3. Architectural Design","text":""},{"location":"api_reference/components/node_building/#31-nodebuilder-class","title":"3.1 NodeBuilder Class","text":"<ul> <li>Design Consideration: The <code>NodeBuilder</code> class is designed to be generic, allowing it to work with any subclass of <code>Node</code>. This flexibility is achieved through the use of Python's generics and type variables.</li> <li>Logic Flow: The <code>NodeBuilder</code> class maintains a dictionary of method overrides, which are applied to the dynamically created node subclass. This allows for the customization of node behavior without altering the original class definitions.</li> <li>Core Philosophy: The component adheres to the principle of separation of concerns by isolating the node creation logic from the node's operational logic.</li> </ul>"},{"location":"api_reference/components/node_building/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/node_building/#41-method-overrides","title":"4.1 Method Overrides","text":"<ul> <li>Non-Obvious Detail: When overriding methods using the <code>NodeBuilder</code>, existing methods with the same name will be replaced. This can lead to unexpected behavior if not managed carefully.</li> <li>Critical Detail: The <code>build</code> method constructs the final node subclass, applying all configured overrides. It is essential to ensure that all necessary configurations are set before calling this method.</li> </ul>"},{"location":"api_reference/components/node_building/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/node_building/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>../_node_builder.py</code>: Contains the implementation of the NodeBuilder class and its methods.</li> </ul>"},{"location":"api_reference/components/node_building/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li>Node Interaction Documentation: Explains how nodes interact within the railtracks framework.</li> </ul>"},{"location":"api_reference/components/node_building/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li>Node Management Documentation: Details the management and lifecycle of nodes within the system.</li> </ul>"},{"location":"api_reference/components/node_building/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/node_creation_validation/","title":"Node Creation Validation","text":"<p>The Node Creation Validation component provides a set of validation functions to ensure the integrity and correctness of nodes, methods, and tool metadata within the Railtracks project. This component is crucial for maintaining the robustness and reliability of the system by preventing invalid configurations and structures from being used.</p> <p>Version: 0.0.1</p> <p>Component Contact: @github_username</p>"},{"location":"api_reference/components/node_creation_validation/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/node_creation_validation/#1-purpose","title":"1. Purpose","text":"<p>The Node Creation Validation component is designed to validate various aspects of node creation and tool metadata. It ensures that nodes and tools are configured correctly before they are used in the system, preventing runtime errors and maintaining system integrity.</p>"},{"location":"api_reference/components/node_creation_validation/#11-validate-function","title":"1.1 Validate Function","text":"<p>The <code>validate_function</code> use case ensures that a function is safe to use in a node by checking for any <code>dict</code> or <code>Dict</code> parameters, including nested structures. This is important to prevent unexpected behavior due to mutable data structures.</p> <p>python def example_function(param1: int, param2: dict):     pass</p> <p>try:     validate_function(example_function) except NodeCreationError as e:     print(e)</p>"},{"location":"api_reference/components/node_creation_validation/#12-validate-tool-metadata","title":"1.2 Validate Tool Metadata","text":"<p>The <code>validate_tool_metadata</code> use case runs multiple checks on tool metadata, such as ensuring unique parameter names and valid system messages. This is crucial for maintaining consistent and error-free tool configurations.</p> <p>python tool_params = [{'name': 'param1'}, {'name': 'param2'}] tool_details = {'description': 'A sample tool'} system_message = SystemMessage(content=\"Sample message\")</p> <p>try:     validate_tool_metadata(tool_params, tool_details, system_message, \"Sample Tool\") except NodeCreationError as e:     print(e)</p>"},{"location":"api_reference/components/node_creation_validation/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/node_creation_validation/#def-validate_functionfunc","title":"<code>def validate_function(func)</code>","text":"<p>Validate that the function is safe to use in a node. If there are any dict or Dict parameters, raise an error. Also checks recursively for any nested dictionary structures, including inside BaseModels.</p> <p>Args:     func: The function to validate.</p> <p>Raises:     NodeCreationError: If the function has dict or Dict parameters, even nested.</p>"},{"location":"api_reference/components/node_creation_validation/#def-check_classmethodmethod-method_name","title":"<code>def check_classmethod(method, method_name)</code>","text":"<p>Ensure the given method is a classmethod.</p> <p>Args:     method: The method to check.     method_name: The name of the method (for error messages).</p> <p>Raises:     NodeCreationError: If the method is not a classmethod.</p>"},{"location":"api_reference/components/node_creation_validation/#def-check_connected_nodesnode_set-node","title":"<code>def check_connected_nodes(node_set, node)</code>","text":"<p>Validate that node_set is non-empty and contains only subclasses of Node or functions.</p> <p>Args:     node_set: The set of nodes to check.     node: The base Node class.</p> <p>Raises:     NodeCreationError: If node_set is empty or contains invalid types.</p>"},{"location":"api_reference/components/node_creation_validation/#def-check_schemamethod-cls","title":"<code>def check_schema(method, cls)</code>","text":"<p>Validate the output_schema returned by a classmethod.</p> <p>Args:     method: The classmethod to call.     cls: The class to pass to the method.</p> <p>Raises:     NodeCreationError: If the output_schema is missing, invalid, or empty.</p>"},{"location":"api_reference/components/node_creation_validation/#def-validate_tool_metadatatool_params-tool_details-system_message-pretty_name-max_tool_calls","title":"<code>def validate_tool_metadata(tool_params, tool_details, system_message, pretty_name, max_tool_calls)</code>","text":"<p>Run all tool metadata validation checks at once.</p> <p>Args:     tool_params: The tool parameters to check.     tool_details: The tool details object.     system_message: The system message to check.     pretty_name: The pretty name to check.     max_tool_calls: The maximum number of tool calls allowed.</p> <p>Raises:     NodeCreationError: If any validation fails.</p>"},{"location":"api_reference/components/node_creation_validation/#3-architectural-design","title":"3. Architectural Design","text":""},{"location":"api_reference/components/node_creation_validation/#31-validation-strategy","title":"3.1 Validation Strategy","text":"<ul> <li> <p>Function Validation: The <code>validate_function</code> method checks for the presence of <code>dict</code> or <code>Dict</code> parameters in function signatures, including nested structures. This design choice prevents the use of mutable data structures that could lead to unpredictable behavior.</p> </li> <li> <p>Tool Metadata Validation: The <code>validate_tool_metadata</code> function consolidates multiple validation checks into a single call, ensuring that all aspects of tool metadata are verified before use. This approach simplifies the validation process and reduces the risk of errors.</p> </li> <li> <p>Class Method Checks: The <code>check_classmethod</code> function ensures that specified methods are class methods, which is a requirement for certain operations within the system.</p> </li> </ul>"},{"location":"api_reference/components/node_creation_validation/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/node_creation_validation/#41-error-handling","title":"4.1 Error Handling","text":"<ul> <li> <p>NodeCreationError: This custom exception is raised during validation failures, providing detailed error messages and notes to assist in debugging. It is defined in the errors.py file.</p> </li> <li> <p>Exception Messages: The component uses a centralized system for managing exception messages, as defined in the exception_messages.py file. This ensures consistency and ease of maintenance.</p> </li> </ul>"},{"location":"api_reference/components/node_creation_validation/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/node_creation_validation/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>validation.py</code>: Contains the implementation of the validation functions.</li> </ul>"},{"location":"api_reference/components/node_creation_validation/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>errors.py</code>: Defines the <code>NodeCreationError</code> used in this component.</li> <li><code>exception_messages.py</code>: Manages exception messages used throughout the component.</li> </ul>"},{"location":"api_reference/components/node_creation_validation/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>validation.md</code>: Provides an overview of the validation feature, including its purpose and capabilities.</li> </ul>"},{"location":"api_reference/components/node_creation_validation/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/node_interaction/","title":"Node Interaction","text":"<p>The Node Interaction component facilitates node execution and interaction within the Railtracks framework, supporting batch processing, synchronous and asynchronous calls, and streaming. It is designed to handle various execution patterns and ensure efficient communication between nodes.</p> <p>Version: 0.0.1</p> <p>Component Contact: @github_username</p>"},{"location":"api_reference/components/node_interaction/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/node_interaction/#1-purpose","title":"1. Purpose","text":"<p>The Node Interaction component is essential for executing nodes in different modes, such as batch processing, synchronous, and asynchronous calls. It also supports streaming, allowing for real-time data processing and communication between nodes.</p>"},{"location":"api_reference/components/node_interaction/#11-batch-processing","title":"1.1 Batch Processing","text":"<p>Batch processing allows for parallel execution of nodes over multiple iterables, improving efficiency and performance.</p> <p>python results = await call_batch(NodeA, [\"hello world\"] * 10) for result in results:     handle(result)</p>"},{"location":"api_reference/components/node_interaction/#12-synchronous-and-asynchronous-calls","title":"1.2 Synchronous and Asynchronous Calls","text":"<p>This component supports both synchronous and asynchronous node execution, providing flexibility in how nodes are called and managed.</p> <p>python</p>"},{"location":"api_reference/components/node_interaction/#asynchronous-call","title":"Asynchronous call","text":"<p>result = await call(NodeA, \"hello world\", 42)</p>"},{"location":"api_reference/components/node_interaction/#synchronous-call","title":"Synchronous call","text":"<p>result = call_sync(NodeA, \"hello world\", 42)</p>"},{"location":"api_reference/components/node_interaction/#13-streaming","title":"1.3 Streaming","text":"<p>Streaming enables real-time data processing by broadcasting messages to other nodes.</p> <p>python await broadcast(\"streaming data\")</p>"},{"location":"api_reference/components/node_interaction/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/node_interaction/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Node Interaction component is designed to handle various execution patterns and ensure efficient communication between nodes. It leverages asynchronous programming to manage node execution and streaming effectively.</p>"},{"location":"api_reference/components/node_interaction/#31-execution-patterns","title":"3.1 Execution Patterns","text":"<ul> <li>Batch Processing: Utilizes <code>asyncio.gather</code> to execute nodes in parallel, returning results in the order of the input iterables.</li> <li>Synchronous and Asynchronous Calls: Supports both modes of execution, with asynchronous calls using <code>asyncio</code> and synchronous calls managing event loops internally.</li> <li>Streaming: Uses a publisher-subscriber model to broadcast messages, ensuring real-time data processing.</li> </ul>"},{"location":"api_reference/components/node_interaction/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/node_interaction/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>Ensure that the <code>railtracks</code> package is properly installed and configured.</li> <li>The component relies on the <code>asyncio</code> library for managing asynchronous operations.</li> </ul>"},{"location":"api_reference/components/node_interaction/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>Batch processing is efficient for large datasets but may require careful management of resources to avoid bottlenecks.</li> <li>Synchronous calls should not be used within an already running event loop to prevent runtime errors.</li> </ul>"},{"location":"api_reference/components/node_interaction/#43-state-management-concurrency","title":"4.3 State Management &amp; Concurrency","text":"<ul> <li>The component manages state using context objects, ensuring thread-safety and proper execution flow.</li> </ul>"},{"location":"api_reference/components/node_interaction/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/node_interaction/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>batch.py</code>: Handles batch processing of nodes.</li> <li><code>call.py</code>: Manages synchronous and asynchronous node calls.</li> <li><code>stream.py</code>: Implements streaming functionality.</li> </ul>"},{"location":"api_reference/components/node_interaction/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>task_execution.md</code>: Details the task execution component, which interacts with node execution.</li> </ul>"},{"location":"api_reference/components/node_interaction/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/node_invocation_validation/","title":"Node Invocation Validation","text":"<p>The Node Invocation Validation component provides utility functions for validating message history, language model, and tool call limits within the system. It ensures that the inputs to the system's nodes are correct and meet the expected criteria, thereby preventing execution errors and maintaining system integrity.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/node_invocation_validation/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/node_invocation_validation/#1-purpose","title":"1. Purpose","text":"<p>The Node Invocation Validation component is crucial for ensuring that the system's nodes receive valid inputs. It performs checks on message history, language model presence, and tool call limits, which are essential for the correct functioning of the system.</p>"},{"location":"api_reference/components/node_invocation_validation/#11-validate-message-history","title":"1.1 Validate Message History","text":"<p>The <code>check_message_history</code> function validates the message history to ensure that it contains valid <code>Message</code> objects and adheres to the expected structure.</p> <p>python from railtracks.llm import MessageHistory</p> <p>message_history = MessageHistory([...])  # Populate with Message objects check_message_history(message_history)</p>"},{"location":"api_reference/components/node_invocation_validation/#12-validate-language-model","title":"1.2 Validate Language Model","text":"<p>The <code>check_llm_model</code> function ensures that a valid language model is provided.</p> <p>python from railtracks.llm import ModelBase</p> <p>llm_model = ModelBase()  # Instantiate a valid model check_llm_model(llm_model)</p>"},{"location":"api_reference/components/node_invocation_validation/#13-validate-tool-call-limits","title":"1.3 Validate Tool Call Limits","text":"<p>The <code>check_max_tool_calls</code> function checks the maximum number of tool calls allowed, ensuring it is a non-negative integer.</p> <p>python max_tool_calls = 5 check_max_tool_calls(max_tool_calls)</p>"},{"location":"api_reference/components/node_invocation_validation/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/node_invocation_validation/#def-check_message_historymessage_history-system_message","title":"<code>def check_message_history(message_history, system_message)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/node_invocation_validation/#def-check_llm_modelllm_model","title":"<code>def check_llm_model(llm_model)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/node_invocation_validation/#def-check_max_tool_callsmax_tool_calls","title":"<code>def check_max_tool_calls(max_tool_calls)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/node_invocation_validation/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Node Invocation Validation component is designed to ensure the integrity and correctness of inputs to the system's nodes. It is built around the following principles:</p> <ul> <li>Input Validation: Ensures that all inputs to the nodes are valid and meet the expected criteria.</li> <li>Error Handling: Utilizes custom exceptions like <code>NodeInvocationError</code> to handle errors gracefully and provide meaningful feedback.</li> <li>Warning System: Issues warnings for non-fatal issues, allowing the system to continue operating while notifying developers of potential problems.</li> </ul>"},{"location":"api_reference/components/node_invocation_validation/#31-error-handling","title":"3.1 Error Handling","text":"<ul> <li>NodeInvocationError: Raised for critical issues that prevent node execution, such as invalid message types or missing models.</li> <li>Warnings: Used for non-critical issues, such as missing system messages, to alert developers without halting execution.</li> </ul>"},{"location":"api_reference/components/node_invocation_validation/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/node_invocation_validation/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>Exception Messages: Utilizes <code>get_message</code> and <code>get_notes</code> from <code>exception_messages.py</code> to retrieve error messages and notes.</li> <li>Message and Model Classes: Relies on <code>Message</code>, <code>MessageHistory</code>, and <code>ModelBase</code> classes from the <code>llm</code> module for input validation.</li> </ul>"},{"location":"api_reference/components/node_invocation_validation/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>Message History Size: The component assumes that the message history is manageable in size and does not perform optimizations for very large histories.</li> </ul>"},{"location":"api_reference/components/node_invocation_validation/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/node_invocation_validation/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>validation.py</code>: Contains the core validation functions for node invocation.</li> </ul>"},{"location":"api_reference/components/node_invocation_validation/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>errors.py</code>: Defines the <code>NodeInvocationError</code> class used for error handling.</li> <li><code>exception_messages.py</code>: Provides the mechanism for retrieving error messages and notes.</li> </ul>"},{"location":"api_reference/components/node_invocation_validation/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>validation.md</code>: Documents the validation feature, including its purpose and usage.</li> </ul>"},{"location":"api_reference/components/node_invocation_validation/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/node_management/","title":"Node Management","text":"<p>The Node Management component is responsible for managing node creation and execution, providing an abstract base class for nodes with asynchronous invocation capabilities.</p> <p>Version: 0.0.1</p> <p>Component Contact: @github_username</p>"},{"location":"api_reference/components/node_management/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/node_management/#1-purpose","title":"1. Purpose","text":"<p>The Node Management component is designed to facilitate the creation and execution of nodes within the system. It provides an abstract base class, <code>Node</code>, which defines the core functionality and lifecycle of a node, including asynchronous invocation and state management. This component is essential for building scalable and efficient workflows that require asynchronous processing.</p>"},{"location":"api_reference/components/node_management/#11-node-creation-and-execution","title":"1.1 Node Creation and Execution","text":"<p>The primary use case of this component is to create and execute nodes that perform specific tasks asynchronously. This is crucial for applications that require non-blocking operations and efficient resource management.</p> <p>python class MyNode(Node):     @classmethod     def name(cls) -&gt; str:         return \"My Custom Node\"</p> <pre><code>async def invoke(self) -&gt; str:\n    # Custom logic for node execution\n    return \"Node executed successfully\"\n</code></pre>"},{"location":"api_reference/components/node_management/#usage","title":"Usage","text":"<p>node = MyNode() result = await node.tracked_invoke() print(result)</p>"},{"location":"api_reference/components/node_management/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/node_management/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Node Management component is built around the concept of nodes as asynchronous units of work. The design emphasizes flexibility, allowing developers to define custom nodes by extending the <code>Node</code> class and implementing the <code>invoke</code> method.</p>"},{"location":"api_reference/components/node_management/#31-node-class","title":"3.1 Node Class","text":"<ul> <li>Asynchronous Invocation: The <code>Node</code> class ensures that the <code>invoke</code> method is always asynchronous, providing a wrapper if necessary.</li> <li>Debugging and Latency Tracking: Nodes can store debug information and track execution latency using <code>DebugDetails</code> and <code>LatencyDetails</code>.</li> <li>State Management: The <code>NodeState</code> class provides a mechanism to serialize and deserialize node states across process boundaries.</li> </ul>"},{"location":"api_reference/components/node_management/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/node_management/#41-debugging-and-performance","title":"4.1 Debugging and Performance","text":"<ul> <li>Debug Details: Use the <code>DebugDetails</code> class to store and access debugging information during node execution.</li> <li>Latency Tracking: The <code>tracked_invoke</code> method automatically measures and records the execution time of the <code>invoke</code> method.</li> </ul>"},{"location":"api_reference/components/node_management/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/node_management/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>nodes.py</code>: Contains the implementation of the Node Management component, including the <code>Node</code> class and related utilities.</li> <li><code>tool_callable.py</code>: Defines the <code>ToolCallable</code> class, which provides methods for tool information and preparation.</li> </ul>"},{"location":"api_reference/components/node_management/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li>Node Building Documentation: Provides guidelines and best practices for building nodes.</li> <li>Response Handling Documentation: Details the response handling mechanisms used in node execution.</li> </ul>"},{"location":"api_reference/components/node_management/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/node_manifest/","title":"Node Manifest","text":"<p>The Node Manifest component is responsible for creating a manifest for a tool, which includes a description and a list of parameters. This component is essential for node-based systems where tools need to be described and parameterized for effective integration and execution.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/node_manifest/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/node_manifest/#1-purpose","title":"1. Purpose","text":"<p>The Node Manifest component is primarily used to encapsulate the metadata of a tool in node-based systems. This includes providing a clear description of the tool and specifying any parameters it requires. This is crucial for ensuring that tools can be correctly instantiated and utilized within the system.</p>"},{"location":"api_reference/components/node_manifest/#11-creating-a-tool-manifest","title":"1.1 Creating a Tool Manifest","text":"<p>The primary use case for this component is to create a manifest for a tool, which involves specifying a description and any parameters the tool might require.</p> <p>python from railtracks.nodes.manifest import ToolManifest from railtracks.llm import Parameter</p>"},{"location":"api_reference/components/node_manifest/#example-of-creating-a-tool-manifest","title":"Example of creating a tool manifest","text":"<p>description = \"This tool processes data and returns results.\" parameters = [Parameter(name=\"input_data\", type=\"str\", description=\"The data to process.\")] tool_manifest = ToolManifest(description=description, parameters=parameters)</p>"},{"location":"api_reference/components/node_manifest/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/node_manifest/#class-toolmanifest","title":"<code>class ToolManifest</code>","text":"<p>Creates a manifest for a tool, which includes its description and parameters.</p> <p>Args:     description (str): A description of the tool.     parameters (Iterable[Parameter] | None): An iterable of parameters for the tool. If None, there are no paramerters.</p>"},{"location":"api_reference/components/node_manifest/#__init__self-description-parameters","title":"<code>.__init__(self, description, parameters)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/node_manifest/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Node Manifest component is designed to be a lightweight and flexible way to describe tools within a node-based system. It leverages Python's typing system to ensure that parameters are clearly defined and can be easily validated.</p>"},{"location":"api_reference/components/node_manifest/#31-toolmanifest-class","title":"3.1 ToolManifest Class","text":"<ul> <li>Description and Parameters: The <code>ToolManifest</code> class is designed to hold a tool's description and its parameters. This design choice allows for easy extension and integration with other components that require tool metadata.</li> <li>Use of Typing: The use of <code>Iterable[Parameter]</code> for parameters ensures that the component can handle a wide range of parameter configurations, providing flexibility in how tools are described.</li> </ul>"},{"location":"api_reference/components/node_manifest/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/node_manifest/#41-parameter-handling","title":"4.1 Parameter Handling","text":"<ul> <li>The <code>ToolManifest</code> class uses the <code>Parameter</code> class from <code>railtracks.llm</code>. Ensure that the <code>Parameter</code> class is correctly imported and utilized to avoid runtime errors.</li> <li>When no parameters are provided, the <code>ToolManifest</code> initializes with an empty list, ensuring that the absence of parameters does not lead to unexpected behavior.</li> </ul>"},{"location":"api_reference/components/node_manifest/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/node_manifest/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>manifest.py</code>: Contains the implementation of the <code>ToolManifest</code> class.</li> </ul>"},{"location":"api_reference/components/node_manifest/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>tool_management.md</code>: This document would provide additional context on how tools are managed within the system. (Currently unavailable)</li> </ul>"},{"location":"api_reference/components/node_manifest/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>node_management.md</code>: This document would detail how nodes are managed and how the Node Manifest integrates with node management. (Currently unavailable)</li> </ul>"},{"location":"api_reference/components/node_manifest/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/node_state_management/","title":"Node State Management","text":"<p>The Node State Management component is responsible for managing a collection of linked nodes, supporting operations such as storing, updating, and retrieving nodes within a structured framework. This component is crucial for maintaining the state and history of nodes in a way that allows for efficient access and manipulation.</p> <p>Version: 0.0.1</p> <p>Component Contact: @github_username</p>"},{"location":"api_reference/components/node_state_management/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/node_state_management/#1-purpose","title":"1. Purpose","text":"<p>The Node State Management component is designed to handle the lifecycle of nodes within a system. It provides mechanisms to store nodes, update their state, and retrieve them efficiently. This is particularly useful in scenarios where nodes represent entities with mutable states that need to be tracked over time.</p>"},{"location":"api_reference/components/node_state_management/#11-storing-nodes","title":"1.1 Storing Nodes","text":"<p>The component allows for the storage of nodes in a structured manner, ensuring that each node is linked to its parent, if applicable. This hierarchical storage is essential for maintaining the integrity of node relationships.</p> <p>python node_forest = NodeForest() node_forest.update(new_node, stamp)</p>"},{"location":"api_reference/components/node_state_management/#12-retrieving-nodes","title":"1.2 Retrieving Nodes","text":"<p>Nodes can be retrieved from the component using their unique identifiers. This retrieval process ensures that the most recent state of the node is accessed.</p> <p>python node = node_forest['node_id']</p>"},{"location":"api_reference/components/node_state_management/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/node_state_management/#class-nodeforestforestlinkednode","title":"<code>class NodeForest(Forest[LinkedNode])</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/node_state_management/#__init__self-node_heap","title":"<code>.__init__(self, node_heap)</code>","text":"<p>Creates a new instance of a node heap with no objects present.</p>"},{"location":"api_reference/components/node_state_management/#to_verticesself","title":"<code>.to_vertices(self)</code>","text":"<p>Converts the current heap into a list of <code>Vertex</code> objects.</p>"},{"location":"api_reference/components/node_state_management/#updateself-new_node-stamp","title":"<code>.update(self, new_node, stamp)</code>","text":"<p>Updates the heap with the provided node. If you are updating a node that is currently present in the heap you must provide the passcode that was returned when you collected the node. You should set passcode to None if this is a new node.</p> <p>Args:     new_node (Node): The node to update the heap with (it could have the same id as one already in the heap)     stamp (Stamp): The stamp you would like to attach to this node update.</p> <p>Raises:</p>"},{"location":"api_reference/components/node_state_management/#get_node_typeself-identifier","title":"<code>.get_node_type(self, identifier)</code>","text":"<p>Gets the type of the node with the provided identifier.</p>"},{"location":"api_reference/components/node_state_management/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Node State Management component is built around the concept of a <code>Forest</code>, which is a collection of <code>LinkedNode</code> objects. Each <code>LinkedNode</code> is an instance of <code>AbstractLinkedObject</code> and represents a node in the system. The design leverages immutability and thread safety to ensure consistent state management.</p>"},{"location":"api_reference/components/node_state_management/#31-core-design-principles","title":"3.1 Core Design Principles","text":"<ul> <li>Immutability: Nodes are stored as immutable objects to prevent unintended side effects during state manipulation.</li> <li>Thread Safety: The component uses locks to ensure that updates to the node heap are thread-safe.</li> <li>Hierarchical Structure: Nodes are linked to their parents, forming a tree-like structure that facilitates efficient traversal and state management.</li> </ul>"},{"location":"api_reference/components/node_state_management/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/node_state_management/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>The component relies on the <code>Node</code> class from <code>railtracks.nodes.nodes</code> and the <code>Stamp</code> class from <code>railtracks.utils.profiling</code>.</li> <li>Ensure that the <code>Node</code> class implements a <code>safe_copy</code> method for deep copying.</li> </ul>"},{"location":"api_reference/components/node_state_management/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>The component is designed to handle a large number of nodes efficiently, but performance may degrade with extremely large datasets.</li> <li>The <code>time_machine</code> method allows for reverting the state of nodes to a previous step, which can be resource-intensive.</li> </ul>"},{"location":"api_reference/components/node_state_management/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/node_state_management/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>node.py</code>: Contains the implementation of the Node State Management component.</li> <li><code>forest.py</code>: Provides the <code>Forest</code> and <code>AbstractLinkedObject</code> classes used by the component.</li> </ul>"},{"location":"api_reference/components/node_state_management/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>node.md</code>: Documentation on the node execution flow and its integration within the system.</li> </ul>"},{"location":"api_reference/components/node_state_management/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/node_to_mcp_server/","title":"Node to MCP Server","text":"<p>The Node to MCP Server component is designed to create a FastMCP server that registers and runs nodes as MCP tools, enabling asynchronous tool execution within the system.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/node_to_mcp_server/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/node_to_mcp_server/#1-purpose","title":"1. Purpose","text":"<p>The primary purpose of this component is to facilitate the integration of nodes as tools within a FastMCP server, allowing for their asynchronous execution. This is particularly useful in scenarios where multiple nodes need to be managed and executed concurrently, leveraging the capabilities of the FastMCP framework.</p>"},{"location":"api_reference/components/node_to_mcp_server/#11-registering-nodes-as-mcp-tools","title":"1.1 Registering Nodes as MCP Tools","text":"<p>This use case involves registering a list of nodes as tools within a FastMCP server. This is crucial for enabling the nodes to be executed asynchronously as part of the server's operations.</p> <p>python from railtracks.rt_mcp.node_to_mcp import create_mcp_server from railtracks.nodes.nodes import Node</p>"},{"location":"api_reference/components/node_to_mcp_server/#define-your-nodes","title":"Define your nodes","text":"<p>nodes = [Node1(), Node2()]</p>"},{"location":"api_reference/components/node_to_mcp_server/#create-the-mcp-server","title":"Create the MCP server","text":"<p>mcp_server = create_mcp_server(nodes, server_name=\"My MCP Server\")</p>"},{"location":"api_reference/components/node_to_mcp_server/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/node_to_mcp_server/#def-create_mcp_servernodes-server_name-fastmcp","title":"<code>def create_mcp_server(nodes, server_name, fastmcp)</code>","text":"<p>Create a FastMCP server that can be used to run nodes as MCP tools.</p> <p>Args:     nodes: List of Node classes to be registered as tools with the MCP server.     server_name: Name of the MCP server instance.     fastmcp: Optional FastMCP instance to use instead of creating a new one.</p> <p>Returns:     A FastMCP server instance.</p>"},{"location":"api_reference/components/node_to_mcp_server/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Node to MCP Server component is designed with the following architectural principles:</p>"},{"location":"api_reference/components/node_to_mcp_server/#31-core-design-principles","title":"3.1 Core Design Principles","text":"<ul> <li>Asynchronous Execution: The component leverages asynchronous programming to allow nodes to be executed concurrently, improving performance and scalability.</li> <li>Modular Design: The component is structured to separate concerns, with distinct functions for creating tool functions and the MCP server.</li> </ul>"},{"location":"api_reference/components/node_to_mcp_server/#32-high-level-architecture","title":"3.2 High-Level Architecture","text":"<p>The component consists of two main functions:</p> <ul> <li><code>_create_tool_function</code>: This function generates a tool function for each node, using the node's metadata to define parameters and documentation.</li> <li><code>create_mcp_server</code>: This function initializes a FastMCP server and registers each node as a tool, using the tool functions created by <code>_create_tool_function</code>.</li> </ul>"},{"location":"api_reference/components/node_to_mcp_server/#33-key-design-decisions","title":"3.3 Key Design Decisions","text":"<ul> <li>Use of FastMCP: The decision to use FastMCP was driven by its robust support for asynchronous tool execution and management.</li> <li>Parameter Schema Handling: The component uses <code>_parameters_to_json_schema</code> to convert node parameters into a JSON schema, ensuring consistent parameter handling.</li> </ul>"},{"location":"api_reference/components/node_to_mcp_server/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/node_to_mcp_server/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>FastMCP Dependency: The component relies on the FastMCP library for server and tool management. Ensure that FastMCP is installed and properly configured.</li> <li>Node Class Requirements: Nodes must implement the <code>tool_info</code> method to provide necessary metadata for registration.</li> </ul>"},{"location":"api_reference/components/node_to_mcp_server/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>Scalability: The component is designed to handle multiple nodes, but performance may degrade with a very large number of nodes due to resource constraints.</li> </ul>"},{"location":"api_reference/components/node_to_mcp_server/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/node_to_mcp_server/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>../packages/railtracks/src/railtracks/rt_mcp/node_to_mcp.py</code>: Contains the implementation of the Node to MCP Server component.</li> </ul>"},{"location":"api_reference/components/node_to_mcp_server/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li>MCP Tool Connection Documentation: Provides additional context on how MCP tools are connected and managed.</li> </ul>"},{"location":"api_reference/components/node_to_mcp_server/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li>MCP Integration Documentation: Discusses the broader integration of MCP within the system.</li> </ul>"},{"location":"api_reference/components/node_to_mcp_server/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/profiling/","title":"Profiling","text":"<p>The Profiling component provides a system for creating and managing \"stamps\" that represent specific points in time, identified by a message and a step number. This is useful for tracking the sequence and timing of events within a system.</p> <p>Version: 0.0.1</p> <p>Component Contact: @github_username</p>"},{"location":"api_reference/components/profiling/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/profiling/#1-purpose","title":"1. Purpose","text":"<p>The Profiling component is primarily used to track and log specific events or actions within a system by creating \"stamps.\" These stamps are useful for debugging, performance monitoring, and understanding the flow of execution in complex systems.</p>"},{"location":"api_reference/components/profiling/#11-creating-a-stamp","title":"1.1 Creating a Stamp","text":"<p>Creating a stamp is essential for logging an event with a specific message and step number. This helps in tracking the sequence of operations.</p> <p>python from railtracks.utils.profiling import StampManager</p> <p>manager = StampManager() stamp = manager.create_stamp(\"Event started\")</p>"},{"location":"api_reference/components/profiling/#12-using-stamp-creator","title":"1.2 Using Stamp Creator","text":"<p>The stamp creator allows for the creation of multiple stamps with shared step values, which is useful for batch operations or grouped events.</p> <p>python stamp_creator = manager.stamp_creator() stamp1 = stamp_creator(\"Batch event 1\") stamp2 = stamp_creator(\"Batch event 2\")</p>"},{"location":"api_reference/components/profiling/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/profiling/#class-stamp","title":"<code>class Stamp</code>","text":"<p>A simple dataclass that represents a stamp in time for the system.</p> <p>Shared actions should have identical stamps, but they do not need to have identical time fields.</p>"},{"location":"api_reference/components/profiling/#class-stampmanager","title":"<code>class StampManager</code>","text":"<p>A simple manager object that can be used to coordinate the creation of a stamps during the runtime of a system.</p>"},{"location":"api_reference/components/profiling/#__init__self","title":"<code>.__init__(self)</code>","text":"<p>Creates a new instance of a <code>StampManager</code> object. It defaults the current step to 0.</p>"},{"location":"api_reference/components/profiling/#create_stampself-message","title":"<code>.create_stamp(self, message)</code>","text":"<p>Creates a new stamp with the given message.</p> <p>Args:     message (str): The message you would like the returned stamp to contain</p> <p>Returns:     Stamp: The newly created stamp with the next step value, your provided message and a timestamp determined      at creation.</p>"},{"location":"api_reference/components/profiling/#stamp_creatorself","title":"<code>.stamp_creator(self)</code>","text":"<p>Creates a method that can be used to create new stamps with shared step values.</p> <p>This method guarantees the following properties:</p> <ul> <li>The stamp created by calling the method will have the timestamp of when the method was called.</li> <li>You can have different messages for each stamp created by the method.</li> <li>All stamps created by the method will share step values</li> </ul> <p>Returns:     (str) -&gt; Stamp: A method that can be used to create new stamps with shared step values.</p>"},{"location":"api_reference/components/profiling/#step_logsself","title":"<code>.step_logs(self)</code>","text":"<p>Returns a copy of a dictionary containing a list of identifiers for each step that exist in the system.</p>"},{"location":"api_reference/components/profiling/#all_stampsself","title":"<code>.all_stamps(self)</code>","text":"<p>Returns a list of the all the stamps that have been created in the system.</p>"},{"location":"api_reference/components/profiling/#3-architectural-design","title":"3. Architectural Design","text":""},{"location":"api_reference/components/profiling/#31-stamp-and-stampmanager","title":"3.1 Stamp and StampManager","text":"<ul> <li>Stamp Class: Represents a point in time with a timestamp, step number, and identifier. It supports comparison based on time and step for ordering.</li> <li>StampManager Class: Manages the creation and storage of stamps. It ensures thread safety using locks and maintains a log of steps and their associated messages.</li> </ul>"},{"location":"api_reference/components/profiling/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/profiling/#41-thread-safety","title":"4.1 Thread Safety","text":"<ul> <li>The <code>StampManager</code> uses a threading lock to ensure that stamp creation is thread-safe. This is crucial in multi-threaded environments to prevent race conditions.</li> </ul>"},{"location":"api_reference/components/profiling/#42-performance","title":"4.2 Performance","text":"<ul> <li>The use of deep copies in <code>step_logs</code> and <code>all_stamps</code> properties ensures that the internal state is not modified externally, but it may have performance implications for large numbers of stamps.</li> </ul>"},{"location":"api_reference/components/profiling/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/profiling/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>profiling.py</code>: Contains the implementation of the <code>Stamp</code> and <code>StampManager</code> classes.</li> </ul>"},{"location":"api_reference/components/profiling/#52-related-feature-files","title":"5.2 Related Feature Files","text":"<ul> <li><code>logging_profiling.md</code>: Documentation for the logging and profiling feature, which includes the use of the Profiling component.</li> </ul>"},{"location":"api_reference/components/profiling/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/prompt_formatting/","title":"Prompt Injection","text":"<p>The Prompt Injection component provides functionality to format and inject values into message prompts using a custom formatter and dictionary. This is particularly useful in scenarios where dynamic data needs to be inserted into predefined prompt templates, enhancing the flexibility and adaptability of the system.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/prompt_formatting/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/prompt_formatting/#1-purpose","title":"1. Purpose","text":"<p>The primary purpose of the Prompt Injection component is to enable the dynamic insertion of values into message prompts. This is achieved through a custom formatter and a dictionary that holds the values to be injected. The component is essential for adapting prompts to the current execution context, especially when certain data is only available at runtime.</p>"},{"location":"api_reference/components/prompt_formatting/#11-dynamic-prompt-filling","title":"1.1 Dynamic Prompt Filling","text":"<p>This use case involves filling a prompt with dynamic values using the <code>fill_prompt</code> function. This is crucial for scenarios where the prompt template contains placeholders that need to be replaced with actual data at runtime.</p> <p>python from railtracks.utils.prompt_injection import fill_prompt, ValueDict</p> <p>prompt_template = \"Find the capital of {country}.\" values = ValueDict(country=\"France\") filled_prompt = fill_prompt(prompt_template, values) print(filled_prompt)  # Output: \"Find the capital of France.\"</p>"},{"location":"api_reference/components/prompt_formatting/#12-message-history-injection","title":"1.2 Message History Injection","text":"<p>This use case demonstrates how to inject values into a series of messages using the <code>inject_values</code> function. This is important for maintaining a consistent and contextually relevant message history.</p> <p>python from railtracks.llm import MessageHistory, Message from railtracks.utils.prompt_injection import inject_values, ValueDict</p> <p>message_history = MessageHistory([     Message(content=\"Find the capital of {country}.\", role=\"user\", inject_prompt=True) ]) values = ValueDict(country=\"Germany\") updated_history = inject_values(message_history, values) print(updated_history)  # Output: \"user: Find the capital of Germany.\"</p>"},{"location":"api_reference/components/prompt_formatting/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/prompt_formatting/#class-keyonlyformatterstringformatter","title":"<code>class KeyOnlyFormatter(string.Formatter)</code>","text":"<p>A simple formatter which will only use keyword arguments to fill placeholders.</p>"},{"location":"api_reference/components/prompt_formatting/#get_valueself-key-args-kwargs","title":"<code>.get_value(self, key, args, kwargs)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/prompt_formatting/#class-valuedictdict","title":"<code>class ValueDict(dict)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/prompt_formatting/#def-fill_promptprompt-value_dict","title":"<code>def fill_prompt(prompt, value_dict)</code>","text":"<p>Fills a prompt using the railtracks context object as its source of truth</p>"},{"location":"api_reference/components/prompt_formatting/#def-inject_valuesmessage_history-value_dict","title":"<code>def inject_values(message_history, value_dict)</code>","text":"<p>Injects the values in the <code>value_dict</code> from the current request into the prompt.</p> <p>Args:     message_history (MessageHistory): The prompts to inject context into.     value_dict (ValueDict): The dictionary containing values to fill in the prompt.</p>"},{"location":"api_reference/components/prompt_formatting/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Prompt Injection component is designed to provide a flexible and efficient way to inject dynamic values into message prompts. The design leverages Python's <code>string.Formatter</code> to create a custom formatter (<code>KeyOnlyFormatter</code>) that only uses keyword arguments for filling placeholders. This ensures that only specified keys are replaced, preventing accidental overwrites.</p>"},{"location":"api_reference/components/prompt_formatting/#31-keyonlyformatter","title":"3.1 KeyOnlyFormatter","text":"<ul> <li>Purpose: Custom formatter that uses only keyword arguments.</li> <li>Design Consideration: Ensures that only specified keys are replaced, maintaining the integrity of the prompt template.</li> </ul>"},{"location":"api_reference/components/prompt_formatting/#32-valuedict","title":"3.2 ValueDict","text":"<ul> <li>Purpose: Custom dictionary that returns placeholders for missing keys.</li> <li>Design Consideration: Provides a fallback mechanism for missing values, ensuring that the prompt structure is preserved even if some data is unavailable.</li> </ul>"},{"location":"api_reference/components/prompt_formatting/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/prompt_formatting/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>The component relies on the <code>railtracks.llm</code> module for message handling. Ensure that this module is correctly configured and available in the environment.</li> </ul>"},{"location":"api_reference/components/prompt_formatting/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>The component is designed for efficiency, but injecting a large number of values or handling extensive message histories may impact performance. Consider optimizing the value dictionary and message history size for better performance.</li> </ul>"},{"location":"api_reference/components/prompt_formatting/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/prompt_formatting/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>prompt_injection.py</code>: Contains the implementation of the Prompt Injection component.</li> </ul>"},{"location":"api_reference/components/prompt_formatting/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>message.py</code>: Defines the <code>Message</code> class used in message handling.</li> <li><code>history.py</code>: Defines the <code>MessageHistory</code> class used for managing message sequences.</li> </ul>"},{"location":"api_reference/components/prompt_formatting/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>llm_integration.md</code>: Provides additional context on how prompt injection is used within the larger LLM integration framework.</li> </ul>"},{"location":"api_reference/components/prompt_formatting/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/prompt_injection/","title":"Prompt Injection","text":"<p>The Prompt Injection component is designed to inject context from the current request into a message history for prompt processing within the Railtracks framework. This allows for dynamic and context-aware interactions with language models.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/prompt_injection/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/prompt_injection/#1-purpose","title":"1. Purpose","text":"<p>The primary purpose of the Prompt Injection component is to enhance the interaction with language models by injecting relevant context into the message history. This is particularly useful in scenarios where the context of a session or request needs to be considered to generate accurate and relevant responses.</p>"},{"location":"api_reference/components/prompt_injection/#11-injecting-context-into-message-history","title":"1.1 Injecting Context into Message History","text":"<p>This use case involves injecting the current request's context into a <code>MessageHistory</code> object, which is then used to interact with language models.</p> <p>python from railtracks.llm import MessageHistory from railtracks.prompts.prompt import inject_context</p>"},{"location":"api_reference/components/prompt_injection/#create-a-messagehistory-object","title":"Create a MessageHistory object","text":"<p>message_history = MessageHistory()</p>"},{"location":"api_reference/components/prompt_injection/#inject-context-into-the-message-history","title":"Inject context into the message history","text":"<p>inject_context(message_history)</p>"},{"location":"api_reference/components/prompt_injection/#use-the-message-history-with-a-language-model","title":"Use the message history with a language model","text":"<p>response = language_model.process(message_history)</p>"},{"location":"api_reference/components/prompt_injection/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/prompt_injection/#def-inject_contextmessage_history","title":"<code>def inject_context(message_history)</code>","text":"<p>Injects the context from the current request into the prompt.</p> <p>Args:     message_history (MessageHistory): The prompts to inject context into.</p>"},{"location":"api_reference/components/prompt_injection/#3-architectural-design","title":"3. Architectural Design","text":""},{"location":"api_reference/components/prompt_injection/#31-context-injection-mechanism","title":"3.1 Context Injection Mechanism","text":"<ul> <li>_ContextDict Class:</li> <li>Inherits from <code>ValueDict</code> and overrides the <code>__getitem__</code> method to fetch context values using <code>context.get(key)</code>.</li> <li> <p>This design allows seamless integration of context values into the message history.</p> </li> <li> <p>inject_context Function:</p> </li> <li>Attempts to retrieve the local configuration using <code>get_local_config()</code>.</li> <li>Checks if prompt injection is enabled via <code>local_config.prompt_injection</code>.</li> <li> <p>If enabled, it uses <code>inject_values</code> to inject context into the <code>MessageHistory</code> using <code>_ContextDict</code>.</p> </li> <li> <p>Design Considerations:</p> </li> <li>The component is designed to handle cases where the context is not set, such as when not running within an <code>rt.Session()</code>.</li> <li>The use of exceptions (<code>ContextError</code>) ensures robust handling of missing context scenarios.</li> </ul>"},{"location":"api_reference/components/prompt_injection/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/prompt_injection/#41-context-management","title":"4.1 Context Management","text":"<ul> <li>The component relies on the <code>railtracks.context</code> module to fetch and manage context values.</li> <li>It is crucial to ensure that the context is correctly set up and managed to avoid <code>ContextError</code>.</li> </ul>"},{"location":"api_reference/components/prompt_injection/#42-performance-and-limitations","title":"4.2 Performance and Limitations","text":"<ul> <li>The performance of context injection is dependent on the efficiency of the <code>context.get</code> method and the size of the context being injected.</li> <li>The component is not suitable for scenarios where context management is not properly configured.</li> </ul>"},{"location":"api_reference/components/prompt_injection/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/prompt_injection/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>../packages/railtracks/src/railtracks/prompts/prompt.py</code>: Contains the implementation of the Prompt Injection component.</li> </ul>"},{"location":"api_reference/components/prompt_injection/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>../components/context_management.md</code>: Provides documentation on context management within the Railtracks framework.</li> </ul>"},{"location":"api_reference/components/prompt_injection/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>../features/llm_integration.md</code>: Details the integration of language models with the Railtracks framework.</li> </ul>"},{"location":"api_reference/components/prompt_injection/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/publisher/","title":"Publisher","text":"<p>The <code>Publisher</code> component implements a simple publish-subscribe pattern using asynchronous programming, allowing subscribers to receive messages in an orderly fashion.</p> <p>Version: 0.0.1</p> <p>Component Contact: @github_username</p>"},{"location":"api_reference/components/publisher/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/publisher/#1-purpose","title":"1. Purpose","text":"<p>The <code>Publisher</code> component is designed to facilitate asynchronous message broadcasting to multiple subscribers. It ensures that messages are processed in the order they are received and provides mechanisms for managing subscriber callbacks.</p>"},{"location":"api_reference/components/publisher/#11-publish-and-subscribe","title":"1.1 Publish and Subscribe","text":"<p>The primary use case of the <code>Publisher</code> is to allow subscribers to register callback functions that are triggered when a message is published.</p> <p>python import asyncio from railtracks.utils.publisher import Publisher</p> <p>async def my_callback(message):     print(f\"Received: {message}\")</p> <p>async def main():     async with Publisher() as publisher:         publisher.subscribe(my_callback)         await publisher.publish(\"Hello, World!\")</p> <p>asyncio.run(main())</p>"},{"location":"api_reference/components/publisher/#12-listener-for-specific-messages","title":"1.2 Listener for Specific Messages","text":"<p>Another use case is to create a listener that waits for a specific message that matches a given filter.</p> <p>python async def message_filter(message):     return message == \"Target Message\"</p> <p>async def main():     async with Publisher() as publisher:         result = await publisher.listener(message_filter)         print(f\"Filtered Message: {result}\")</p> <p>asyncio.run(main())</p>"},{"location":"api_reference/components/publisher/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/publisher/#class-subscribergeneric_t","title":"<code>class Subscriber(Generic[_T])</code>","text":"<p>A simple wrapper class of a callback function.</p>"},{"location":"api_reference/components/publisher/#__init__self-callback-name","title":"<code>.__init__(self, callback, name)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/publisher/#triggerself-message","title":"<code>.trigger(self, message)</code>","text":"<p>Trigger this broadcast_callback with the given message.</p>"},{"location":"api_reference/components/publisher/#class-publishergeneric_t","title":"<code>class Publisher(Generic[_T])</code>","text":"<p>A simple publisher object with some basic functionality to publish and suvbscribe to messages.</p> <p>Note a couple of things: - Message will be handled in the order they came in (no jumping the line) - If you add a broadcast_callback during the operation it will handle any new messages that come in after the subscription     took place - Calling the shutdown method will kill the publisher forever. You will have to make a new one after.</p>"},{"location":"api_reference/components/publisher/#__init__self","title":"<code>.__init__(self)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/publisher/#startself","title":"<code>.start(self)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/publisher/#publishself-message","title":"<code>.publish(self, message)</code>","text":"<p>Publish a message the publisher. This will trigger all subscribers to receive the message.</p> <p>Args:     message: The message you would like to publish.</p>"},{"location":"api_reference/components/publisher/#subscribeself-callback-name","title":"<code>.subscribe(self, callback, name)</code>","text":"<p>Subscribe the publisher so whenever we receive a message the callback will be triggered.</p> <p>Args:     callback: The callback function that will be triggered when a message is published.     name: Optional name for the broadcast_callback, mainly used for debugging.</p> <p>Returns:     str: A unique identifier for the broadcast_callback. You can use this key to unsubscribe later.</p>"},{"location":"api_reference/components/publisher/#unsubscribeself-identifier","title":"<code>.unsubscribe(self, identifier)</code>","text":"<p>Unsubscribe the publisher so the given broadcast_callback will no longer receive messages.</p> <p>Args:     identifier: The unique identifier of the broadcast_callback to remove.</p> <p>Raises:     KeyError: If no broadcast_callback with the given identifier exists.</p>"},{"location":"api_reference/components/publisher/#listenerself-message_filter-result_mapping-listener_name","title":"<code>.listener(self, message_filter, result_mapping, listener_name)</code>","text":"<p>Creates a special listener object that will wait for the first message that matches the given filter.</p> <p>After receiving the message it will run the result_mapping function on the message and return the result, and kill the broadcast_callback.</p> <p>Args:     message_filter: A function that takes a message and returns True if the message should be returned.     result_mapping: A function that maps the message into a final result.     listener_name: Optional name for the listener, mainly used for debugging.</p>"},{"location":"api_reference/components/publisher/#shutdownself","title":"<code>.shutdown(self)</code>","text":"<p>Shutdowns the publisher and halts the listener loop.</p> <p>Note that this will work slowly, as it will wait for the current messages in the queue to be processed before shutting down.</p>"},{"location":"api_reference/components/publisher/#is_runningself","title":"<code>.is_running(self)</code>","text":"<p>Check if the publisher is currently running.</p> <p>Returns:     bool: True if the publisher is running, False otherwise.</p>"},{"location":"api_reference/components/publisher/#3-architectural-design","title":"3. Architectural Design","text":""},{"location":"api_reference/components/publisher/#31-core-design","title":"3.1 Core Design","text":"<ul> <li>Asynchronous Processing: The <code>Publisher</code> uses Python's <code>asyncio</code> library to handle asynchronous message processing, ensuring non-blocking operations.</li> <li>Subscriber Management: Subscribers are managed through the <code>Subscriber</code> class, which wraps callback functions and provides a unique identifier for each subscriber.</li> <li>Message Queue: An internal <code>asyncio.Queue</code> is used to store messages, ensuring they are processed in the order they are received.</li> </ul>"},{"location":"api_reference/components/publisher/#32-key-design-decisions","title":"3.2 Key Design Decisions","text":"<ul> <li>Extendability: The <code>Subscriber</code> class is designed to be extendable, allowing for future enhancements without breaking existing functionality.</li> <li>Error Handling: Errors in subscriber callbacks are logged but do not interrupt the message processing loop, ensuring robustness.</li> </ul>"},{"location":"api_reference/components/publisher/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/publisher/#41-implementation-details","title":"4.1 Implementation Details","text":"<ul> <li>Concurrency: The <code>Publisher</code> is not thread-safe and should be used within a single event loop.</li> <li>Shutdown Behavior: The <code>shutdown</code> method waits for all messages in the queue to be processed before stopping the publisher, which may introduce delays.</li> </ul>"},{"location":"api_reference/components/publisher/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/publisher/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>../utils/publisher.py</code>: Contains the implementation of the <code>Publisher</code> and <code>Subscriber</code> classes.</li> </ul>"},{"location":"api_reference/components/publisher/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>../components/pubsub_messaging.md</code>: Provides an overview of the publish-subscribe messaging system.</li> </ul>"},{"location":"api_reference/components/publisher/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>../features/task_execution.md</code>: Describes how the <code>Publisher</code> integrates with task execution features.</li> </ul>"},{"location":"api_reference/components/publisher/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/pubsub_messaging/","title":"Pub/Sub Messaging","text":"<p>The Pub/Sub Messaging component is designed to facilitate asynchronous communication within the RailTracks system by implementing a publisher-subscriber pattern. This component allows different parts of the system to publish and subscribe to messages, enabling decoupled and efficient message handling.</p> <p>Version: 0.0.1</p> <p>Component Contact: @railtracks_dev</p>"},{"location":"api_reference/components/pubsub_messaging/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/pubsub_messaging/#1-purpose","title":"1. Purpose","text":"<p>The Pub/Sub Messaging component is primarily used for handling request completions within the RailTracks system. It supports publishing and subscribing to various types of messages, such as request success, failure, and creation messages. This component is crucial for enabling asynchronous communication and decoupling different parts of the system.</p>"},{"location":"api_reference/components/pubsub_messaging/#11-publishing-messages","title":"1.1 Publishing Messages","text":"<p>The primary use case is to publish messages related to request completions. This allows different parts of the system to react to these events without being tightly coupled.</p> <p>python from railtracks.pubsub.publisher import RTPublisher from railtracks.pubsub.messages import RequestSuccess</p> <p>async def main():     publisher = RTPublisher()     await publisher.start()     message = RequestSuccess(request_id=\"123\", node_state=some_node_state, result=\"Hello World\")     await publisher.publish(message)     await publisher.shutdown()</p>"},{"location":"api_reference/components/pubsub_messaging/#12-subscribing-to-messages","title":"1.2 Subscribing to Messages","text":"<p>Another key use case is subscribing to specific message types to perform actions based on the message content.</p> <p>python def handle_success(message):     if isinstance(message, RequestSuccess):         print(f\"Request {message.request_id} succeeded with: {message.result}\")</p> <p>publisher.subscribe(handle_success, \"success_handler\")</p>"},{"location":"api_reference/components/pubsub_messaging/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/pubsub_messaging/#class-requestcompletionmessageabc","title":"<code>class RequestCompletionMessage(ABC)</code>","text":"<p>The base class for all messages on the request completion system.</p>"},{"location":"api_reference/components/pubsub_messaging/#log_messageself","title":"<code>.log_message(self)</code>","text":"<p>Converts the message to a string ready to be logged.</p>"},{"location":"api_reference/components/pubsub_messaging/#class-requestsuccessrequestfinishedbase","title":"<code>class RequestSuccess(RequestFinishedBase)</code>","text":"<p>A message that indicates the succseful completion of a request.</p>"},{"location":"api_reference/components/pubsub_messaging/#__init__self","title":"<code>.__init__(self)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/pubsub_messaging/#log_messageself_1","title":"<code>.log_message(self)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/pubsub_messaging/#class-requestfailurerequestfinishedbase","title":"<code>class RequestFailure(RequestFinishedBase)</code>","text":"<p>A message that indicates a failure in the request execution.</p>"},{"location":"api_reference/components/pubsub_messaging/#__init__self_1","title":"<code>.__init__(self)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/pubsub_messaging/#log_messageself_2","title":"<code>.log_message(self)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/pubsub_messaging/#class-requestcreationfailurerequestfinishedbase","title":"<code>class RequestCreationFailure(RequestFinishedBase)</code>","text":"<p>A special class for situations where the creation of a new request fails before it was ever able to run.</p>"},{"location":"api_reference/components/pubsub_messaging/#__init__self_2","title":"<code>.__init__(self)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/pubsub_messaging/#log_messageself_3","title":"<code>.log_message(self)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/pubsub_messaging/#class-requestcreationrequestcompletionmessage","title":"<code>class RequestCreation(RequestCompletionMessage)</code>","text":"<p>A message that describes the creation of a new request in the system.</p>"},{"location":"api_reference/components/pubsub_messaging/#__init__self_3","title":"<code>.__init__(self)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/pubsub_messaging/#class-fatalfailurerequestcompletionmessage","title":"<code>class FatalFailure(RequestCompletionMessage)</code>","text":"<p>A message that indicates an irrecoverable failure in the request completion system.</p>"},{"location":"api_reference/components/pubsub_messaging/#__init__self_4","title":"<code>.__init__(self)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/pubsub_messaging/#class-streamingrequestcompletionmessage","title":"<code>class Streaming(RequestCompletionMessage)</code>","text":"<p>A message that indicates a streaming operation in the request completion system.</p>"},{"location":"api_reference/components/pubsub_messaging/#__init__self_5","title":"<code>.__init__(self)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/pubsub_messaging/#def-output_mappingresult","title":"<code>def output_mapping(result)</code>","text":"<p>Maps the result of a RequestCompletionMessage to its final output.</p>"},{"location":"api_reference/components/pubsub_messaging/#def-stream_subscribersub_callback","title":"<code>def stream_subscriber(sub_callback)</code>","text":"<p>Converts the basic streamer callback into a broadcast_callback handler designed to take in <code>RequestCompletionMessage</code></p>"},{"location":"api_reference/components/pubsub_messaging/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Pub/Sub Messaging component is designed around the publisher-subscriber pattern, which decouples message producers from consumers. This design allows for scalable and maintainable communication within the system.</p>"},{"location":"api_reference/components/pubsub_messaging/#31-core-components","title":"3.1 Core Components","text":"<ul> <li>Messages (<code>messages.py</code>): Defines various message types that can be published and subscribed to, such as <code>RequestSuccess</code>, <code>RequestFailure</code>, and <code>Streaming</code>.</li> <li>Publisher (<code>publisher.py</code>): Manages the distribution of messages to subscribers. It uses asynchronous operations to ensure non-blocking message handling.</li> <li>Subscriber (<code>_subscriber.py</code>): Provides utilities for creating subscribers that can handle specific message types.</li> <li>Utilities (<code>utils.py</code>): Contains helper functions like <code>output_mapping</code> to process message results.</li> </ul>"},{"location":"api_reference/components/pubsub_messaging/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/pubsub_messaging/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>Ensure that the <code>railtracks</code> package is installed and properly configured.</li> <li>The component relies on <code>asyncio</code> for asynchronous operations.</li> </ul>"},{"location":"api_reference/components/pubsub_messaging/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>The system is designed to handle a high volume of messages efficiently.</li> <li>Ensure that subscribers are optimized to prevent bottlenecks.</li> </ul>"},{"location":"api_reference/components/pubsub_messaging/#43-debugging-observability","title":"4.3 Debugging &amp; Observability","text":"<ul> <li>Enable debug logging to trace message flows.</li> <li>Use the <code>log_message()</code> method in messages for standardized logging.</li> </ul>"},{"location":"api_reference/components/pubsub_messaging/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/pubsub_messaging/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>_subscriber.py</code>: Contains subscriber utilities.</li> <li><code>messages.py</code>: Defines message types.</li> <li><code>publisher.py</code>: Implements the publisher.</li> <li><code>utils.py</code>: Provides utility functions.</li> </ul>"},{"location":"api_reference/components/pubsub_messaging/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>context_management.md</code>: Discusses context management related to message handling.</li> </ul>"},{"location":"api_reference/components/pubsub_messaging/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>task_execution.md</code>: Describes task execution features that integrate with the Pub/Sub system.</li> </ul>"},{"location":"api_reference/components/pubsub_messaging/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/rag_core/","title":"RAG Core","text":"<p>The RAG Core component implements a Retrieval-Augmented Generation (RAG) system for document processing, embedding, and search. It is designed to efficiently handle large volumes of text data by chunking, embedding, and storing them for quick retrieval and similarity search.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/rag_core/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/rag_core/#1-purpose","title":"1. Purpose","text":"<p>The RAG Core component is primarily used for processing and managing large text datasets. It allows for efficient text chunking, embedding, and storage, enabling fast and accurate similarity searches. This is particularly useful in applications such as document retrieval, question answering, and content recommendation systems.</p>"},{"location":"api_reference/components/rag_core/#11-text-chunking-and-embedding","title":"1.1 Text Chunking and Embedding","text":"<p>The component processes raw text by dividing it into manageable chunks and generating embeddings for each chunk. This is crucial for handling large documents and ensuring that the embeddings are accurate and relevant.</p> <p>python from rag_core import RAG</p>"},{"location":"api_reference/components/rag_core/#initialize-rag-with-text-documents","title":"Initialize RAG with text documents","text":"<p>rag = RAG(docs=[\"This is a sample document.\"], input_type=\"text\")</p>"},{"location":"api_reference/components/rag_core/#embed-all-documents","title":"Embed all documents","text":"<p>rag.embed_all()</p>"},{"location":"api_reference/components/rag_core/#12-similarity-search","title":"1.2 Similarity Search","text":"<p>Once the text is processed and stored, the component allows for efficient similarity searches, returning the most relevant documents or text chunks based on a query.</p> <p>python</p>"},{"location":"api_reference/components/rag_core/#perform-a-search-with-a-query","title":"Perform a search with a query","text":"<p>results = rag.search(query=\"sample query\", top_k=3) for result in results:     print(result.record.text)</p>"},{"location":"api_reference/components/rag_core/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/rag_core/#3-architectural-design","title":"3. Architectural Design","text":"<p>The RAG Core component is designed with modularity and scalability in mind. It integrates several sub-components to achieve its functionality:</p> <ul> <li>TextChunkingService: Responsible for dividing text into chunks. It supports multiple strategies, such as chunking by character or token. See <code>chunking_service.py</code></li> <li>EmbeddingService: Utilizes the <code>litellm</code> library to generate embeddings for text chunks. See <code>embedding_service.py</code></li> <li>VectorStore: Manages the storage and retrieval of vector embeddings. It supports CRUD operations and similarity searches. See <code>vector_store/base.py</code></li> </ul>"},{"location":"api_reference/components/rag_core/#31-design-considerations","title":"3.1 Design Considerations","text":"<ul> <li>Modularity: Each service (chunking, embedding, storage) is encapsulated in its own class, allowing for easy replacement or extension.</li> <li>Scalability: The system is designed to handle large datasets by processing text in chunks and storing embeddings efficiently.</li> <li>Flexibility: Supports different chunking strategies and embedding models, making it adaptable to various use cases.</li> </ul>"},{"location":"api_reference/components/rag_core/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/rag_core/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>Environment Variables: The <code>EmbeddingService</code> may require an API key for <code>litellm</code>, which can be set via the <code>OPENAI_API_KEY</code> environment variable.</li> <li>Configuration: The component accepts configuration dictionaries for embedding, storage, and chunking services, allowing for customization.</li> </ul>"},{"location":"api_reference/components/rag_core/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>Chunk Size: The performance of the chunking process can be affected by the chosen chunk size and overlap. It is recommended to keep the overlap less than or equal to 40% of the chunk size.</li> <li>Embedding Model: The choice of embedding model impacts both the quality of embeddings and the computational resources required.</li> </ul>"},{"location":"api_reference/components/rag_core/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/rag_core/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>chunking_service.py</code>: Handles text chunking operations.</li> <li><code>embedding_service.py</code>: Manages text embedding using <code>litellm</code>.</li> <li><code>text_object.py</code>: Defines the <code>TextObject</code> class for managing text metadata and embeddings.</li> <li><code>vector_store/base.py</code>: Provides the base class for vector storage and retrieval.</li> </ul>"},{"location":"api_reference/components/rag_core/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>chunking_service.md</code>: Documentation for the chunking service component.</li> <li><code>embedding_service.md</code>: Documentation for the embedding service component.</li> </ul>"},{"location":"api_reference/components/rag_core/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>rag_system.md</code>: Documentation for the RAG system feature.</li> </ul>"},{"location":"api_reference/components/rag_core/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/rag_node/","title":"RAG Node","text":"<p>The RAG Node component is designed to facilitate vector-based document search by embedding and chunking documents, allowing for efficient retrieval of relevant information. It serves as a crucial part of the larger RAG (Retrieval-Augmented Generation) system, enabling seamless integration with document processing workflows.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/rag_node/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/rag_node/#1-purpose","title":"1. Purpose","text":"<p>The RAG Node component is primarily used for embedding and chunking documents to enable vector-based search. This is particularly useful in scenarios where quick retrieval of relevant document sections is needed, such as in large-scale document management systems or AI-driven content analysis.</p>"},{"location":"api_reference/components/rag_node/#11-document-embedding-and-chunking","title":"1.1 Document Embedding and Chunking","text":"<p>This use case involves embedding documents using a specified model and chunking them into manageable pieces for efficient search.</p> <p>python from railtracks.prebuilt.rag_node import rag_node</p>"},{"location":"api_reference/components/rag_node/#example-usage","title":"Example usage","text":"<p>documents = [\"Document 1 content\", \"Document 2 content\"] node = rag_node(documents) results = node(\"search query\") print(results)</p>"},{"location":"api_reference/components/rag_node/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/rag_node/#def-rag_nodedocuments-embed_model-token_count_model-chunk_size-chunk_overlap","title":"<code>def rag_node(documents, embed_model, token_count_model, chunk_size, chunk_overlap)</code>","text":"<p>Creates a rag node that allows you to vector the search the provided documents.</p> <p>Args:</p>"},{"location":"api_reference/components/rag_node/#3-architectural-design","title":"3. Architectural Design","text":"<p>The RAG Node component is built around the concept of embedding and chunking documents to facilitate efficient vector-based search. It leverages the RAG core functionalities to manage document processing and retrieval.</p>"},{"location":"api_reference/components/rag_node/#31-core-design-principles","title":"3.1 Core Design Principles","text":"<ul> <li>Embedding and Chunking: The component uses the <code>EmbeddingService</code> and <code>TextChunkingService</code> to process documents, ensuring that each document is broken down into chunks and embedded for search.</li> <li>Vector Store Integration: The component integrates with a vector store to manage and search through the embedded document vectors.</li> <li>Modular Configuration: The component allows for flexible configuration of embedding models, chunk sizes, and overlap, making it adaptable to various use cases.</li> </ul>"},{"location":"api_reference/components/rag_node/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/rag_node/#41-configuration-and-dependencies","title":"4.1 Configuration and Dependencies","text":"<ul> <li>Embedding Model: The default embedding model is <code>text-embedding-3-small</code>, but this can be configured as needed.</li> <li>Chunking Parameters: The default chunk size is 1000 tokens with an overlap of 200 tokens, which can be adjusted based on document size and search requirements.</li> <li>Dependencies: Ensure that the required models and services are available and properly configured in the environment.</li> </ul>"},{"location":"api_reference/components/rag_node/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/rag_node/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>../rag_node.py</code>: Implements the RAG Node functionality for document embedding and chunking.</li> <li><code>../rag_core.py</code>: Provides core functionalities for the RAG system, including embedding and vector store management.</li> </ul>"},{"location":"api_reference/components/rag_node/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>../rag_core.md</code>: Documentation for the RAG core component, detailing the underlying services and architecture.</li> </ul>"},{"location":"api_reference/components/rag_node/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>../rag_system.md</code>: Documentation for the RAG system feature, outlining the overall system architecture and integration points.</li> </ul>"},{"location":"api_reference/components/rag_node/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/rag_utilities/","title":"RAG Utilities","text":"<p>The RAG Utilities component provides essential utility functions for file operations and tokenization within the RAG system, facilitating efficient data handling and processing.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/rag_utilities/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/rag_utilities/#1-purpose","title":"1. Purpose","text":"<p>The RAG Utilities component is designed to support the RAG system by providing utility functions that handle file operations and tokenization. These utilities are crucial for managing data efficiently, ensuring smooth operation of the RAG system.</p>"},{"location":"api_reference/components/rag_utilities/#11-file-operations","title":"1.1 File Operations","text":"<p>The file operations utility functions allow for listing, reading, writing, and retrieving information about files. These operations are essential for managing data files within the RAG system.</p> <p>python</p>"},{"location":"api_reference/components/rag_utilities/#example-listing-all-txt-files-in-a-directory","title":"Example: Listing all .txt files in a directory","text":"<p>files = list_files(\"/path/to/directory\", extensions=[\".txt\"])</p>"},{"location":"api_reference/components/rag_utilities/#12-tokenization","title":"1.2 Tokenization","text":"<p>The tokenization utilities provide functionality to encode and decode text using different tokenization models, which is vital for processing textual data in the RAG system.</p> <p>python</p>"},{"location":"api_reference/components/rag_utilities/#example-encoding-text-using-the-loragtokenizer","title":"Example: Encoding text using the LORAGTokenizer","text":"<p>tokenizer = LORAGTokenizer() tokens = tokenizer.encode(\"Sample text\")</p>"},{"location":"api_reference/components/rag_utilities/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/rag_utilities/#class-loragtokenizer","title":"<code>class LORAGTokenizer</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/rag_utilities/#__init__self-token_encoding","title":"<code>.__init__(self, token_encoding)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/rag_utilities/#decodeself-tokens","title":"<code>.decode(self, tokens)</code>","text":"<p>Detokenize a list of tokens into text.</p>"},{"location":"api_reference/components/rag_utilities/#encodeself-text","title":"<code>.encode(self, text)</code>","text":"<p>Tokenize a string into a list of tokens.</p>"},{"location":"api_reference/components/rag_utilities/#count_tokenself-text","title":"<code>.count_token(self, text)</code>","text":"<p>Get the number of tokens in a string.</p>"},{"location":"api_reference/components/rag_utilities/#class-tokenizer","title":"<code>class Tokenizer</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/rag_utilities/#__init__self-model","title":"<code>.__init__(self, model)</code>","text":"<p>Initialize the tokenizer with a specific model.</p> <p>Args:     model: Model name (e.g., 'gpt-3.5-turbo')</p>"},{"location":"api_reference/components/rag_utilities/#encodeself-text_1","title":"<code>.encode(self, text)</code>","text":"<p>Tokenize a string into a list of tokens.</p> <p>Args:     text: Text to tokenize</p> <p>Returns:     List of token IDs</p>"},{"location":"api_reference/components/rag_utilities/#decodeself-tokens_1","title":"<code>.decode(self, tokens)</code>","text":"<p>Detokenize a list of tokens into text.</p> <p>Args:     tokens: List of token IDs or a single token string</p> <p>Returns:     Decoded text</p>"},{"location":"api_reference/components/rag_utilities/#count_tokenself-text_1","title":"<code>.count_token(self, text)</code>","text":"<p>Get the number of tokens in a string.</p> <p>Args:     text: Text to count tokens for</p> <p>Returns:     Number of tokens</p>"},{"location":"api_reference/components/rag_utilities/#def-list_filesdirectory-extensions-recursive","title":"<code>def list_files(directory, extensions, recursive)</code>","text":"<p>List files in a directory.</p> <p>Args:     directory: Directory to list files from     extensions: List of file extensions to include (e.g., ['.txt', '.md'])     recursive: Whether to search recursively</p> <p>Returns:     List of file paths</p>"},{"location":"api_reference/components/rag_utilities/#def-read_filefile_path","title":"<code>def read_file(file_path)</code>","text":"<p>Read a file.</p> <p>Args:     file_path: Path to the file</p> <p>Returns:     File content</p>"},{"location":"api_reference/components/rag_utilities/#def-write_filefile_path-content","title":"<code>def write_file(file_path, content)</code>","text":"<p>Write content to a file.</p> <p>Args:     file_path: Path to the file     content: Content to write</p>"},{"location":"api_reference/components/rag_utilities/#def-get_file_infofile_path","title":"<code>def get_file_info(file_path)</code>","text":"<p>Get information about a file.</p> <p>Args:     file_path: Path to the file</p> <p>Returns:     Dictionary containing file information</p>"},{"location":"api_reference/components/rag_utilities/#3-architectural-design","title":"3. Architectural Design","text":""},{"location":"api_reference/components/rag_utilities/#31-tokenization-classes","title":"3.1 Tokenization Classes","text":"<ul> <li>LORAGTokenizer</li> <li>Utilizes the <code>tiktoken</code> library to encode and decode text.</li> <li> <p>Supports counting tokens in a string, which is useful for understanding data size and processing needs.</p> </li> <li> <p>Tokenizer</p> </li> <li>Provides a model-based approach to tokenization using the <code>litellm</code> library.</li> <li>Allows for flexible tokenization based on different models, such as \"gpt-3.5-turbo\".</li> </ul>"},{"location":"api_reference/components/rag_utilities/#32-file-operations","title":"3.2 File Operations","text":"<ul> <li>list_files</li> <li> <p>Uses <code>os</code> and <code>glob</code> to list files in a directory, supporting recursive search and filtering by file extension.</p> </li> <li> <p>read_file</p> </li> <li> <p>Reads the content of a file, ensuring the file exists before attempting to read.</p> </li> <li> <p>write_file</p> </li> <li> <p>Writes content to a file, creating directories if they do not exist.</p> </li> <li> <p>get_file_info</p> </li> <li>Retrieves metadata about a file, such as name, size, and modification time.</li> </ul>"},{"location":"api_reference/components/rag_utilities/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/rag_utilities/#41-tokenization","title":"4.1 Tokenization","text":"<ul> <li>The <code>LORAGTokenizer</code> and <code>Tokenizer</code> classes rely on external libraries (<code>tiktoken</code> and <code>litellm</code>), which must be installed and properly configured.</li> </ul>"},{"location":"api_reference/components/rag_utilities/#42-file-operations","title":"4.2 File Operations","text":"<ul> <li>Ensure that the directory paths provided to file operations exist and are accessible to avoid runtime errors.</li> <li>The <code>list_files</code> function can be resource-intensive if used on large directories with recursive search enabled.</li> </ul>"},{"location":"api_reference/components/rag_utilities/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/rag_utilities/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>../packages/railtracks/src/railtracks/rag/utils.py</code>: Contains the implementation of utility functions for file operations and tokenization.</li> </ul>"},{"location":"api_reference/components/rag_utilities/#52-related-feature-files","title":"5.2 Related Feature Files","text":"<ul> <li><code>../features/rag_system.md</code>: Documents the RAG system, which utilizes these utility functions.</li> </ul>"},{"location":"api_reference/components/rag_utilities/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/request_management/","title":"Request Management","text":"<p>The Request Management component is responsible for managing a system of requests and their relationships, providing mechanisms to create, update, and query requests within the system.</p> <p>Version: 0.0.1</p> <p>Component Contact: @github_username</p>"},{"location":"api_reference/components/request_management/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/request_management/#1-purpose","title":"1. Purpose","text":"<p>The Request Management component is designed to handle the lifecycle of requests within a system. It allows for the creation, updating, and querying of requests, which can be linked to form a directed graph of operations. This is particularly useful in systems where operations are dependent on the completion of previous tasks.</p>"},{"location":"api_reference/components/request_management/#11-creating-a-request","title":"1.1 Creating a Request","text":"<p>Creating a request is a fundamental operation that initializes a new request in the system.</p> <p>python from railtracks.state.request import RequestForest, Stamp</p> <p>request_forest = RequestForest() identifier = request_forest.create(     identifier=\"unique_id\",     source_id=None,     sink_id=\"sink_node\",     input_args=(),     input_kwargs={},     stamp=Stamp() )</p>"},{"location":"api_reference/components/request_management/#12-updating-a-request","title":"1.2 Updating a Request","text":"<p>Updating a request allows you to modify the output of an existing request, marking it as completed or failed.</p> <p>python request_forest.update(     identifier=\"unique_id\",     output=\"result_data\",     stamp=Stamp() )</p>"},{"location":"api_reference/components/request_management/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/request_management/#class-requesttemplateabstractlinkedobject","title":"<code>class RequestTemplate(AbstractLinkedObject)</code>","text":"<p>A simple object containing details about a request in the system.</p>"},{"location":"api_reference/components/request_management/#to_edgeself","title":"<code>.to_edge(self)</code>","text":"<p>Converts the request template to an edge representation.</p>"},{"location":"api_reference/components/request_management/#closedself","title":"<code>.closed(self)</code>","text":"<p>If the request has an output it is closed</p>"},{"location":"api_reference/components/request_management/#is_insertionself","title":"<code>.is_insertion(self)</code>","text":"<p>If the request is an insertion request it will return True.</p>"},{"location":"api_reference/components/request_management/#statusself","title":"<code>.status(self)</code>","text":"<p>Gets the current status of the request.</p>"},{"location":"api_reference/components/request_management/#get_all_parentsself","title":"<code>.get_all_parents(self)</code>","text":"<p>Recursely collects all the parents for the request.</p>"},{"location":"api_reference/components/request_management/#get_terminal_parentself","title":"<code>.get_terminal_parent(self)</code>","text":"<p>Returns the terminal parent of the request.</p> <p>If this request is the parent then it will return itself.</p>"},{"location":"api_reference/components/request_management/#duration_detailself","title":"<code>.duration_detail(self)</code>","text":"<p>Returns the difference in time between the parent and the current request stamped time.</p>"},{"location":"api_reference/components/request_management/#generate_idcls","title":"<code>.generate_id(cls)</code>","text":"<p>Generates a new unique identifier for the request. This is suitable for use as a request identifier.</p>"},{"location":"api_reference/components/request_management/#downstreamcls-requests-source_id","title":"<code>.downstream(cls, requests, source_id)</code>","text":"<p>Collects the requests one level downstream from the provided source_id.</p>"},{"location":"api_reference/components/request_management/#upstreamcls-requests-sink_id","title":"<code>.upstream(cls, requests, sink_id)</code>","text":"<p>Collects the requests one level upstream from the provided sink_id.</p>"},{"location":"api_reference/components/request_management/#all_downstreamcls-requests-source_id","title":"<code>.all_downstream(cls, requests, source_id)</code>","text":"<p>Collects all the downstream requests from the provided source_id.</p>"},{"location":"api_reference/components/request_management/#open_tailscls-requests-source_id","title":"<code>.open_tails(cls, requests, source_id)</code>","text":"<p>Traverses down the provided tree to find all the open tails.</p> <p>Open Tail: is defined as any node which currently holds an open request and does not have any open ones beneath it.</p>"},{"location":"api_reference/components/request_management/#children_completecls-requests-source_node_id","title":"<code>.children_complete(cls, requests, source_node_id)</code>","text":"<p>Checks if all the downstream requests of a given parent node are complete. If so returns True.  Otherwise, returns False.</p>"},{"location":"api_reference/components/request_management/#class-requestforestforestrequesttemplate","title":"<code>class RequestForest(Forest[RequestTemplate])</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/request_management/#__init__self-request_heap","title":"<code>.__init__(self, request_heap)</code>","text":"<p>Creates a new instance of a request heap with no objects present.</p>"},{"location":"api_reference/components/request_management/#to_edgesself","title":"<code>.to_edges(self)</code>","text":"<p>Converts the current heap into a list of <code>Edge</code> objects.</p>"},{"location":"api_reference/components/request_management/#createself-identifier-source_id-sink_id-input_args-input_kwargs-stamp","title":"<code>.create(self, identifier, source_id, sink_id, input_args, input_kwargs, stamp)</code>","text":"<p>Creates a new instance of a request from the provided details and places it into the heap.</p> <p>Args:     identifier (str): The identifier of the request     source_id (Optional[str]): The node id of the source, None if it is an insertion request.     sink_id (str): The node id in the sink.     input_args (Tuple): The input arguments for the request     input_kwargs (Dict): The input keyword arguments for the request     stamp (Stamp): The stamp that you would like this request to be tied to.</p> <p>Returns:     str: The identifier of the request that was created.</p>"},{"location":"api_reference/components/request_management/#updateself-identifier-output-stamp","title":"<code>.update(self, identifier, output, stamp)</code>","text":"<p>Updates the heap with the provided request details. Note you must call this function on a request that exist in the heap.</p> <p>The function will replace the old request with a new updated one with the provided output attached to the provided stamp.</p> <p>I will outline the special cases for this function: 1. If you have provided a request id that does not exist in the heap, it will raise <code>RequestDoesNotExistError</code></p> <p>Args:     identifier (str): The identifier of the request     output (Optional[RequestOutput]): The output of the request, None if the request is not completed.     stamp (Stamp): The stamp that you would like this request addition to be tied to.</p> <p>Raises:     RequestDoesNotExistError: If the request with the provided identifier does not exist in the heap.</p>"},{"location":"api_reference/components/request_management/#childrenself-parent_id","title":"<code>.children(self, parent_id)</code>","text":"<p>Finds all the children of the provided parent_id.</p>"},{"location":"api_reference/components/request_management/#generate_graphcls-heap","title":"<code>.generate_graph(cls, heap)</code>","text":"<p>Generates a dictionary representation contain the edges in the graph. The key of the dictionary is the source and the value is a list of tuples where the first element is the sink_id and the second element is the request        id. Complexity: O(n) where n is the number of identifiers in the heap.</p> <p>Args:     heap (Dict[str, RequestTemplate]): The heap of requests to generate the graph from.</p> <p>Returns:     Dict[str, List[Tuple[str, str]]]: The graph representation of the heap.</p>"},{"location":"api_reference/components/request_management/#get_request_from_child_idself-child_id","title":"<code>.get_request_from_child_id(self, child_id)</code>","text":"<p>Gets the request where this child_id is the sink_id of the request.</p> <p>Via the invariants of the system. There must only be 1 request that satisfies the above requirement.</p>"},{"location":"api_reference/components/request_management/#open_tailsself","title":"<code>.open_tails(self)</code>","text":"<p>Collects the current open tails in the heap. See <code>RequestTemplate.open_tails</code> for more information.</p>"},{"location":"api_reference/components/request_management/#children_requests_completeself-parent_node_id","title":"<code>.children_requests_complete(self, parent_node_id)</code>","text":"<p>Checks if all the downstream requests (one level down) if the given parent node are complete. If they are  then it will return the request id of the parent node. Otherwise, it will return None.</p> <p>Note that you are providing the node_id of the parent node and downstream requests of that node is defined  as any of the requests which have the matching parent_node.</p> <p>Args:     parent_node_id (str): The parent node id</p> <p>Returns:     The request_id string of the parent node if all the children are complete otherwise None.</p>"},{"location":"api_reference/components/request_management/#insertion_requestself","title":"<code>.insertion_request(self)</code>","text":"<p>Collects a list of all the insertion requests in the heap.</p> <p>They will be returned in the order that they were created.</p>"},{"location":"api_reference/components/request_management/#answerself","title":"<code>.answer(self)</code>","text":"<p>Collects the answer to the insertion request.</p> <p>The behavior of the function can be split into two cases:</p> <ol> <li>There is either 1 or 0 insertion requests present:</li> <li> <p>In this case, it will return the output of the insertion request if it exists, otherwise None</p> </li> <li> <p>There is more than 1 insertion request:</p> </li> <li>Returns an ordered list of outputs of all the insertion requests. If one has not yet completed, it will    return None in that index.</li> </ol>"},{"location":"api_reference/components/request_management/#3-architectural-design","title":"3. Architectural Design","text":""},{"location":"api_reference/components/request_management/#31-request-template","title":"3.1 Request Template","text":"<ul> <li>Request Representation: Each request is represented by a <code>RequestTemplate</code> object, which includes details such as source and sink identifiers, input and output data, and parent-child relationships.</li> <li>Graph Structure: Requests can be linked to form a directed graph, allowing for complex workflows where requests depend on the completion of others.</li> </ul>"},{"location":"api_reference/components/request_management/#32-request-forest","title":"3.2 Request Forest","text":"<ul> <li>Heap Management: The <code>RequestForest</code> class manages a collection of requests, stored in a heap. It provides thread-safe operations to create, update, and query requests.</li> <li>Concurrency Considerations: The use of locks ensures that operations on the request heap are thread-safe, preventing race conditions.</li> </ul>"},{"location":"api_reference/components/request_management/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/request_management/#41-concurrency-and-thread-safety","title":"4.1 Concurrency and Thread Safety","text":"<ul> <li>Locking Mechanism: The <code>RequestForest</code> class uses a locking mechanism to ensure that operations on the request heap are thread-safe. This is crucial for preventing race conditions in a multi-threaded environment.</li> </ul>"},{"location":"api_reference/components/request_management/#42-error-handling","title":"4.2 Error Handling","text":"<ul> <li>Custom Exceptions: The component defines custom exceptions such as <code>RequestDoesNotExistError</code> and <code>RequestAlreadyExistsError</code> to handle specific error cases related to request management.</li> </ul>"},{"location":"api_reference/components/request_management/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/request_management/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>../state/request.py</code>: Contains the implementation of the Request Management component, including the <code>RequestTemplate</code> and <code>RequestForest</code> classes.</li> </ul>"},{"location":"api_reference/components/request_management/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>../components/state_management.md</code>: Provides an overview of the state management system, of which the Request Management component is a part.</li> </ul>"},{"location":"api_reference/components/request_management/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>../features/state_management.md</code>: Describes the state management features and how they integrate with the Request Management component.</li> </ul>"},{"location":"api_reference/components/request_management/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/response_handling/","title":"Response Handling","text":"<p>The Response Handling component defines response objects for handling outputs from LLM nodes, supporting both structured and string responses. It plays a crucial role in the Railtracks system by encapsulating the outputs from LLM nodes, allowing for consistent and structured interaction management.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/response_handling/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/response_handling/#1-purpose","title":"1. Purpose","text":"<p>The primary purpose of the Response Handling component is to provide a standardized way to encapsulate and manage responses from LLM nodes within the Railtracks system. This component supports two main types of responses: structured and string-based, catering to different output needs of LLM nodes.</p>"},{"location":"api_reference/components/response_handling/#11-handling-structured-responses","title":"1.1 Handling Structured Responses","text":"<p>Structured responses are crucial when the output from an LLM node needs to be in a specific format or schema, often defined by a Pydantic model. This ensures that the data adheres to expected structures, facilitating easier downstream processing and validation.</p> <p>python from pydantic import BaseModel from railtracks.llm import MessageHistory from railtracks.nodes.concrete.response import StructuredResponse</p> <p>class MyModel(BaseModel):     field1: str     field2: int</p> <p>message_history = MessageHistory() response = StructuredResponse(model=MyModel(field1=\"value\", field2=42), message_history=message_history) print(response.structured)</p>"},{"location":"api_reference/components/response_handling/#12-handling-string-responses","title":"1.2 Handling String Responses","text":"<p>String responses are used when the output from an LLM node is a simple text string. This is useful for scenarios where the response does not need to adhere to a specific structure.</p> <p>python from railtracks.llm import MessageHistory from railtracks.nodes.concrete.response import StringResponse</p> <p>message_history = MessageHistory() response = StringResponse(content=\"This is a simple string response.\", message_history=message_history) print(response.text)</p>"},{"location":"api_reference/components/response_handling/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/response_handling/#class-llmresponsegeneric_t","title":"<code>class LLMResponse(Generic[_T])</code>","text":"<p>A special response object designed to be returned by an LLM node in the RT system.</p> <p>Args:     content: The content of the response, which can be any content of a message     message_history: The history of messages exchanged during the interaction.</p>"},{"location":"api_reference/components/response_handling/#__init__self-content-message_history","title":"<code>.__init__(self, content, message_history)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/response_handling/#class-structuredresponsellmresponse_tbasemodel","title":"<code>class StructuredResponse(LLMResponse[_TBaseModel])</code>","text":"<p>A specialized response object for structured outputs from LLMs.</p> <p>Args:     model: The structured model that defines the content of the response.     message_history: The history of messages exchanged during the interaction.</p>"},{"location":"api_reference/components/response_handling/#__init__self-model-message_history","title":"<code>.__init__(self, model, message_history)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/response_handling/#structuredself","title":"<code>.structured(self)</code>","text":"<p>Returns the structured content of the response.</p>"},{"location":"api_reference/components/response_handling/#class-stringresponsellmresponsestr","title":"<code>class StringResponse(LLMResponse[str])</code>","text":"<p>A specialized response object for string outputs from LLMs.</p> <p>Args:     content: The string content of the response.     message_history: The history of messages exchanged during the interaction.</p>"},{"location":"api_reference/components/response_handling/#__init__self-content-message_history_1","title":"<code>.__init__(self, content, message_history)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/response_handling/#textself","title":"<code>.text(self)</code>","text":"<p>Returns the text content of the response.</p>"},{"location":"api_reference/components/response_handling/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Response Handling component is designed to encapsulate the outputs from LLM nodes in a consistent manner. It leverages Python's type system and Pydantic for structured data validation, ensuring that responses are both type-safe and easy to work with.</p>"},{"location":"api_reference/components/response_handling/#31-core-design-principles","title":"3.1 Core Design Principles","text":"<ul> <li>Type Safety: By using Python's generics and Pydantic models, the component ensures that responses are type-safe and adhere to expected schemas.</li> <li>Flexibility: Supports both structured and unstructured (string) responses, catering to a wide range of use cases.</li> <li>Consistency: Provides a unified interface for handling different types of responses, simplifying the interaction with LLM nodes.</li> </ul>"},{"location":"api_reference/components/response_handling/#4-important-considerations","title":"4. Important Considerations","text":"<ul> <li>Dependencies: This component relies on Pydantic for data validation and type management. Ensure that Pydantic is included in your project's dependencies.</li> <li>Performance: While the component is designed to be efficient, the use of Pydantic models can introduce some overhead. Consider this when working with large datasets or in performance-critical applications.</li> <li>Security: Ensure that any data passed to the response objects is sanitized and validated to prevent security vulnerabilities.</li> </ul>"},{"location":"api_reference/components/response_handling/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/response_handling/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>../packages/railtracks/src/railtracks/nodes/concrete/response.py</code>: Defines the <code>LLMResponse</code>, <code>StructuredResponse</code>, and <code>StringResponse</code> classes.</li> </ul>"},{"location":"api_reference/components/response_handling/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>../components/llm_node_base.md</code>: Provides foundational information about LLM nodes, which utilize the response handling component.</li> </ul>"},{"location":"api_reference/components/response_handling/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>../features/node_management.md</code>: Discusses the management of nodes, including those that produce responses handled by this component.</li> </ul>"},{"location":"api_reference/components/response_handling/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/rt_state_management/","title":"RT State Management","text":"<p>The RT State Management component is responsible for managing the state of a request completion system, handling node execution, exceptions, and logging. It plays a crucial role in ensuring that requests are processed efficiently and that any issues are logged and managed appropriately.</p> <p>Version: 0.0.1</p> <p>Component Contact: @github_username</p>"},{"location":"api_reference/components/rt_state_management/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/rt_state_management/#1-purpose","title":"1. Purpose","text":"<p>The RT State Management component is designed to manage the lifecycle of requests within a system. It handles the creation, execution, and completion of requests, as well as managing exceptions and logging actions. This component is essential for maintaining the integrity and efficiency of the request processing system.</p>"},{"location":"api_reference/components/rt_state_management/#11-request-lifecycle-management","title":"1.1 Request Lifecycle Management","text":"<p>The primary use case of this component is to manage the lifecycle of requests. This includes creating new requests, executing them, handling any exceptions that occur, and logging the results.</p> <p>python</p>"},{"location":"api_reference/components/rt_state_management/#example-of-creating-and-handling-a-request","title":"Example of creating and handling a request","text":"<p>state = RTState(execution_info, executor_config, coordinator, publisher) await state.call_nodes(parent_node_id=\"node_1\", request_id=None, node=my_node, args=(), kwargs={})</p>"},{"location":"api_reference/components/rt_state_management/#12-exception-handling-and-logging","title":"1.2 Exception Handling and Logging","text":"<p>Another critical use case is handling exceptions that occur during request execution and logging these events for future analysis.</p> <p>python</p>"},{"location":"api_reference/components/rt_state_management/#example-of-handling-an-exception","title":"Example of handling an exception","text":"<p>try:     await state.call_nodes(parent_node_id=\"node_1\", request_id=None, node=my_node, args=(), kwargs={}) except Exception as e:     state.logger.error(\"An error occurred\", exc_info=e)</p>"},{"location":"api_reference/components/rt_state_management/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/rt_state_management/#3-architectural-design","title":"3. Architectural Design","text":"<p>The RT State Management component is designed to be a comprehensive state management system for request processing. It is built around several key principles and design decisions:</p>"},{"location":"api_reference/components/rt_state_management/#31-core-philosophy-design-principles","title":"3.1 Core Philosophy &amp; Design Principles","text":"<ul> <li>Stateful Management: The component maintains a stateful representation of the request processing system, allowing for detailed tracking and management of requests.</li> <li>Asynchronous Execution: Utilizes asynchronous programming to handle multiple requests concurrently, improving performance and responsiveness.</li> <li>Robust Exception Handling: Implements comprehensive exception handling to ensure that errors are logged and managed without disrupting the entire system.</li> </ul>"},{"location":"api_reference/components/rt_state_management/#32-high-level-architecture-data-flow","title":"3.2 High-Level Architecture &amp; Data Flow","text":"<p>The component is structured around the <code>RTState</code> class, which manages the lifecycle of requests. It interacts with other components such as the <code>Coordinator</code>, <code>RTPublisher</code>, and <code>ExecutionInfo</code> to execute requests and handle results.</p> <ul> <li>Data Flow: Requests are created and executed through the <code>call_nodes</code> method, with results being handled by the <code>handle_result</code> method. Exceptions are managed through the <code>_handle_failed_request</code> method.</li> </ul>"},{"location":"api_reference/components/rt_state_management/#33-key-design-decisions-trade-offs","title":"3.3 Key Design Decisions &amp; Trade-offs","text":"<ul> <li>Asynchronous Task Management: Chose asynchronous task management to improve performance, at the cost of increased complexity in handling concurrency.</li> <li>Centralized Logging: Centralized logging within the <code>RTState</code> class to ensure all actions are recorded, which aids in debugging and analysis.</li> </ul>"},{"location":"api_reference/components/rt_state_management/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/rt_state_management/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>External Services: The component relies on external services such as the <code>RTPublisher</code> for message handling.</li> <li>Configuration: Requires an <code>ExecutorConfig</code> object to configure execution parameters.</li> </ul>"},{"location":"api_reference/components/rt_state_management/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>Concurrency: Designed to handle multiple requests concurrently, but care must be taken to manage resources effectively.</li> <li>Error Handling: While robust, the error handling system can introduce latency if not managed properly.</li> </ul>"},{"location":"api_reference/components/rt_state_management/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/rt_state_management/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>state.py</code>: Contains the implementation of the <code>RTState</code> class and its methods.</li> <li><code>info.py</code>: Defines the <code>ExecutionInfo</code> class used for managing execution state information.</li> <li><code>utils.py</code>: Provides utility functions for managing state information.</li> </ul>"},{"location":"api_reference/components/rt_state_management/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>context/central.py</code>: Manages context variables and provides utility functions for context management.</li> </ul>"},{"location":"api_reference/components/rt_state_management/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/serialization/","title":"Serialization","text":"<p>The Serialization component provides custom serialization logic for specific object types using a custom JSON encoder. It is designed to extend the default JSON encoding capabilities to handle complex objects used within the system.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/serialization/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/serialization/#1-purpose","title":"1. Purpose","text":"<p>The Serialization component is primarily used to convert complex objects into a JSON-compatible format. This is essential for tasks such as logging, data storage, and inter-process communication where JSON is the preferred data interchange format.</p>"},{"location":"api_reference/components/serialization/#11-encoding-complex-objects","title":"1.1 Encoding Complex Objects","text":"<p>The component supports encoding a variety of complex objects, including but not limited to <code>Edge</code>, <code>Vertex</code>, <code>Stamp</code>, <code>RequestDetails</code>, <code>Message</code>, <code>ToolResponse</code>, <code>ToolCall</code>, <code>LatencyDetails</code>, and Pydantic's <code>BaseModel</code>. This is crucial for ensuring that these objects can be easily serialized and deserialized across different parts of the system.</p> <p>python from railtracks.state.serialize import RTJSONEncoder import json</p>"},{"location":"api_reference/components/serialization/#example-of-encoding-a-complex-object","title":"Example of encoding a complex object","text":"<p>edge = Edge(source=\"A\", target=\"B\", identifier=\"edge1\", stamp=Stamp(...), details={}, parent=None) json_data = json.dumps(edge, cls=RTJSONEncoder)</p>"},{"location":"api_reference/components/serialization/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/serialization/#def-encoder_extendero","title":"<code>def encoder_extender(o)</code>","text":"<p>Extends the encoding of supported types to their dictionary representation.</p> <p>We support the following types as of right now: - Edge - Vertex - Stamp - RequestDetails - Message - ToolResponse - ToolCall - LatencyDetails - BaseModel (Pydantic models)</p>"},{"location":"api_reference/components/serialization/#def-encode_tool_calltool_call","title":"<code>def encode_tool_call(tool_call)</code>","text":"<p>Encodes a ToolCall object to a dictionary representation.</p>"},{"location":"api_reference/components/serialization/#def-encode_latency_detailslatency_details","title":"<code>def encode_latency_details(latency_details)</code>","text":"<p>Encodes LatencyDetails to a dictionary representation.</p>"},{"location":"api_reference/components/serialization/#def-encode_edgeedge","title":"<code>def encode_edge(edge)</code>","text":"<p>Encodes an Edge object to a dictionary representation.</p>"},{"location":"api_reference/components/serialization/#def-encode_vertexvertex","title":"<code>def encode_vertex(vertex)</code>","text":"<p>Encodes a Vertex object to a dictionary representation.</p>"},{"location":"api_reference/components/serialization/#def-encode_stampstamp","title":"<code>def encode_stamp(stamp)</code>","text":"<p>Encodes a Stamp object to a dictionary representation.</p>"},{"location":"api_reference/components/serialization/#def-encode_request_detailsdetails","title":"<code>def encode_request_details(details)</code>","text":"<p>Encodes a RequestDetails object to a dictionary representation.</p>"},{"location":"api_reference/components/serialization/#def-encode_messagemessage","title":"<code>def encode_message(message)</code>","text":"<p>Encodes a Message object to a dictionary representation.</p>"},{"location":"api_reference/components/serialization/#def-encode_contentcontent","title":"<code>def encode_content(content)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/serialization/#def-encode_base_modelmodel","title":"<code>def encode_base_model(model)</code>","text":"<p>Encodes a BaseModel object to a dictionary representation.</p>"},{"location":"api_reference/components/serialization/#class-rtjsonencoderjsonjsonencoder","title":"<code>class RTJSONEncoder(json.JSONEncoder)</code>","text":"<p>A custom JSON encoder that extends the default JSONEncoder to handle specific types used in the system.</p> <p>Please consult <code>supported_types</code> for the list of supported types.</p>"},{"location":"api_reference/components/serialization/#defaultself-o","title":"<code>.default(self, o)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/serialization/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Serialization component is designed to extend the default JSON encoding capabilities by providing a custom encoder, <code>RTJSONEncoder</code>, which leverages the <code>encoder_extender</code> function to handle specific object types. This design allows for flexibility and scalability, as new object types can be easily added to the <code>supported_types</code> tuple and corresponding encoding functions can be implemented.</p>"},{"location":"api_reference/components/serialization/#31-design-considerations","title":"3.1 Design Considerations","text":"<ul> <li>Extensibility: The use of a custom encoder allows for easy extension to support new object types.</li> <li>Maintainability: The current design suggests refactoring the <code>encoder_extender</code> function to use a mapping of types to encoding functions for better scalability and maintainability.</li> <li>Error Handling: The <code>RTJSONEncoder</code> provides a fallback mechanism to handle unsupported types by returning a string representation of the object.</li> </ul>"},{"location":"api_reference/components/serialization/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/serialization/#41-implementation-details","title":"4.1 Implementation Details","text":"<ul> <li>Dependencies: The component relies on the <code>pydantic</code> library for handling <code>BaseModel</code> objects.</li> <li>Performance: The custom encoder is designed to efficiently handle serialization of complex objects, but performance may vary depending on the size and complexity of the objects being serialized.</li> <li>Error Handling: Unsupported types are handled gracefully by the <code>RTJSONEncoder</code>, which attempts to convert them to a string representation.</li> </ul>"},{"location":"api_reference/components/serialization/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/serialization/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>../packages/railtracks/src/railtracks/state/serialize.py</code>: Contains the implementation of the Serialization component.</li> </ul>"},{"location":"api_reference/components/serialization/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>../components/state_management.md</code>: Provides documentation on state management, which is closely related to serialization.</li> </ul>"},{"location":"api_reference/components/serialization/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>../features/state_management.md</code>: Describes the state management feature, which utilizes serialization for handling state data.</li> </ul>"},{"location":"api_reference/components/serialization/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/session_management/","title":"Session Management","text":"<p>The <code>Session</code> component is a crucial part of the Railtracks framework, responsible for managing execution sessions. It sets up essential components like the coordinator and publisher, ensuring seamless execution of tasks within the system.</p> <p>Version: 0.0.1</p> <p>Component Contact: @railtracks_dev</p>"},{"location":"api_reference/components/session_management/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/session_management/#1-purpose","title":"1. Purpose","text":"<p>The <code>Session</code> component is designed to manage the lifecycle of execution sessions in the Railtracks framework. It initializes and configures all necessary components, such as the coordinator and publisher, to facilitate task execution. The <code>Session</code> also handles context management, logging, and state persistence, providing a comprehensive environment for running workflows.</p>"},{"location":"api_reference/components/session_management/#11-session-initialization-and-execution","title":"1.1 Session Initialization and Execution","text":"<p>The primary use case for the <code>Session</code> component is to initialize and manage the execution of tasks within a session. This involves setting up the necessary components and executing tasks in a controlled environment.</p> <p>python import railtracks as rt</p> <p>with rt.Session() as run:     result = rt.call_sync(rt.nodes.NodeA, \"Hello World\")</p>"},{"location":"api_reference/components/session_management/#12-context-and-state-management","title":"1.2 Context and State Management","text":"<p>Another critical use case is managing the context and state of the execution session. The <code>Session</code> ensures that global context variables are correctly registered and cleaned up, and it saves the execution state to a file if configured to do so.</p> <p>python session = rt.Session(context={\"key\": \"value\"}, save_state=True)</p>"},{"location":"api_reference/components/session_management/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/session_management/#class-session","title":"<code>class Session</code>","text":"<p>The main class for managing an execution session.</p> <p>This class is responsible for setting up all the necessary components for running a Railtracks execution, including the coordinator, publisher, and state management.</p> <p>For the configuration parameters of the setting. It will follow this precedence: 1. The parameters in the <code>Session</code> constructor. 2. The parameters in global context variables. 3. The default values.</p> <p>Default Values: - <code>timeout</code>: 150.0 seconds - <code>end_on_error</code>: False - <code>logging_setting</code>: \"REGULAR\" - <code>log_file</code>: None (logs will not be written to a file) - <code>broadcast_callback</code>: None (no callback for broadcast messages) - <code>run_identifier</code>: None (a random identifier will be generated) - <code>prompt_injection</code>: True (the prompt will be automatically injected from context variables) - <code>save_state</code>: True (the state of the execution will be saved to a file at the end of the run in the <code>.railtracks</code> directory)</p> <p>Args:     context (Dict[str, Any], optional): A dictionary of global context variables to be used during the execution.     timeout (float, optional): The maximum number of seconds to wait for a response to your top-level request.     end_on_error (bool, optional): If True, the execution will stop when an exception is encountered.     logging_setting (allowable_log_levels, optional): The setting for the level of logging you would like to have.     log_file (str | os.PathLike | None, optional): The file to which the logs will be written.     broadcast_callback (Callable[[str], None] | Callable[[str], Coroutine[None, None, None]] | None, optional): A callback function that will be called with the broadcast messages.     run_identifier (str | None, optional): A unique identifier for the run.     prompt_injection (bool, optional): If True, the prompt will be automatically injected from context variables.     save_state (bool, optional): If True, the state of the execution will be saved to a file at the end of the run in the <code>.railtracks</code> directory.</p> <p>Example Usage: python import railtracks as rt</p> <p>with rt.Session() as run:     result = rt.call_sync(rt.nodes.NodeA, \"Hello World\")</p>"},{"location":"api_reference/components/session_management/#__init__self-context","title":"<code>.__init__(self, context)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/session_management/#global_config_precedencecls-timeout-end_on_error-logging_setting-log_file-broadcast_callback-run_identifier-prompt_injection-save_state","title":"<code>.global_config_precedence(cls, timeout, end_on_error, logging_setting, log_file, broadcast_callback, run_identifier, prompt_injection, save_state)</code>","text":"<p>Uses the following precedence order to determine the configuration parameters: 1. The parameters in the method parameters. 2. The parameters in global context variables. 3. The default values.</p>"},{"location":"api_reference/components/session_management/#infoself","title":"<code>.info(self)</code>","text":"<p>Returns the current state of the runner.</p> <p>This is useful for debugging and viewing the current state of the run.</p>"},{"location":"api_reference/components/session_management/#3-architectural-design","title":"3. Architectural Design","text":"<p>The <code>Session</code> component is designed to provide a unified interface for managing execution sessions. It integrates various subsystems, such as the coordinator, publisher, and state management, into a cohesive unit.</p>"},{"location":"api_reference/components/session_management/#31-core-philosophy-design-principles","title":"3.1 Core Philosophy &amp; Design Principles","text":"<ul> <li>Modularity: The <code>Session</code> encapsulates all necessary components, allowing for easy management and extension.</li> <li>Flexibility: Configuration options are provided to customize the session's behavior, such as logging settings and error handling.</li> <li>Robustness: The <code>Session</code> ensures proper initialization and cleanup of resources, preventing resource leaks and ensuring consistent execution.</li> </ul>"},{"location":"api_reference/components/session_management/#32-high-level-architecture-data-flow","title":"3.2 High-Level Architecture &amp; Data Flow","text":"<p>The <code>Session</code> initializes the <code>Coordinator</code>, <code>RTPublisher</code>, and <code>RTState</code>, setting up the necessary infrastructure for task execution. It manages the flow of data and control through these components, coordinating task execution and handling completion messages.</p>"},{"location":"api_reference/components/session_management/#33-key-design-decisions-trade-offs","title":"3.3 Key Design Decisions &amp; Trade-offs","text":"<ul> <li>Asynchronous Execution: The use of <code>AsyncioExecutionStrategy</code> allows for efficient task execution, but requires careful management of asynchronous operations.</li> <li>State Persistence: Saving the execution state to a file provides valuable debugging information, but may introduce performance overhead.</li> </ul>"},{"location":"api_reference/components/session_management/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/session_management/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>Environment Variables: Ensure that any required environment variables are set before initializing a <code>Session</code>.</li> <li>Configuration Files: The <code>Session</code> relies on configuration files for logging and execution settings.</li> </ul>"},{"location":"api_reference/components/session_management/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>Concurrency: The <code>Session</code> supports concurrent task execution, but care must be taken to avoid race conditions.</li> <li>Resource Usage: Monitor memory and CPU usage, especially when handling large datasets or complex workflows.</li> </ul>"},{"location":"api_reference/components/session_management/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/session_management/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>session.py</code>: Contains the implementation of the <code>Session</code> class.</li> </ul>"},{"location":"api_reference/components/session_management/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>task_execution.md</code>: Details the task execution process and its integration with the <code>Session</code>.</li> <li><code>pubsub_messaging.md</code>: Describes the pub/sub messaging system used by the <code>Session</code>.</li> </ul>"},{"location":"api_reference/components/session_management/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>task_execution.md</code>: Provides an overview of task execution features and their relationship with the <code>Session</code>.</li> </ul>"},{"location":"api_reference/components/session_management/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/state_management/","title":"State Management","text":"<p>The <code>Forest</code> component is designed to manage a collection of linked objects, tracking their history and state over time. It provides a mechanism to record and access the history of immutable objects, allowing developers to manage state changes efficiently.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/state_management/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/state_management/#1-purpose","title":"1. Purpose","text":"<p>The <code>Forest</code> component is primarily used to manage and track the history of objects by linking them together based on identifiers. This allows developers to access any point in the object's history, making it useful for applications that require state management and historical data tracking.</p>"},{"location":"api_reference/components/state_management/#11-managing-object-history","title":"1.1 Managing Object History","text":"<p>The primary use case of the <code>Forest</code> component is to manage the history of objects. By linking objects with the same identifier, developers can create a timeline of immutable objects that can be accessed at any point in time.</p> <p>python from railtracks.state.forest import Forest, AbstractLinkedObject</p>"},{"location":"api_reference/components/state_management/#define-a-custom-linked-object","title":"Define a custom linked object","text":"<p>class MyLinkedObject(AbstractLinkedObject):     pass</p>"},{"location":"api_reference/components/state_management/#initialize-a-forest","title":"Initialize a Forest","text":"<p>forest = Forest()</p>"},{"location":"api_reference/components/state_management/#add-objects-to-the-forest","title":"Add objects to the forest","text":"<p>obj1 = MyLinkedObject(identifier=\"obj1\", stamp=Stamp(step=1), parent=None) forest._update_heap(obj1)</p>"},{"location":"api_reference/components/state_management/#access-the-most-recent-object","title":"Access the most recent object","text":"<p>recent_obj = forest[\"obj1\"]</p>"},{"location":"api_reference/components/state_management/#12-time-travel-for-objects","title":"1.2 Time Travel for Objects","text":"<p>Another important use case is the ability to revert objects to a previous state using the <code>time_machine</code> method. This allows developers to \"time travel\" objects to a specific point in their history.</p> <p>python</p>"},{"location":"api_reference/components/state_management/#revert-objects-to-a-previous-state","title":"Revert objects to a previous state","text":"<p>forest.time_machine(step=1, item_list=[\"obj1\"])</p>"},{"location":"api_reference/components/state_management/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/state_management/#class-forestgenerict","title":"<code>class Forest(Generic[T])</code>","text":"<p>A general base class for any heap object. These heap objects have a non-intuitive structure. A common use case of a type like this is used to record history of some object. By linking together objects with the same identifier, you can create a history of immutable objects that can be accessed at any point in time. You can also build out any of your own desired functionality of the object by subclassing <code>Forest</code>.</p> <p>The general principle of the object is you can add any subclass of <code>AbstractLinkedObject</code> to the heap. The heap will track any object with identical identifiers as connected objects. Any object which you add that already exists in the heap (and by that I mean an object with the same identifier) must have a parent in the graph that matches that object. Once you have added that new object it is now the object that you can access from the heap. Conveniently because all <code>T</code> are immutable, you can pass around the objects without worry of pass by reference bugs.</p>"},{"location":"api_reference/components/state_management/#__init__self-heap","title":"<code>.__init__(self, heap)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/state_management/#heapself","title":"<code>.heap(self)</code>","text":"<p>Returns a passed by value dictionary of all the data in the heap.</p> <p>NOTE: You can do whatever you please with this object, and it will not affect the inner workings of the object.</p>"},{"location":"api_reference/components/state_management/#full_dataself-at_step","title":"<code>.full_data(self, at_step)</code>","text":"<p>Returns a passed by value list of all the data in the heap.</p> <p>NOTE: You can do whatever you please with this object, and it will not affect the inner workings of the object.</p>"},{"location":"api_reference/components/state_management/#time_machineself-step-item_list","title":"<code>.time_machine(self, step, item_list)</code>","text":"<p>This function mutates the state of self such that all items you have provided are returned to state at the given step. If you have not provided any items it will be assumed that you want the entire heap to be returned to the given step.</p> <p>Note that it will include all items with the given step and less (it is inclusive of the step).</p> <p>If none of the items with the given ID are less than or equal to the given step, then the item will be removed.</p> <p>Args:     step (int): The step to return the items to (inclusive of that step). If none then return the current state.     item_list (Optional[List[str]]): The list of identifiers to return to the given step. If None, then all items         will be returned to the given step. Note an empty list will mean that nothing will happen</p>"},{"location":"api_reference/components/state_management/#3-architectural-design","title":"3. Architectural Design","text":"<p>The <code>Forest</code> component is designed to manage a collection of linked objects, each represented by the <code>AbstractLinkedObject</code> class. The core philosophy is to maintain immutability and provide a mechanism to track object history efficiently.</p>"},{"location":"api_reference/components/state_management/#31-core-design-principles","title":"3.1 Core Design Principles","text":"<ul> <li>Immutability: All objects managed by the <code>Forest</code> are immutable, ensuring that state changes do not lead to unintended side effects.</li> <li>Thread Safety: The component uses a reentrant lock (<code>RLock</code>) to ensure thread-safe operations when updating the heap.</li> <li>History Tracking: By linking objects with the same identifier, the component creates a history of objects that can be accessed at any point in time.</li> </ul>"},{"location":"api_reference/components/state_management/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/state_management/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>The component relies on the <code>Stamp</code> class from <code>railtracks.utils.profiling</code> to manage timestamps for objects.</li> </ul>"},{"location":"api_reference/components/state_management/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>The component is designed to handle a large number of objects, but performance may degrade with extremely large datasets due to the recursive nature of the <code>_create_full_data_from_heap</code> method.</li> </ul>"},{"location":"api_reference/components/state_management/#43-state-management-concurrency","title":"4.3 State Management &amp; Concurrency","text":"<ul> <li>The component uses a reentrant lock to manage concurrent access, ensuring that updates to the heap are thread-safe.</li> </ul>"},{"location":"api_reference/components/state_management/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/state_management/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>forest.py</code>: Contains the implementation of the <code>Forest</code> component and related classes.</li> </ul>"},{"location":"api_reference/components/state_management/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>profiling.py</code>: Provides the <code>Stamp</code> class used for managing timestamps in the <code>Forest</code> component.</li> </ul>"},{"location":"api_reference/components/state_management/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/state_utilities/","title":"State Utilities","text":"<p>The State Utilities component provides functionality to create a subset of node and request data structures based on specified parent IDs. This is essential for managing and manipulating subsets of data within larger node and request heaps, allowing for efficient data processing and analysis.</p> <p>Version: 0.0.1</p> <p>Component Contact: @github_username</p>"},{"location":"api_reference/components/state_utilities/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/state_utilities/#1-purpose","title":"1. Purpose","text":"<p>The primary purpose of the State Utilities component is to filter and create subsets of node and request data structures. This is particularly useful in scenarios where only a specific portion of the data is needed for processing, thereby optimizing performance and resource usage.</p>"},{"location":"api_reference/components/state_utilities/#11-creating-sub-state-information","title":"1.1 Creating Sub-State Information","text":"<p>The <code>create_sub_state_info</code> function is the core utility provided by this component. It allows developers to create a subset of the original node and request heaps based on specified parent IDs. This function is crucial for tasks that require focused data manipulation without the overhead of processing the entire dataset.</p> <p>python from railtracks.state.utils import create_sub_state_info</p>"},{"location":"api_reference/components/state_utilities/#example-usage","title":"Example usage","text":"<p>node_heap = {...}  # Dictionary of LinkedNode objects request_heap = {...}  # Dictionary of RequestTemplate objects parent_ids = \"parent_id_1\"</p> <p>node_forest, request_forest = create_sub_state_info(node_heap, request_heap, parent_ids)</p>"},{"location":"api_reference/components/state_utilities/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/state_utilities/#def-create_sub_state_infonode_heap-request_heap-parent_ids","title":"<code>def create_sub_state_info(node_heap, request_heap, parent_ids)</code>","text":"<p>Creates a subset of the original heaps to include only the nodes and requests.</p> <p>The parent_ids will identify how the filtering should occur. - If a single ID is provided, it will be used as the root to find all downstream requests. - If a list of IDs is provided, it will find all requests downstream of each ID in the list. - If you provide multiple IDs on the same chain the behavior is undetermined.</p>"},{"location":"api_reference/components/state_utilities/#3-architectural-design","title":"3. Architectural Design","text":"<p>The design of the State Utilities component is centered around the efficient creation of sub-state information from larger data structures. The component leverages the <code>LinkedNode</code> and <code>RequestTemplate</code> classes to represent nodes and requests, respectively.</p>"},{"location":"api_reference/components/state_utilities/#31-design-considerations","title":"3.1 Design Considerations","text":"<ul> <li>Data Filtering: The component filters nodes and requests based on parent IDs, allowing for targeted data processing.</li> <li>Efficiency: By creating subsets of data, the component reduces the computational load and improves performance.</li> <li>Flexibility: The function can handle both single and multiple parent IDs, providing flexibility in data selection.</li> </ul>"},{"location":"api_reference/components/state_utilities/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/state_utilities/#41-implementation-details","title":"4.1 Implementation Details","text":"<ul> <li>Node and Request Classes: The component relies on the <code>LinkedNode</code> and <code>RequestTemplate</code> classes, which are defined in the node.py and request.py files, respectively.</li> <li>Data Integrity: The function ensures that there are no duplicate entries in the resulting subsets, maintaining data integrity.</li> </ul>"},{"location":"api_reference/components/state_utilities/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/state_utilities/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>utils.py</code>: Contains the implementation of the <code>create_sub_state_info</code> function.</li> </ul>"},{"location":"api_reference/components/state_utilities/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>state_management.md</code>: Provides additional context and documentation on state management within the project.</li> </ul>"},{"location":"api_reference/components/state_utilities/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>state_management.md</code>: Details the features related to state management, including the use of state utilities.</li> </ul>"},{"location":"api_reference/components/state_utilities/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/task_execution/","title":"Task Execution","text":"<p>The Task Execution component is responsible for managing task execution strategies and job coordination, supporting different execution models like asyncio and threads.</p> <p>Version: 0.0.1</p> <p>Component Contact: @github_username</p>"},{"location":"api_reference/components/task_execution/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/task_execution/#1-purpose","title":"1. Purpose","text":"<p>The Task Execution component is designed to handle the execution of tasks using various strategies, such as asyncio and threading. It coordinates the execution of tasks, manages their lifecycle, and ensures that tasks are executed according to the specified strategy.</p>"},{"location":"api_reference/components/task_execution/#11-task-submission-and-execution","title":"1.1 Task Submission and Execution","text":"<p>This use case involves submitting a task to the coordinator for execution using a specified execution strategy.</p> <p>python from railtracks.execution.coordinator import Coordinator from railtracks.execution.execution_strategy import AsyncioExecutionStrategy from railtracks.execution.task import Task from railtracks.pubsub.messages import ExecutionConfigurations</p>"},{"location":"api_reference/components/task_execution/#initialize-the-execution-strategy","title":"Initialize the execution strategy","text":"<p>execution_modes = {     ExecutionConfigurations.ASYNCIO: AsyncioExecutionStrategy(), }</p>"},{"location":"api_reference/components/task_execution/#create-a-coordinator","title":"Create a coordinator","text":"<p>coordinator = Coordinator(execution_modes=execution_modes)</p>"},{"location":"api_reference/components/task_execution/#define-a-task","title":"Define a task","text":"<p>task = Task(request_id=\"123\", node=my_node)</p>"},{"location":"api_reference/components/task_execution/#submit-the-task-for-execution","title":"Submit the task for execution","text":"<p>await coordinator.submit(task, ExecutionConfigurations.ASYNCIO)</p>"},{"location":"api_reference/components/task_execution/#12-job-management","title":"1.2 Job Management","text":"<p>This use case demonstrates how to manage jobs within the coordinator, including adding and ending jobs.</p> <p>python from railtracks.execution.coordinator import CoordinatorState, Job from railtracks.execution.task import Task</p>"},{"location":"api_reference/components/task_execution/#create-a-coordinator-state","title":"Create a coordinator state","text":"<p>state = CoordinatorState.empty()</p>"},{"location":"api_reference/components/task_execution/#define-a-task_1","title":"Define a task","text":"<p>task = Task(request_id=\"123\", node=my_node)</p>"},{"location":"api_reference/components/task_execution/#add-a-job","title":"Add a job","text":"<p>state.add_job(task)</p>"},{"location":"api_reference/components/task_execution/#end-a-job","title":"End a job","text":"<p>state.end_job(request_id=\"123\", result=\"success\")</p>"},{"location":"api_reference/components/task_execution/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/task_execution/#class-coordinatorstate","title":"<code>class CoordinatorState</code>","text":"<p>A simple object that stores the state of the coordinator in terms of the jobs it has and is currently processing.</p> <p>The API supports simple operations that will allow you to interact with the jobs.</p>"},{"location":"api_reference/components/task_execution/#__init__self-job_list","title":"<code>.__init__(self, job_list)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/task_execution/#emptycls","title":"<code>.empty(cls)</code>","text":"<p>Creates an empty CoordinatorState instance.</p> <p>One which no jobs have been completed</p>"},{"location":"api_reference/components/task_execution/#add_jobself-task","title":"<code>.add_job(self, task)</code>","text":"<p>Adds a job to the coordinator state.</p> <p>Args:     task (Task): The task to create a job from.</p>"},{"location":"api_reference/components/task_execution/#end_jobself-request_id-result","title":"<code>.end_job(self, request_id, result)</code>","text":"<p>End a job with the given request_id and result.</p>"},{"location":"api_reference/components/task_execution/#class-coordinator","title":"<code>class Coordinator</code>","text":"<p>The coordinator object is the concrete invoker of tasks that are passed into any of the configured execution strategies.</p>"},{"location":"api_reference/components/task_execution/#__init__self-execution_modes","title":"<code>.__init__(self, execution_modes)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/task_execution/#startself-publisher","title":"<code>.start(self, publisher)</code>","text":"<p>Starts the coordinator by attaching any relevant subscribers to the provided publisher.</p>"},{"location":"api_reference/components/task_execution/#handle_itemself-item","title":"<code>.handle_item(self, item)</code>","text":"<p>The basic handler to attach to the RequestCompletionPublisher.</p>"},{"location":"api_reference/components/task_execution/#submitself-task-mode","title":"<code>.submit(self, task, mode)</code>","text":"<p>Submits a task to the coordinator for execution.</p> <p>Args:     task (Task): The task to be executed.     mode (ExecutionConfigurations): The execution mode to use for the task.</p>"},{"location":"api_reference/components/task_execution/#system_detailself","title":"<code>.system_detail(self)</code>","text":"<p>Collects and returns details about the current state of Coordinator</p>"},{"location":"api_reference/components/task_execution/#shutdownself","title":"<code>.shutdown(self)</code>","text":"<p>Shuts down all active execution strategies.</p>"},{"location":"api_reference/components/task_execution/#class-taskexecutionstrategyabc","title":"<code>class TaskExecutionStrategy(ABC)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/task_execution/#shutdownself_1","title":"<code>.shutdown(self)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/task_execution/#executeself-task","title":"<code>.execute(self, task)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/task_execution/#class-asyncioexecutionstrategytaskexecutionstrategy","title":"<code>class AsyncioExecutionStrategy(TaskExecutionStrategy)</code>","text":"<p>An async-await style execution approach for tasks.</p>"},{"location":"api_reference/components/task_execution/#shutdownself_2","title":"<code>.shutdown(self)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/task_execution/#executeself-task_1","title":"<code>.execute(self, task)</code>","text":"<p>Executes the task using asyncio.</p> <p>Args:     task (Task): The task to be executed.</p>"},{"location":"api_reference/components/task_execution/#class-taskgeneric_toutput","title":"<code>class Task(Generic[_TOutput])</code>","text":"<p>A simple class used to represent a task to be completed.</p>"},{"location":"api_reference/components/task_execution/#__init__self-request_id-node","title":"<code>.__init__(self, request_id, node)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/task_execution/#invokeself","title":"<code>.invoke(self)</code>","text":"<p>The callable that this task is representing.</p>"},{"location":"api_reference/components/task_execution/#3-architectural-design","title":"3. Architectural Design","text":""},{"location":"api_reference/components/task_execution/#31-coordinator-and-job-management","title":"3.1 Coordinator and Job Management","text":"<ul> <li>Coordinator: Acts as the invoker of tasks, managing their execution through different strategies. It maintains a state of all jobs using <code>CoordinatorState</code>.</li> <li>Job: Represents a unit of work with a lifecycle, including states such as \"opened\" and \"closed\".</li> <li>CoordinatorState: Manages the list of jobs, providing methods to add and end jobs.</li> </ul>"},{"location":"api_reference/components/task_execution/#32-execution-strategies","title":"3.2 Execution Strategies","text":"<ul> <li>TaskExecutionStrategy: An abstract base class defining the interface for execution strategies.</li> <li>AsyncioExecutionStrategy: Implements an async-await style execution for tasks, leveraging Python's asyncio library.</li> <li>ThreadedExecutionStrategy: Extends <code>ConcurrentFuturesExecutor</code> to provide a threaded execution model (currently not fully implemented).</li> </ul>"},{"location":"api_reference/components/task_execution/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/task_execution/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>The component relies on the <code>railtracks.pubsub</code> module for message handling and publishing.</li> <li>Execution strategies must be provided for all configurations defined in <code>ExecutionConfigurations</code>.</li> </ul>"},{"location":"api_reference/components/task_execution/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>The <code>ConcurrentFuturesExecutor</code> and <code>ProcessExecutionStrategy</code> are not fully implemented, limiting execution to asyncio for now.</li> <li>Ensure that tasks are idempotent and can handle retries, as execution strategies may re-invoke tasks.</li> </ul>"},{"location":"api_reference/components/task_execution/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/task_execution/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>coordinator.py</code>: Manages task coordination and job lifecycle.</li> <li><code>execution_strategy.py</code>: Defines execution strategies for tasks.</li> <li><code>task.py</code>: Represents a task to be executed.</li> </ul>"},{"location":"api_reference/components/task_execution/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>context_management.md</code>: Details on context management used within tasks.</li> <li><code>exception_handling.md</code>: Describes exception handling mechanisms relevant to task execution.</li> </ul>"},{"location":"api_reference/components/task_execution/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/terminal_node_base/","title":"Terminal Node Base","text":"<p>The Terminal Node Base component implements a simple LLM node for processing input messages and returning responses, handling different input formats. It is designed to be the simplest of all LLM nodes, providing a straightforward interface for interacting with language models.</p> <p>Version: 0.0.1</p> <p>Component Contact: @github_username</p>"},{"location":"api_reference/components/terminal_node_base/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/terminal_node_base/#1-purpose","title":"1. Purpose","text":"<p>The Terminal Node Base component is primarily used to process input messages and return responses using a language model. It supports various input formats, including <code>MessageHistory</code>, <code>UserMessage</code>, and <code>str</code>, making it versatile for different use cases.</p>"},{"location":"api_reference/components/terminal_node_base/#11-process-input-messages","title":"1.1 Process Input Messages","text":"<p>The component can process input messages in different formats and return a response from the language model.</p> <p>python</p>"},{"location":"api_reference/components/terminal_node_base/#using-messagehistory","title":"Using MessageHistory","text":"<p>mh = MessageHistory([UserMessage(\"Tell me about the world around us\")]) result = await rc.call(TerminalLLM, user_input=mh)</p>"},{"location":"api_reference/components/terminal_node_base/#using-usermessage","title":"Using UserMessage","text":"<p>user_msg = UserMessage(\"Tell me about the world around us\") result = await rc.call(TerminalLLM, user_input=user_msg)</p>"},{"location":"api_reference/components/terminal_node_base/#using-string","title":"Using string","text":"<p>result = await rc.call(     TerminalLLM, user_input=\"Tell me about the world around us\" )</p>"},{"location":"api_reference/components/terminal_node_base/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/terminal_node_base/#class-terminalllmstringoutputmixin-llmbasestringresponse","title":"<code>class TerminalLLM(StringOutputMixIn, LLMBase[StringResponse])</code>","text":"<p>A simple LLM node that takes in a message and returns a response. It is the simplest of all LLMs.</p> <p>This node accepts message_history in the following formats: - MessageHistory: A list of Message objects - UserMessage: A single UserMessage object - str: A string that will be converted to a UserMessage</p> <p>Examples:     python     # Using MessageHistory     mh = MessageHistory([UserMessage(\"Tell me about the world around us\")])     result = await rc.call(TerminalLLM, user_input=mh)</p> <pre><code># Using UserMessage\nuser_msg = UserMessage(\"Tell me about the world around us\")\nresult = await rc.call(TerminalLLM, user_input=user_msg)\n\n# Using string\nresult = await rc.call(\n    TerminalLLM, user_input=\"Tell me about the world around us\"\n)\n</code></pre>"},{"location":"api_reference/components/terminal_node_base/#__init__self-user_input-llm_model","title":"<code>.__init__(self, user_input, llm_model)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/terminal_node_base/#namecls","title":"<code>.name(cls)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/terminal_node_base/#invokeself","title":"<code>.invoke(self)</code>","text":"<p>Makes a call containing the inputted message and system prompt to the llm model and returns the response</p> <p>Returns:     (TerminalLLM.Output): The response message from the llm model</p>"},{"location":"api_reference/components/terminal_node_base/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Terminal Node Base is designed to be a simple and efficient interface for interacting with language models. It extends the <code>LLMBase</code> class, which provides the foundational structure for handling message history and model interactions.</p>"},{"location":"api_reference/components/terminal_node_base/#31-design-considerations","title":"3.1 Design Considerations","text":"<ul> <li>Input Handling: The component accepts input in various formats (<code>MessageHistory</code>, <code>UserMessage</code>, <code>str</code>) and converts them into a <code>MessageHistory</code> object for processing.</li> <li>Model Interaction: It interacts with a language model to generate responses, utilizing hooks for pre-processing and post-processing messages.</li> <li>Error Handling: The component raises <code>LLMError</code> for exceptions during model invocation, ensuring robust error management.</li> </ul>"},{"location":"api_reference/components/terminal_node_base/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/terminal_node_base/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>The component relies on the <code>railtracks.llm</code> package for message handling and model interactions.</li> <li>Ensure that the language model used is compatible with the <code>ModelBase</code> interface.</li> </ul>"},{"location":"api_reference/components/terminal_node_base/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>The component is designed for simplicity and may not be suitable for complex message processing tasks.</li> <li>It is optimized for handling small to medium-sized message histories.</li> </ul>"},{"location":"api_reference/components/terminal_node_base/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/terminal_node_base/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>../concrete/terminal_llm_base.py</code>: Implements the TerminalLLM class for processing input messages and returning responses.</li> </ul>"},{"location":"api_reference/components/terminal_node_base/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>../llm_node_base.md</code>: Provides documentation for the base LLM node components.</li> </ul>"},{"location":"api_reference/components/terminal_node_base/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>../node_management.md</code>: Describes the node management features related to LLM nodes.</li> </ul>"},{"location":"api_reference/components/terminal_node_base/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul> <p>This documentation provides a comprehensive overview of the Terminal Node Base component, detailing its purpose, design, and usage. It includes links to related files and components for further exploration.</p>"},{"location":"api_reference/components/text_object_management/","title":"Text Object Management","text":"<p>The Text Object Management component is responsible for handling resource objects, specifically focusing on text resources. It includes functionalities for metadata management and content processing, making it a crucial part of the larger project.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/text_object_management/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/text_object_management/#1-purpose","title":"1. Purpose","text":"<p>The Text Object Management component is designed to manage text resources by providing functionalities for metadata handling and content processing. It is primarily used to store and manage text-specific metadata and embeddings, which are essential for various text processing tasks.</p>"},{"location":"api_reference/components/text_object_management/#11-text-resource-management","title":"1.1 Text Resource Management","text":"<p>This use case involves creating and managing text resources, including setting and retrieving metadata.</p> <p>python from railtracks.rag.text_object import TextObject</p>"},{"location":"api_reference/components/text_object_management/#create-a-textobject-instance","title":"Create a TextObject instance","text":"<p>text_obj = TextObject(raw_content=\"Sample text content\", path=\"/path/to/text/file.txt\")</p>"},{"location":"api_reference/components/text_object_management/#retrieve-metadata","title":"Retrieve metadata","text":"<p>metadata = text_obj.get_metadata() print(metadata)</p>"},{"location":"api_reference/components/text_object_management/#12-text-content-processing","title":"1.2 Text Content Processing","text":"<p>This use case demonstrates how to process text content by chunking and embedding.</p> <p>python</p>"},{"location":"api_reference/components/text_object_management/#set-chunked-content","title":"Set chunked content","text":"<p>text_obj.set_chunked([\"chunk1\", \"chunk2\", \"chunk3\"])</p>"},{"location":"api_reference/components/text_object_management/#set-embeddings","title":"Set embeddings","text":"<p>text_obj.set_embeddings([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])</p>"},{"location":"api_reference/components/text_object_management/#retrieve-updated-metadata","title":"Retrieve updated metadata","text":"<p>updated_metadata = text_obj.get_metadata() print(updated_metadata)</p>"},{"location":"api_reference/components/text_object_management/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/text_object_management/#class-textobjectresourceinstance","title":"<code>class TextObject(ResourceInstance)</code>","text":"<p>Text-specific metadata and embedding storage.</p>"},{"location":"api_reference/components/text_object_management/#__init__self-raw_content-path-kwargs","title":"<code>.__init__(self, raw_content, path, **kwargs)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/text_object_management/#set_chunkedself-chunks","title":"<code>.set_chunked(self, chunks)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/text_object_management/#set_embeddingsself-vectors","title":"<code>.set_embeddings(self, vectors)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/text_object_management/#get_metadataself","title":"<code>.get_metadata(self)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/text_object_management/#3-architectural-design","title":"3. Architectural Design","text":""},{"location":"api_reference/components/text_object_management/#31-design-considerations","title":"3.1 Design Considerations","text":"<ul> <li>ResourceInstance Class: Serves as a base class for any resource object, providing common functionalities such as path normalization, name extraction, and hash generation.</li> <li>TextObject Class: Inherits from <code>ResourceInstance</code> and adds text-specific functionalities, including raw content storage, chunked content management, and embedding storage.</li> </ul> <p>The design leverages inheritance to extend the capabilities of a generic resource instance to handle text-specific requirements. This approach promotes code reuse and modularity.</p>"},{"location":"api_reference/components/text_object_management/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/text_object_management/#41-implementation-details","title":"4.1 Implementation Details","text":"<ul> <li>Path Normalization: The <code>normalize_path</code> method ensures consistent path formatting across different operating systems.</li> <li>Hash Generation: The <code>get_resource_hash</code> method computes a SHA-256 hash of the file content, which is crucial for verifying data integrity.</li> <li>Metadata Management: The <code>get_metadata</code> method provides a comprehensive view of the text object's properties, including content-specific details like the number of chunks and embeddings.</li> </ul>"},{"location":"api_reference/components/text_object_management/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/text_object_management/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>text_object.py</code>: Contains the implementation of the <code>ResourceInstance</code> and <code>TextObject</code> classes.</li> </ul>"},{"location":"api_reference/components/text_object_management/#52-related-feature-files","title":"5.2 Related Feature Files","text":"<ul> <li><code>rag_system.md</code>: Provides an overview of the RAG (Retrieval-Augmented Generation) system, which utilizes the Text Object Management component.</li> </ul>"},{"location":"api_reference/components/text_object_management/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/tool_call_node_base/","title":"Tool Call Node Base","text":"<p>The Tool Call Node Base provides a foundational class for nodes that are capable of making tool calls, effectively managing interactions between Language Learning Models (LLMs) and tools.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/tool_call_node_base/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/tool_call_node_base/#1-purpose","title":"1. Purpose","text":"<p>The primary purpose of the Tool Call Node Base is to facilitate the creation and management of nodes that can make tool calls within a system that integrates LLMs. This component is crucial for enabling LLMs to interact with various tools, process responses, and manage the flow of information between the LLM and the tools.</p>"},{"location":"api_reference/components/tool_call_node_base/#11-node-creation-and-management","title":"1.1 Node Creation and Management","text":"<p>This use case involves creating nodes from tool names and arguments, ensuring that the nodes are correctly instantiated and connected.</p> <p>python node = tool_call_node_base.create_node(tool_name=\"example_tool\", arguments={\"arg1\": \"value1\"})</p>"},{"location":"api_reference/components/tool_call_node_base/#12-handling-tool-calls","title":"1.2 Handling Tool Calls","text":"<p>This use case demonstrates how the component handles tool calls, including managing the maximum number of tool calls and processing responses.</p> <p>python await tool_call_node_base.invoke()</p>"},{"location":"api_reference/components/tool_call_node_base/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/tool_call_node_base/#class-structuredtoolcallllmstructuredoutputmixin_tbasemodel-outputlesstoolcallllm_tbasemodel-abc-generic_tbasemodel","title":"<code>class StructuredToolCallLLM(StructuredOutputMixIn[_TBaseModel], OutputLessToolCallLLM[_TBaseModel], ABC, Generic[_TBaseModel])</code>","text":"<p>A base class for structured tool call LLMs that do not return an output. This class is used to define the structure of the tool call and handle the structured output.</p>"},{"location":"api_reference/components/tool_call_node_base/#__init__self-user_input-llm_model-max_tool_calls","title":"<code>.__init__(self, user_input, llm_model, max_tool_calls)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/tool_call_node_base/#invokeself","title":"<code>.invoke(self)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/tool_call_node_base/#class-outputlesstoolcallllmllmbase_t-abc-generic_t","title":"<code>class OutputLessToolCallLLM(LLMBase[_T], ABC, Generic[_T])</code>","text":"<p>A base class that is a node which contains  an LLm that can make tool calls. The tool calls will be returned as calls or if there is a response, the response will be returned as an output</p>"},{"location":"api_reference/components/tool_call_node_base/#__init__self-user_input-llm_model-max_tool_calls_1","title":"<code>.__init__(self, user_input, llm_model, max_tool_calls)</code>","text":"<p>Class constructor.</p>"},{"location":"api_reference/components/tool_call_node_base/#namecls","title":"<code>.name(cls)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/tool_call_node_base/#tool_nodescls","title":"<code>.tool_nodes(cls)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/tool_call_node_base/#create_nodeself-tool_name-arguments","title":"<code>.create_node(self, tool_name, arguments)</code>","text":"<p>A function which creates a new instance of a node Class from a tool name and arguments.</p> <p>This function may be overwritten to fit the needs of the given node as needed.</p>"},{"location":"api_reference/components/tool_call_node_base/#toolsself","title":"<code>.tools(self)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/tool_call_node_base/#invokeself_1","title":"<code>.invoke(self)</code>","text":"<p>No docstring found.</p>"},{"location":"api_reference/components/tool_call_node_base/#3-architectural-design","title":"3. Architectural Design","text":""},{"location":"api_reference/components/tool_call_node_base/#31-core-design-principles","title":"3.1 Core Design Principles","text":"<ul> <li>Node Management: The component is designed to manage nodes that can make tool calls, ensuring that each node is correctly instantiated and connected.</li> <li>Tool Call Handling: The component handles tool calls by managing the flow of information between the LLM and the tools, including processing responses and managing the maximum number of tool calls.</li> </ul>"},{"location":"api_reference/components/tool_call_node_base/#32-high-level-architecture","title":"3.2 High-Level Architecture","text":"<p>The component is structured around the <code>OutputLessToolCallLLM</code> and <code>StructuredToolCallLLM</code> classes, which provide the core functionality for managing tool calls and processing structured outputs.</p> <ul> <li>OutputLessToolCallLLM: This class provides the base functionality for nodes that can make tool calls without returning an output. It manages the creation of nodes, handling of tool calls, and processing of responses.</li> <li>StructuredToolCallLLM: This class extends the functionality of <code>OutputLessToolCallLLM</code> to handle structured outputs, ensuring that the responses are formatted according to a specified schema.</li> </ul>"},{"location":"api_reference/components/tool_call_node_base/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/tool_call_node_base/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>The component relies on the <code>railtracks</code> library for managing LLM interactions and tool calls.</li> <li>Ensure that the <code>railtracks</code> library is correctly installed and configured in your environment.</li> </ul>"},{"location":"api_reference/components/tool_call_node_base/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>The component is designed to handle a limited number of tool calls, as specified by the <code>max_tool_calls</code> parameter. Exceeding this limit will result in a forced final response.</li> </ul>"},{"location":"api_reference/components/tool_call_node_base/#43-debugging-observability","title":"4.3 Debugging &amp; Observability","text":"<ul> <li>The component provides detailed error messages and warnings to assist with debugging and troubleshooting.</li> <li>Use the logging functionality provided by the <code>railtracks</code> library to monitor the flow of information and identify potential issues.</li> </ul>"},{"location":"api_reference/components/tool_call_node_base/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/tool_call_node_base/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>../packages/railtracks/src/railtracks/nodes/concrete/_tool_call_base.py</code>: Contains the implementation of the <code>OutputLessToolCallLLM</code> class.</li> <li><code>../packages/railtracks/src/railtracks/nodes/concrete/structured_tool_call_llm_base.py</code>: Contains the implementation of the <code>StructuredToolCallLLM</code> class.</li> </ul>"},{"location":"api_reference/components/tool_call_node_base/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>../components/tool_management.md</code>: Documentation for the tool management component, which is closely related to the functionality provided by the Tool Call Node Base.</li> </ul>"},{"location":"api_reference/components/tool_call_node_base/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>../features/node_management.md</code>: Documentation for the node management feature, which includes the Tool Call Node Base as a key component.</li> </ul>"},{"location":"api_reference/components/tool_call_node_base/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/tool_callable_node/","title":"Tool Callable Node","text":"<p>The <code>ToolCallable</code> component defines a base class for creating tool nodes that can be used with LLM (Language Model) tool calling systems. It provides a structure for defining and preparing tools that can be integrated into larger systems, particularly those involving language models.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/tool_callable_node/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/tool_callable_node/#1-purpose","title":"1. Purpose","text":"<p>The <code>ToolCallable</code> class is primarily used to define nodes that can be integrated into LLM tool calling systems. It provides a standardized way to define tool information and prepare tools with specific parameters, facilitating their use in complex workflows involving language models.</p>"},{"location":"api_reference/components/tool_callable_node/#11-defining-tool-information","title":"1.1 Defining Tool Information","text":"<p>The <code>tool_info</code> method is crucial for providing information about the node in the form of a tool definition. This is essential for integrating the node with LLM tool calling systems.</p> <p>python class MyToolNode(ToolCallable):     @classmethod     def tool_info(cls) -&gt; Tool:         # Implementation of tool information         pass</p>"},{"location":"api_reference/components/tool_callable_node/#12-preparing-tools","title":"1.2 Preparing Tools","text":"<p>The <code>prepare_tool</code> method allows for the creation of a new instance of the node by unpacking the tool parameters. This method can be overridden for custom behavior.</p> <p>python class MyToolNode(ToolCallable):     @classmethod     def prepare_tool(cls, tool_parameters: Dict[str, Any]) -&gt; Self:         # Custom preparation logic         return super().prepare_tool(tool_parameters)</p>"},{"location":"api_reference/components/tool_callable_node/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/tool_callable_node/#3-architectural-design","title":"3. Architectural Design","text":""},{"location":"api_reference/components/tool_callable_node/#31-core-philosophy-design-principles","title":"3.1 Core Philosophy &amp; Design Principles","text":"<ul> <li>Standardization: The <code>ToolCallable</code> class provides a standardized interface for defining and preparing tools, ensuring consistency across different nodes.</li> <li>Extensibility: By allowing methods like <code>prepare_tool</code> to be overridden, the class supports extensibility and customization.</li> </ul>"},{"location":"api_reference/components/tool_callable_node/#32-high-level-architecture-data-flow","title":"3.2 High-Level Architecture &amp; Data Flow","text":"<p>The <code>ToolCallable</code> class serves as a base class, and its methods are intended to be overridden by subclasses that define specific tool nodes. The data flow involves defining tool information and preparing tools with parameters, which are then used in LLM tool calling systems.</p>"},{"location":"api_reference/components/tool_callable_node/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/tool_callable_node/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>The <code>ToolCallable</code> class relies on the <code>Tool</code> class from the <code>railtracks.llm</code> module. Ensure that this module is correctly imported and available in your environment.</li> </ul>"},{"location":"api_reference/components/tool_callable_node/#42-customization","title":"4.2 Customization","text":"<ul> <li>The <code>tool_info</code> method must be implemented in subclasses to provide specific tool information.</li> <li>The <code>prepare_tool</code> method can be overridden to customize the preparation of tool instances.</li> </ul>"},{"location":"api_reference/components/tool_callable_node/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/tool_callable_node/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>tool_callable.py</code>: Contains the implementation of the <code>ToolCallable</code> class.</li> </ul>"},{"location":"api_reference/components/tool_callable_node/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li>Tool Call Node Base Documentation: Provides additional context and details about the base class for tool nodes.</li> </ul>"},{"location":"api_reference/components/tool_callable_node/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li>Node Management Documentation: Discusses the management and integration of nodes within the system.</li> </ul>"},{"location":"api_reference/components/tool_callable_node/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/tool_management/","title":"Tool Management","text":"<p>The Tool Management component is responsible for managing tool creation and interaction, specifically converting functions and MCP tools into callable tool instances. This component plays a crucial role in the larger project by enabling dynamic tool creation and management, which is essential for flexible and scalable system operations.</p> <p>Version: 0.0.1</p> <p>Component Contact: @github_username</p>"},{"location":"api_reference/components/tool_management/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/tool_management/#1-purpose","title":"1. Purpose","text":"<p>The Tool Management component is designed to facilitate the creation and management of tools within the system. It allows developers to convert functions and MCP tools into callable tool instances, which can then be used within the system to perform various tasks. This component is essential for enabling dynamic and flexible tool usage, which is critical for the scalability and adaptability of the system.</p>"},{"location":"api_reference/components/tool_management/#11-creating-a-tool-from-a-function","title":"1.1 Creating a Tool from a Function","text":"<p>This use case involves creating a tool from a Python function. This is important for enabling the dynamic creation of tools based on existing functions, allowing for greater flexibility and reuse of code.</p> <p>python from my_project.my_component import Tool</p> <p>def my_function(param1: int, param2: str) -&gt; None:     \"\"\"Example function to be converted into a tool.\"\"\"     pass</p> <p>tool_instance = Tool.from_function(my_function)</p>"},{"location":"api_reference/components/tool_management/#12-creating-a-tool-from-an-mcp-tool","title":"1.2 Creating a Tool from an MCP Tool","text":"<p>This use case involves creating a tool from an MCP tool object. This is important for integrating MCP tools into the system, allowing for seamless interaction and management of these tools.</p> <p>python from my_project.my_component import Tool</p> <p>mcp_tool = get_mcp_tool()  # Assume this function retrieves an MCP tool tool_instance = Tool.from_mcp(mcp_tool)</p>"},{"location":"api_reference/components/tool_management/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/tool_management/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Tool Management component is designed with flexibility and scalability in mind. It provides a framework for creating and managing tools, allowing for dynamic tool creation and interaction. The component is built around the <code>Tool</code> class, which represents a single tool object with a name, description, and parameters.</p>"},{"location":"api_reference/components/tool_management/#31-tool-class","title":"3.1 Tool Class","text":"<ul> <li>Design Consideration: The <code>Tool</code> class is designed to be quasi-immutable, meaning that once a tool is created, its properties cannot be changed. This ensures consistency and reliability in tool usage.</li> <li>Design Consideration: The <code>Tool</code> class supports the creation of tools from both functions and MCP tools, providing a unified interface for tool management.</li> </ul>"},{"location":"api_reference/components/tool_management/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/tool_management/#41-parameter-handling","title":"4.1 Parameter Handling","text":"<ul> <li>The <code>Tool</code> class uses a variety of parameter handlers to manage different types of parameters, including <code>PydanticModelHandler</code>, <code>SequenceParameterHandler</code>, and <code>UnionParameterHandler</code>. These handlers ensure that parameters are correctly parsed and managed.</li> </ul>"},{"location":"api_reference/components/tool_management/#42-error-handling","title":"4.2 Error Handling","text":"<ul> <li>The <code>ToolCreationError</code> class is used to handle errors that occur during tool creation. This class provides detailed error messages and debugging tips to assist developers in resolving issues.</li> </ul>"},{"location":"api_reference/components/tool_management/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/tool_management/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>../packages/railtracks/src/railtracks/llm/tools/tool.py</code>: Contains the implementation of the <code>Tool</code> class and related functionality.</li> <li><code>../packages/railtracks/src/railtracks/llm/models/_litellm_wrapper.py</code>: Contains the <code>LiteLLMWrapper</code> class, which interacts with the <code>Tool</code> class for tool management.</li> </ul>"},{"location":"api_reference/components/tool_management/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>../components/tool_parsing.md</code>: Provides additional documentation on tool parsing and management.</li> </ul>"},{"location":"api_reference/components/tool_management/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/tool_parsing/","title":"Tool Parsing","text":"<p>The Tool Parsing component provides utilities for parsing docstrings and handling tool parameters, supporting schema conversion and parameter handling within the larger project. It is designed to facilitate the extraction and management of parameter information from Python docstrings and JSON schemas, enabling seamless integration and interaction with various tools.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/tool_parsing/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/tool_parsing/#1-purpose","title":"1. Purpose","text":"<p>The Tool Parsing component is primarily used for extracting and managing parameter information from Python docstrings and JSON schemas. This is crucial for developers who need to handle tool parameters efficiently, ensuring that parameter definitions are consistent and well-documented across the project.</p>"},{"location":"api_reference/components/tool_parsing/#11-parsing-docstring-arguments","title":"1.1 Parsing Docstring Arguments","text":"<p>The component can parse the 'Args:' section from a Python docstring to extract parameter names and their descriptions. This is important for generating documentation and ensuring that parameter information is easily accessible.</p> <p>python docstring = \"\"\" Parses the 'Args:' section from a docstring. Args:     docstring: The docstring to parse. Returns:     A dictionary mapping parameter names to their descriptions. \"\"\" parsed_args = parse_docstring_args(docstring)</p>"},{"location":"api_reference/components/tool_parsing/#12-handling-json-schemas","title":"1.2 Handling JSON Schemas","text":"<p>The component can parse JSON schemas into Parameter objects, which can then be used to create Pydantic models. This is important for developers who need to work with structured data and ensure that their tools are compatible with JSON schema standards.</p> <p>python json_schema = {     \"type\": \"object\",     \"properties\": {         \"name\": {\"type\": \"string\"},         \"age\": {\"type\": \"integer\"}     } } parameters = parse_json_schema_to_parameter(\"person\", json_schema, required=True)</p>"},{"location":"api_reference/components/tool_parsing/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/tool_parsing/#def-parse_docstring_argsdocstring","title":"<code>def parse_docstring_args(docstring)</code>","text":"<p>Parses the 'Args:' section from a docstring. Returns a dictionary mapping parameter names to their descriptions.</p> <p>Args:     docstring: The docstring to parse.</p> <p>Returns:     A dictionary mapping parameter names to their descriptions.</p>"},{"location":"api_reference/components/tool_parsing/#class-parameter","title":"<code>class Parameter</code>","text":"<p>Base class for representing a tool parameter.</p>"},{"location":"api_reference/components/tool_parsing/#__init__self-name-param_type-description-required-additional_properties-enum-default","title":"<code>.__init__(self, name, param_type, description, required, additional_properties, enum, default)</code>","text":"<p>Creates a new instance of a parameter object.</p> <p>Args:     name: The name of the parameter.     param_type: The type of the parameter.     description: A description of the parameter.     required: Whether the parameter is required. Defaults to True.     additional_properties: Whether to allow additional properties for object types. Defaults to False.     enum: The enum values for the parameter.     default: The default value for the parameter.</p>"},{"location":"api_reference/components/tool_parsing/#enumself","title":"<code>.enum(self)</code>","text":"<p>Get the enum values for the parameter, if any.</p>"},{"location":"api_reference/components/tool_parsing/#defaultself","title":"<code>.default(self)</code>","text":"<p>Get the default value for the parameter, if any.</p>"},{"location":"api_reference/components/tool_parsing/#nameself","title":"<code>.name(self)</code>","text":"<p>Get the name of the parameter.</p>"},{"location":"api_reference/components/tool_parsing/#param_typeself","title":"<code>.param_type(self)</code>","text":"<p>Get the type of the parameter.</p>"},{"location":"api_reference/components/tool_parsing/#descriptionself","title":"<code>.description(self)</code>","text":"<p>Get the description of the parameter.</p>"},{"location":"api_reference/components/tool_parsing/#requiredself","title":"<code>.required(self)</code>","text":"<p>Check if the parameter is required.</p>"},{"location":"api_reference/components/tool_parsing/#additional_propertiesself","title":"<code>.additional_properties(self)</code>","text":"<p>Check if additional properties are allowed for object types.</p>"},{"location":"api_reference/components/tool_parsing/#class-parameterhandlerabc","title":"<code>class ParameterHandler(ABC)</code>","text":"<p>Base abstract class for parameter handlers.</p>"},{"location":"api_reference/components/tool_parsing/#can_handleself-param_annotation","title":"<code>.can_handle(self, param_annotation)</code>","text":"<p>Determines if this handler can process the given parameter annotation.</p> <p>Args:     param_annotation: The parameter annotation to check.</p> <p>Returns:     True if this handler can process the annotation, False otherwise.</p>"},{"location":"api_reference/components/tool_parsing/#create_parameterself-param_name-param_annotation-description-required","title":"<code>.create_parameter(self, param_name, param_annotation, description, required)</code>","text":"<p>Creates a Parameter object for the given parameter.</p> <p>Args:     param_name: The name of the parameter.     param_annotation: The parameter's type annotation.     description: The parameter's description.     required: Whether the parameter is required.</p> <p>Returns:     A Parameter object representing the parameter.</p>"},{"location":"api_reference/components/tool_parsing/#def-parse_json_schema_to_parametername-prop_schema-required","title":"<code>def parse_json_schema_to_parameter(name, prop_schema, required)</code>","text":"<p>Given a JSON-output_schema for a property, returns a Parameter or PydanticParameter. If prop_schema defines nested properties, this is done recursively.</p> <p>Args:     name: The name of the parameter.     prop_schema: The JSON output_schema definition for the property.     required: Whether the parameter is required.</p> <p>Returns:     A Parameter or PydanticParameter object representing the output_schema.</p>"},{"location":"api_reference/components/tool_parsing/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Tool Parsing component is designed to provide a robust and flexible framework for handling tool parameters. It leverages Python's typing and Pydantic models to ensure that parameter definitions are both type-safe and easily extensible.</p>"},{"location":"api_reference/components/tool_parsing/#31-docstring-parsing","title":"3.1 Docstring Parsing","text":"<ul> <li>Function: <code>parse_docstring_args</code></li> <li>Design Consideration: Utilizes regular expressions to extract parameter information from docstrings, ensuring that the parsing logic is both efficient and accurate.</li> <li>Design Consideration: Handles both simple and complex parameter definitions, including those with type annotations.</li> </ul>"},{"location":"api_reference/components/tool_parsing/#32-json-schema-parsing","title":"3.2 JSON Schema Parsing","text":"<ul> <li>Function: <code>parse_json_schema_to_parameter</code></li> <li>Design Consideration: Supports a wide range of JSON schema features, including nested objects and arrays, to ensure compatibility with complex data structures.</li> <li>Design Consideration: Utilizes a modular approach, with separate functions for handling different schema constructs (e.g., <code>allOf</code>, <code>anyOf</code>).</li> </ul>"},{"location":"api_reference/components/tool_parsing/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/tool_parsing/#41-dependencies-setup","title":"4.1 Dependencies &amp; Setup","text":"<ul> <li>The component relies on the <code>pydantic</code> library for handling Pydantic models. Ensure that this library is included in your <code>requirements.txt</code> file.</li> </ul>"},{"location":"api_reference/components/tool_parsing/#42-performance-limitations","title":"4.2 Performance &amp; Limitations","text":"<ul> <li>The docstring parsing functions use regular expressions, which can be computationally expensive for very large docstrings. Consider optimizing docstring size where possible.</li> </ul>"},{"location":"api_reference/components/tool_parsing/#43-security-considerations","title":"4.3 Security Considerations","text":"<ul> <li>Ensure that any input data used with this component is sanitized to prevent injection attacks, especially when parsing docstrings or JSON schemas from untrusted sources.</li> </ul>"},{"location":"api_reference/components/tool_parsing/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/tool_parsing/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>docstring_parser.py</code>: Contains functions for parsing Python docstrings.</li> <li><code>parameter.py</code>: Defines the <code>Parameter</code> class and its extensions for representing tool parameters.</li> <li><code>parameter_handlers.py</code>: Contains handler classes for different parameter types.</li> <li><code>schema_parser.py</code>: Provides functions for parsing JSON schemas into Parameter objects.</li> </ul>"},{"location":"api_reference/components/tool_parsing/#52-related-feature-files","title":"5.2 Related Feature Files","text":"<ul> <li><code>tool_management.md</code>: Documents the broader tool management feature that this component is a part of.</li> </ul>"},{"location":"api_reference/components/tool_parsing/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/vector_store_base/","title":"Vector Store Base","text":"<p>The Vector Store Base defines an abstract interface for a vector store, supporting operations like adding, searching, and updating vector records. It serves as a foundational component for managing and querying vector data efficiently.</p> <p>Version: 0.0.1</p> <p>Component Contact: @github_username</p>"},{"location":"api_reference/components/vector_store_base/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/vector_store_base/#1-purpose","title":"1. Purpose","text":"<p>The Vector Store Base is designed to handle vector data operations, which are crucial for applications involving similarity search, such as recommendation systems, information retrieval, and machine learning model inference.</p>"},{"location":"api_reference/components/vector_store_base/#11-adding-vectors","title":"1.1 Adding Vectors","text":"<p>The primary use case is to add vectors or text data to the store, which can then be embedded into vectors.</p> <p>python</p>"},{"location":"api_reference/components/vector_store_base/#example-of-adding-vectors-to-the-store","title":"Example of adding vectors to the store","text":"<p>vector_store.add(     texts_or_records=[\"sample text\"],     embed=True,     metadata=[{\"category\": \"example\"}] )</p>"},{"location":"api_reference/components/vector_store_base/#12-searching-vectors","title":"1.2 Searching Vectors","text":"<p>Another key use case is searching for the most similar vectors to a given query vector or text.</p> <p>python</p>"},{"location":"api_reference/components/vector_store_base/#example-of-searching-for-similar-vectors","title":"Example of searching for similar vectors","text":"<p>results = vector_store.search(     query=\"query text\",     top_k=5,     embed=True )</p>"},{"location":"api_reference/components/vector_store_base/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/vector_store_base/#class-abstractvectorstoreabcabc","title":"<code>class AbstractVectorStore(abc.ABC)</code>","text":"<p>Abstract base class for a vector store interface. Concrete implementations must provide persistent storage and similarity search.</p>"},{"location":"api_reference/components/vector_store_base/#addself-texts_or_records","title":"<code>.add(self, texts_or_records)</code>","text":"<p>Add new vectors (or texts to be embedded) to the store.</p> <p>Args:     texts_or_records: A sequence of texts (if embed=True) or full VectorRecord objects.     embed: If True, input texts will be encoded to vectors first. If False, they are expected to be VectorRecords.     metadata: Optional sequence of metadata dicts, aligned with input texts/records (ignored if records already have metadata).</p> <p>Returns:     List of ids for the added records.</p>"},{"location":"api_reference/components/vector_store_base/#searchself-query-top_k","title":"<code>.search(self, query, top_k)</code>","text":"<p>Search for the top-k most similar vectors in the store to a query string or vector.</p> <p>Args:     query: Input text (if embed=True) or vector to search against the collection.     k: Number of top results to return.     embed: If True, the query is assumed to be a string to embed.</p> <p>Returns:     List of search results ordered by decreasing similarity (or increasing distance).</p>"},{"location":"api_reference/components/vector_store_base/#deleteself-ids","title":"<code>.delete(self, ids)</code>","text":"<p>Remove records from the vector store by their ID.</p> <p>Args:     ids: Sequence of ids to remove.</p> <p>Returns:     Number of records deleted.</p>"},{"location":"api_reference/components/vector_store_base/#updateself-id-new_text_or_vector-metadata","title":"<code>.update(self, id, new_text_or_vector, **metadata)</code>","text":"<p>Update a record, replacing its vector (or text) and/or metadata.</p> <p>Args:     id: ID of the record to update.     new_text_or_vector: New text (if embed=True) or vector.     embed: Whether to embed the new text input. Ignored if passing a vector.     **metadata: Key-value pairs to update in the record's metadata.</p> <p>Returns:     None</p>"},{"location":"api_reference/components/vector_store_base/#countself","title":"<code>.count(self)</code>","text":"<p>Return the number of records currently stored.</p>"},{"location":"api_reference/components/vector_store_base/#persistself-path","title":"<code>.persist(self, path)</code>","text":"<p>Save the store's contents to disk for later reloading.</p> <p>Args:     path: Optional filesystem path to save to. If not provided, use the store's default path.</p> <p>Returns:     None</p>"},{"location":"api_reference/components/vector_store_base/#loadcls-path","title":"<code>.load(cls, path)</code>","text":"<p>Rehydrate a vector store previously saved to disk.</p> <p>Args:     path: The filesystem path to load from.</p> <p>Returns:     An instance of the concrete VectorStore subclass.</p>"},{"location":"api_reference/components/vector_store_base/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Vector Store Base is designed with flexibility and extensibility in mind, allowing for various implementations that can handle different storage backends and similarity metrics.</p>"},{"location":"api_reference/components/vector_store_base/#31-core-design-principles","title":"3.1 Core Design Principles","text":"<ul> <li>Abstraction: The use of an abstract base class (<code>AbstractVectorStore</code>) allows for different storage mechanisms to be implemented without changing the interface.</li> <li>Metric Flexibility: Supports multiple similarity metrics such as cosine similarity, Euclidean distance, and dot product, defined in the <code>Metric</code> enum.</li> <li>Data Encapsulation: Utilizes data classes (<code>VectorRecord</code> and <code>SearchResult</code>) to encapsulate vector data and search results, ensuring a consistent data structure.</li> </ul>"},{"location":"api_reference/components/vector_store_base/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/vector_store_base/#41-implementation-details","title":"4.1 Implementation Details","text":"<ul> <li>Embedding Requirement: When adding or searching with text, embedding is required to convert text into vectors.</li> <li>Persistence: Implementations must handle data persistence and reloading through the <code>persist</code> and <code>load</code> methods.</li> </ul>"},{"location":"api_reference/components/vector_store_base/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/vector_store_base/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>base.py</code>: Contains the abstract base class and related data structures for vector store operations.</li> </ul>"},{"location":"api_reference/components/vector_store_base/#52-related-feature-files","title":"5.2 Related Feature Files","text":"<ul> <li><code>rag_system.md</code>: Describes the broader RAG (Retrieval-Augmented Generation) system that utilizes the vector store.</li> </ul>"},{"location":"api_reference/components/vector_store_base/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/vector_store_factory/","title":"Vector Store Factory","text":"<p>The Vector Store Factory provides a utility function to create a vector store based on a given configuration. It abstracts the complexity of initializing different types of vector stores, allowing users to specify their preferences through a configuration dictionary.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/vector_store_factory/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/vector_store_factory/#1-purpose","title":"1. Purpose","text":"<p>The primary purpose of the Vector Store Factory is to provide a flexible and extensible way to create vector stores. This is particularly useful in applications that require efficient storage and retrieval of high-dimensional vectors, such as machine learning models and recommendation systems.</p>"},{"location":"api_reference/components/vector_store_factory/#11-creating-an-in-memory-vector-store","title":"1.1 Creating an In-Memory Vector Store","text":"<p>The factory can be used to create an in-memory vector store, which is useful for applications that require fast access to vectors without the overhead of persistent storage.</p> <p>python from railtracks.rag.vector_store.factory import create_store</p> <p>config = {     \"backend\": \"memory\",     \"metric\": \"cosine\",     \"workspace\": \"~/.vector_store\",     \"dim\": 768 }</p> <p>vector_store = create_store(config)</p>"},{"location":"api_reference/components/vector_store_factory/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/vector_store_factory/#def-create_storecfg","title":"<code>def create_store(cfg)</code>","text":"<p>Factory utility. Example cfg:     {         \"backend\": \"memory\"         \"metric\": \"cosine\",         \"workspace\": \"~/.vector_store\",         \"dim\": 768     }</p>"},{"location":"api_reference/components/vector_store_factory/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Vector Store Factory is designed to be extensible and easy to use. It currently supports the creation of in-memory vector stores, with the potential to add more backends in the future.</p>"},{"location":"api_reference/components/vector_store_factory/#31-design-considerations","title":"3.1 Design Considerations","text":"<ul> <li>Extensibility: The factory pattern allows for easy addition of new vector store backends. Developers can extend the factory to support new types of vector stores by adding new conditions in the <code>create_store</code> function.</li> <li>Simplicity: The use of a configuration dictionary makes it easy to specify the desired properties of the vector store without needing to understand the underlying implementation details.</li> </ul>"},{"location":"api_reference/components/vector_store_factory/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/vector_store_factory/#41-configuration-details","title":"4.1 Configuration Details","text":"<ul> <li>The <code>cfg</code> parameter is a dictionary that specifies the configuration for the vector store. It must include a <code>backend</code> key, which determines the type of vector store to create.</li> <li>The <code>embedding_service</code> parameter is optional and can be used to specify a custom embedding service for the vector store.</li> </ul>"},{"location":"api_reference/components/vector_store_factory/#42-error-handling","title":"4.2 Error Handling","text":"<ul> <li>If an unknown backend is specified in the configuration, the factory will raise a <code>ValueError</code>. This ensures that only supported backends are used.</li> </ul>"},{"location":"api_reference/components/vector_store_factory/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/vector_store_factory/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>../in_memory.py</code>: Contains the implementation of the <code>InMemoryVectorStore</code> class, which is used when the <code>memory</code> backend is specified.</li> </ul>"},{"location":"api_reference/components/vector_store_factory/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li><code>../components/vector_store_base.md</code>: Provides the base interface and common functionality for all vector store implementations.</li> </ul>"},{"location":"api_reference/components/vector_store_factory/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li><code>../features/rag_system.md</code>: Describes the RAG (Retrieval-Augmented Generation) system, which utilizes vector stores for efficient data retrieval.</li> </ul>"},{"location":"api_reference/components/vector_store_factory/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/components/vector_store_utilities/","title":"Vector Store Utilities","text":"<p>The Vector Store Utilities component provides essential utility functions for vector operations, unique identifier generation, and hashing. It is a crucial part of the larger project, facilitating efficient vector computations and data integrity through hashing.</p> <p>Version: 0.0.1</p> <p>Component Contact: @developer_username</p>"},{"location":"api_reference/components/vector_store_utilities/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Purpose</li> <li>2. Public API</li> <li>3. Architectural Design</li> <li>4. Important Considerations</li> <li>5. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/components/vector_store_utilities/#1-purpose","title":"1. Purpose","text":"<p>The Vector Store Utilities component is designed to support various vector operations, including normalization and distance calculations, as well as generating unique identifiers and stable hashes. These utilities are essential for tasks such as vector similarity computations and ensuring data consistency.</p>"},{"location":"api_reference/components/vector_store_utilities/#11-vector-normalization","title":"1.1 Vector Normalization","text":"<p>Vector normalization is crucial for ensuring that vectors have a unit length, which is often required in machine learning and data processing tasks.</p> <p>python from vector_store.utils import normalize_vector</p> <p>vector = [3.0, 4.0] normalized_vector = normalize_vector(vector) print(normalized_vector)  # Output: [0.6, 0.8]</p>"},{"location":"api_reference/components/vector_store_utilities/#12-distance-calculation","title":"1.2 Distance Calculation","text":"<p>Calculating the distance between vectors is fundamental for similarity measurements. This utility supports multiple metrics, including L2, dot product, and cosine similarity.</p> <p>python from vector_store.utils import distance</p> <p>vector_a = [1.0, 2.0] vector_b = [2.0, 3.0] cosine_distance = distance(vector_a, vector_b, metric=\"cosine\") print(cosine_distance)</p>"},{"location":"api_reference/components/vector_store_utilities/#2-public-api","title":"2. Public API","text":""},{"location":"api_reference/components/vector_store_utilities/#3-architectural-design","title":"3. Architectural Design","text":"<p>The Vector Store Utilities component is designed with simplicity and efficiency in mind, providing essential operations without unnecessary complexity.</p>"},{"location":"api_reference/components/vector_store_utilities/#31-design-considerations","title":"3.1 Design Considerations","text":"<ul> <li>Normalization Function: Utilizes the Euclidean norm to scale vectors to unit length, ensuring compatibility with various machine learning algorithms.</li> <li>Distance Function: Supports multiple metrics, allowing flexibility in similarity computations. The choice of metric can significantly impact performance and accuracy.</li> <li>UUID Generation: Provides a straightforward method to generate unique identifiers, crucial for tracking and managing data entities.</li> <li>Stable Hashing: Uses SHA-256 to ensure consistent and secure hashing of text data, which is vital for data integrity and verification.</li> </ul>"},{"location":"api_reference/components/vector_store_utilities/#4-important-considerations","title":"4. Important Considerations","text":""},{"location":"api_reference/components/vector_store_utilities/#41-implementation-details","title":"4.1 Implementation Details","text":"<ul> <li>Vector Operations: Ensure that vectors are non-empty to avoid division by zero during normalization.</li> <li>Distance Metrics: The choice of metric should align with the specific requirements of the application, as it affects the interpretation of similarity.</li> <li>Hashing: The <code>stable_hash</code> function is designed for text data; ensure input is properly encoded.</li> </ul>"},{"location":"api_reference/components/vector_store_utilities/#5-related-files","title":"5. Related Files","text":""},{"location":"api_reference/components/vector_store_utilities/#51-code-files","title":"5.1 Code Files","text":"<ul> <li><code>utils.py</code>: Contains the implementation of utility functions for vector operations and hashing.</li> </ul>"},{"location":"api_reference/components/vector_store_utilities/#52-related-component-files","title":"5.2 Related Component Files","text":"<ul> <li>Vector Store Base Documentation: Provides an overview of the vector store's foundational components and their interactions.</li> </ul>"},{"location":"api_reference/components/vector_store_utilities/#53-related-feature-files","title":"5.3 Related Feature Files","text":"<ul> <li>RAG System Documentation: Describes the role of vector operations within the RAG system, highlighting their importance in retrieval-augmented generation tasks.</li> </ul>"},{"location":"api_reference/components/vector_store_utilities/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"api_reference/features/cli_interface/","title":"Cli interface","text":""},{"location":"api_reference/features/cli_interface/#cli-interface","title":"CLI Interface","text":"<p>Provides a developer-friendly command-line interface for initializing a Railtracks workspace and launching the local visualization server.</p> <p>Version: 0.0.1\u2003</p>"},{"location":"api_reference/features/cli_interface/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Functional Overview</li> <li>1.1 Environment Initialization (<code>railtracks init</code>)</li> <li>1.2 Development Server / Visualizer (<code>railtracks viz</code>)</li> <li>2. External Contracts</li> <li>2.1 CLI Commands</li> <li>2.2 HTTP Endpoints</li> <li>2.3 Configuration &amp; Flags</li> <li>3. Design and Architecture</li> <li>3.1 Control-Flow Overview</li> <li>3.2 File-Watcher Pipeline</li> <li>3.3 HTTP Serving Pipeline</li> <li>3.4 Design Decisions &amp; Trade-offs</li> <li>4. Related Files</li> <li>4.1 Related Component Files</li> <li>4.2 External Dependencies</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/features/cli_interface/#1-functional-overview","title":"1. Functional Overview","text":"<p>The CLI interface exposes two primary task sets that together streamline the developer experience around Railtracks:</p> <ul> <li>create a self-contained workspace with all required frontend assets, and  </li> <li>spin up a local web server for inspecting and interacting with runtime   data (JSON state, logs, etc.).</li> </ul> <p>Although the underlying implementation lives in <code>packages/railtracks-cli/src/railtracks_cli</code>, developers interact exclusively through a single shell command: <code>railtracks</code>.</p>"},{"location":"api_reference/features/cli_interface/#11-environment-initialization-railtracks-init","title":"1.1 Environment Initialization (<code>railtracks init</code>)","text":"<p>Prepares a project directory for Railtracks development.</p> <p>Responsibilities \u2022 Creates a hidden workspace directory <code>.railtracks/</code> \u2022 Ensures <code>.railtracks/</code> is listed in <code>.gitignore</code> \u2022 Downloads the latest standalone UI bundle and unpacks it into   <code>.railtracks/ui/</code> </p> <pre><code># one-time setup per repository\n$ railtracks init\n[railtracks] Creating .railtracks directory...\n[railtracks] Downloading latest frontend UI...\n[railtracks] \ud83d\ude80 railtracks initialization completed!\n</code></pre> <p>Developers can safely commit the rest of their repository while keeping the large UI bundle out of version control.</p>"},{"location":"api_reference/features/cli_interface/#12-development-server-visualizer-railtracks-viz","title":"1.2 Development Server / Visualizer (<code>railtracks viz</code>)","text":"<p>Launches a file-watching HTTP server backed by Python\u2019s <code>http.server</code> module.  The server:</p> <ul> <li>hosts static files from <code>.railtracks/ui/</code> on   <code>http://localhost:3030</code> (default),</li> <li>exposes a lightweight JSON API for the UI, and</li> <li>monitors <code>.railtracks/*.json</code> files, surfacing real-time changes to the   browser via an <code>/api/refresh</code> flag.</li> </ul> <pre><code># start interactive visualizer\n$ railtracks viz\n[railtracks] \ud83d\ude80 railtracks server running at http://localhost:3030\n[railtracks] \ud83d\udcc1 Serving files from: .railtracks/ui/\n[railtracks] \ud83d\udc40 Watching for changes in: .railtracks/\n</code></pre> <p>Any JSON file dropped into <code>.railtracks/</code> immediately appears in the web UI, making it ideal for debugging long-running sessions or inspecting serialized state.</p>"},{"location":"api_reference/features/cli_interface/#2-external-contracts","title":"2. External Contracts","text":""},{"location":"api_reference/features/cli_interface/#21-cli-commands","title":"2.1 CLI Commands","text":"Command Description Owned By Notes <code>railtracks init</code> Bootstrap local workspace and download UI CLI Interface Idempotent; safe to re-run <code>railtracks viz</code> Start development server (port 3030) CLI Interface Creates <code>.railtracks/</code> if missing <code>python -m railtracks_cli</code> Low-level entry point; identical to <code>railtracks</code> wrapper CLI Entry Point Provided for parity with <code>python -m</code> patterns"},{"location":"api_reference/features/cli_interface/#22-http-endpoints","title":"2.2 HTTP Endpoints","text":"Endpoint Method Description Delivered By <code>/api/files</code> GET Returns an array of available <code>.json</code> files <code>RailtracksHTTPHandler.handle_api_files</code> <code>/api/json/&lt;name&gt;</code> GET Returns raw JSON content of given file <code>handle_api_json</code> <code>/api/refresh</code> POST UI polls to trigger live reload <code>handle_refresh</code> <code>/*</code> GET Serves static UI assets, SPA fallback to <code>index.html</code> <code>serve_static_file</code> <p>All endpoints add <code>Access-Control-Allow-Origin: *</code>, allowing the UI bundle to be hosted separately during development if desired.</p>"},{"location":"api_reference/features/cli_interface/#23-configuration-flags","title":"2.3 Configuration &amp; Flags","text":"Name Default Purpose <code>PORT</code> (up-coming) <code>3030</code> Expected future enhancement for custom ports <code>RAILTRACKS_UI_URL</code> internal constant (<code>latest_ui_url</code>) Override download source for offline testing <p>The current code does not parse environment variables; contributions are welcome to surface these as proper CLI flags (<code>-p</code>, <code>--ui-url</code>, etc.).</p>"},{"location":"api_reference/features/cli_interface/#3-design-and-architecture","title":"3. Design and Architecture","text":""},{"location":"api_reference/features/cli_interface/#31-control-flow-overview","title":"3.1 Control-Flow Overview","text":"<pre><code>graph TD\n    A[Shell] --&gt;|railtracks init| B[create_railtracks_dir]\n    B --&gt; C[download_and_extract_ui]\n    A --&gt;|railtracks viz| D[start]\n    D --&gt; E[start_file_watcher]\n    D --&gt; F[start_http_server]\n    E --&gt;|File events| G[FileChangeHandler]\n    F --&gt;|HTTP requests| H[RailtracksHTTPHandler]\n</code></pre> <ul> <li><code>create_railtracks_dir()</code> \u2014 guarantees workspace &amp; <code>.gitignore</code> entry  </li> <li><code>download_and_extract_ui()</code> \u2014 pulls a zip bundle \u2011&gt; <code>.railtracks/ui/</code> </li> <li><code>start_file_watcher()</code> \u2014 <code>watchdog.Observer</code> debounces JSON changes  </li> <li><code>start_http_server()</code> \u2014 <code>HTTPServer</code> bound to <code>RailtracksHTTPHandler</code> </li> <li>File &amp; HTTP stages run concurrently (<code>threading.Thread</code> for watcher).</li> </ul>"},{"location":"api_reference/features/cli_interface/#32-file-watcher-pipeline","title":"3.2 File-Watcher Pipeline","text":"<ol> <li><code>watchdog.Observer</code> recursively monitors <code>.railtracks/</code>.</li> <li><code>FileChangeHandler.on_modified</code> filters for <code>.json</code> files.</li> <li>Debounce logic (<code>DEBOUNCE_INTERVAL = 0.5 s</code>) suppresses rapid events.</li> <li>A console status line is printed; the UI polls <code>/api/refresh</code> to reflect    changes.</li> </ol> <p>No direct IPC is required\u2014simply touching/rewriting JSON files is enough.</p>"},{"location":"api_reference/features/cli_interface/#33-http-serving-pipeline","title":"3.3 HTTP Serving Pipeline","text":"<ul> <li>Static content \u2014 path-based routing; unknown routes fall back to SPA   <code>index.html</code> enabling client-side routing.</li> <li>JSON API \u2014 thin wrappers around <code>Path.glob(\"*.json\")</code> and file I/O.</li> <li>CORS \u2014 wildcard origin for local development convenience.</li> <li>Logging \u2014 overridden <code>BaseHTTPRequestHandler.log_message</code> hooks into   <code>print_status()</code> for unified, color-ready console output.</li> </ul>"},{"location":"api_reference/features/cli_interface/#34-design-decisions-trade-offs","title":"3.4 Design Decisions &amp; Trade-offs","text":"Decision Rationale Trade-off Pure-Python <code>http.server</code> Zero external dependency; easy to embed Limited performance; no TLS Bundled UI via zip Offline-friendly; single download Requires re-download on every version bump File polling via <code>watchdog</code> Cross-platform file events Adds optional runtime dependency Workspace in <code>.railtracks/</code> Keeps repo clean, easily ignored Hidden directory may confuse newcomers No global install assumed Encourages <code>pip install -e</code> monorepo pattern Requires <code>python -m railtracks_cli</code> for alias-less use <p>Future improvements (tracked via GitHub Issues):</p> <ul> <li>expose <code>--port</code>, <code>--ui-url</code>, <code>--debug</code> flags,</li> <li>TLS &amp; auth support for secure remote demos,</li> <li>pluggable back-end to serve live protobuf/log streams.</li> </ul>"},{"location":"api_reference/features/cli_interface/#4-related-files","title":"4. Related Files","text":""},{"location":"api_reference/features/cli_interface/#41-related-component-files","title":"4.1 Related Component Files","text":"<ul> <li><code>../components/cli_entry_point.md</code>:   Tracks the minimal <code>__main__.py</code> shim that funnels <code>python -m</code> execution into   the CLI.</li> <li><code>../components/logging_configuration.md</code>:   Explains the shared helper utilities used by <code>print_status</code>, <code>print_success</code>,   etc. (colour, formatting).</li> <li><code>../components/pubsub_messaging.md</code>:   The UI relies on Pub/Sub topics documented here when embedded within broader   Railtracks deployments.</li> </ul>"},{"location":"api_reference/features/cli_interface/#42-external-dependencies","title":"4.2 External Dependencies","text":"<ul> <li><code>https://pypi.org/project/watchdog</code>:   Cross-platform filesystem event monitoring.</li> <li><code>https://pypi.org/project/ruff</code>   (dev-only) linting, referenced in <code>pyproject.toml</code>.</li> </ul>"},{"location":"api_reference/features/cli_interface/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) <code>&lt;COMMIT_HASH&gt;</code>: Initial feature documentation.</li> </ul>"},{"location":"api_reference/features/context_management/","title":"Context management","text":""},{"location":"api_reference/features/context_management/#context-management","title":"Context Management","text":"<p>Provides a unified mechanism for propagating execution-scoped information (IDs, configs, variables, loggers, publishers) across asynchronous and multi-threaded code paths without leaking state between concurrent runners.</p> <p>Version: 0.0.1\u2003</p>"},{"location":"api_reference/features/context_management/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Functional Overview</li> <li>2. External Contracts</li> <li>3. Design and Architecture</li> <li>4. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/features/context_management/#1-functional-overview","title":"1. Functional Overview","text":"<p>The feature answers the question: \u201cHow do I attach per-request data (IDs, configs, scratch-space) to the current execution and retrieve it from anywhere in the call-stack\u2014sync or async\u2014without manually passing arguments?\u201d</p> <p>It is built around a thin facade, <code>railtracks.context.central</code>, which exposes helpers for:</p> <ol> <li>Creating an execution \u201crunner\u201d and registering its context  </li> <li>Reading / mutating context variables at any depth in the code path  </li> <li>Customising global &amp; per-runner <code>ExecutorConfig</code> </li> <li>Starting / shutting down the pub-sub <code>RTPublisher</code> tied to the runner  </li> <li>Safely detecting absence of context to avoid misuse outside an active session</li> </ol> <p>The following task sets illustrate typical usage.</p>"},{"location":"api_reference/features/context_management/#11-runner-context-life-cycle","title":"1.1 Runner Context Life-Cycle","text":"<p>Create, activate, use, and finally tear down a runner context.</p> <pre><code>import railtracks.context.central as ctx\nfrom railtracks.utils.config import ExecutorConfig\n\n# --- bootstrapping (normally done by high-level rt.Session) -------------\nctx.register_globals(\n    runner_id=\"runner_42\",\n    rt_publisher=None,\n    parent_id=None,\n    executor_config=ExecutorConfig(timeout=90.0),\n    global_context_vars={\"user_id\": \"u_123\"}\n)\n\n# --- anywhere deep in the call-stack -----------------------------------\nassert ctx.is_context_present()\nprint(\"Runner:\", ctx.get_runner_id())           # \u279c runner_42\nprint(\"User  :\", ctx.get(\"user_id\"))            # \u279c u_123\n\n# Update parent_id when spawning a child node\nctx.update_parent_id(\"node_a\")\n\n# Tear-down (handled automatically by Session end)\nctx.delete_globals()\n</code></pre>"},{"location":"api_reference/features/context_management/#12-external-context-variable-crud","title":"1.2 External Context Variable CRUD","text":"<p>The external context is a mutable key-value store designed for arbitrary, user-defined data.</p> <pre><code>from railtracks.context.central import get, put, update, delete\n\nput(\"trace_id\", \"abc-xyz\")\nassert get(\"trace_id\") == \"abc-xyz\"\n\nupdate({\"stage\": \"pre-processing\", \"retry\": 0})\ndelete(\"trace_id\")\n</code></pre>"},{"location":"api_reference/features/context_management/#13-dynamic-executor-configuration","title":"1.3 Dynamic Executor Configuration","text":"<p>Fine-tune behaviour for all future runners (global) or just the current runner (local).</p> <pre><code>from railtracks.context.central import set_config, set_local_config, get_global_config\n\n# Affect future runners only\nset_config(timeout=300.0, logging_setting=\"DEBUG\")\n\n# Locally override within this runner\nfrom railtracks.utils.config import ExecutorConfig\nset_local_config(ExecutorConfig(timeout=30.0, end_on_error=True))\n\nprint(get_global_config().timeout)     # \u279c 300.0\n</code></pre>"},{"location":"api_reference/features/context_management/#2-external-contracts","title":"2. External Contracts","text":"<p>Other subsystems rely on the implicit presence of a runner context rather than formal network APIs or CLIs. The table below documents these contracts.</p>"},{"location":"api_reference/features/context_management/#21-implicit-contracts","title":"2.1 Implicit Contracts","text":"Contract Producer / Consumer Notes <code>runner_context</code> (<code>contextvars.ContextVar</code>) Producer: <code>railtracks.context.central</code>Consumer: ALL internal modules that call <code>central.safe_get_runner_context()</code> Accessing it without a registered context raises <code>ContextError</code>. <code>global_executor_config</code> (<code>ContextVar</code>) Producer: <code>central.set_config()</code> Supplies default <code>ExecutorConfig</code> to new runners. Pub-Sub <code>RTPublisher</code> Producer: <code>central.activate_publisher()</code> Must be started before emission; shutting down is mandatory to flush buffers."},{"location":"api_reference/features/context_management/#22-configuration-flags","title":"2.2 Configuration &amp; Flags","text":"Name/Call Scope Purpose <code>set_config(**kwargs)</code> global Mutate defaults for future runners; warns if called after start. <code>ExecutorConfig.prompt_injection</code> per run Enables automatic context injection into LLM prompts. <code>ExecutorConfig.save_state</code> per run Persist execution state to disk for replay/debug."},{"location":"api_reference/features/context_management/#3-design-and-architecture","title":"3. Design and Architecture","text":"<p>The feature is realised by the Context Management component (docs). Below is a feature-level narrative; component-level details live in that document.</p>"},{"location":"api_reference/features/context_management/#31-core-abstractions","title":"3.1 Core Abstractions","text":"<pre><code>classDiagram\n    class RunnerContextVars {\n        +str runner_id\n        +InternalContext internal_context\n        +ExternalContext external_context\n        +prepare_new(new_parent_id)\n    }\n\n    class InternalContext {\n        +RTPublisher publisher\n        +str parent_id\n        +str runner_id\n        +ExecutorConfig executor_config\n        +is_active()\n        +prepare_new()\n    }\n\n    class ExternalContext &lt;&gt;\n    class MutableExternalContext\n\n    RunnerContextVars --&gt; InternalContext\n    RunnerContextVars --&gt; ExternalContext\n    ExternalContext &lt;|.. MutableExternalContext\n\n<p>\u2022 RunnerContextVars \u2013 Immutable container installed into a <code>ContextVar</code>.\n\u2022 InternalContext \u2013 RT-specific, readonly to most callers; tracks publisher, parent ID, config.\n\u2022 ExternalContext \u2013 User-facing scratch-space with CRUD operations.  </p>"},{"location":"api_reference/features/context_management/#32-data-control-flow","title":"3.2 Data &amp; Control Flow","text":"<ol>\n<li><code>register_globals()</code> constructs RunnerContextVars and stores it in <code>runner_context</code>.  </li>\n<li>Any function can call <code>safe_get_runner_context()</code>; the lookup walks only the current async context guaranteeing isolation between concurrent tasks.  </li>\n<li>The attached <code>RTPublisher</code> is lazily started by <code>activate_publisher()</code>\u2014decoupling publication from bootstrap time.  </li>\n<li>When a node spawns children, it calls <code>update_parent_id()</code>; this returns a new RunnerContextVars instance ensuring structural sharing but new parent linkage.  </li>\n</ol>"},{"location":"api_reference/features/context_management/#33-trade-offs-rationale","title":"3.3 Trade-offs &amp; Rationale","text":"Decision\nReasoning\nTrade-off / Mitigation\n\n\n\n\n<code>contextvars</code> over thread-locals\nWorks across <code>asyncio</code>, <code>trio</code>, and threads.\nSlight lookup overhead; negligible compared to I/O-bound workload.\n\n\nImmutable <code>RunnerContextVars</code> replacement\nEasier reasoning; prevents accidental side-effects between libraries.\nExtra allocations; insignificant (dozens per execution tree).\n\n\nSeparate Internal vs External Context\nKeeps RT-private machinery from polluting user dictionary and vice-versa.\nDouble indirection when both are needed; encapsulated by helper funcs.\n\n\nGlobal <code>ExecutorConfig</code> ContextVar\nAllows default-overrides without mutating process-wide globals (test isolation).\nMust warn if a runner is already active \u2013 implemented via <code>warnings</code>.\n\n\nNo automatic publisher start on register\nForces explicit intention; avoids IO in code paths that only need context vars.\nRequires documentation (<code>activate_publisher()</code>); safeguarded by checks."},{"location":"api_reference/features/context_management/#34-rejected-alternatives","title":"3.4 Rejected Alternatives","text":"<ul>\n<li>Passing context down the call-stack manually \u2013 tedious, brittle, breaks 3rd-party API boundaries.  </li>\n<li>Single global module-level dict \u2013 unsafe in concurrent runners; leaks state across tests.  </li>\n<li><code>threading.local()</code> \u2013 fails for <code>asyncio</code> coroutines that share threads.  </li>\n</ul>"},{"location":"api_reference/features/context_management/#4-related-files","title":"4. Related Files","text":""},{"location":"api_reference/features/context_management/#41-related-component-files","title":"4.1 Related Component Files","text":"<ul>\n<li><code>../components/context_management.md</code>: Low-level API and implementation rationale.</li>\n<li><code>../components/executor_configuration.md</code>: In-depth description of <code>ExecutorConfig</code>.</li>\n</ul>"},{"location":"api_reference/features/context_management/#42-related-feature-files","title":"4.2 Related Feature Files","text":"<ul>\n<li><code>../features/logging_profiling.md</code>: Context\u2019s <code>ExecutorConfig.logging_setting</code> interacts with logging feature.</li>\n<li><code>../features/task_execution.md</code>: Task executor consumes runner context to honour timeouts and cancellation.</li>\n</ul>"},{"location":"api_reference/features/context_management/#43-external-dependencies","title":"4.3 External Dependencies","text":"<ul>\n<li><code>https://docs.python.org/3/library/contextvars.html</code> \u2013 Python std-lib mechanism underpinning isolation.</li>\n</ul>"},{"location":"api_reference/features/context_management/#changelog","title":"CHANGELOG","text":"<ul>\n<li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial creation of feature documentation.</li>\n</ul>"},{"location":"api_reference/features/exception_handling/","title":"Exception handling","text":""},{"location":"api_reference/features/exception_handling/#exception-handling","title":"Exception Handling","text":"<p>Provides a unified error-handling surface that standardizes how failures are represented, enriched, and surfaced across the entire Railtracks runtime\u2014\u200bfrom low-level model interaction to high-level node orchestration.</p> <p>Version: 0.0.1\u2003</p>"},{"location":"api_reference/features/exception_handling/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Functional Overview</li> <li>2. External Contracts</li> <li>3. Design and Architecture</li> <li>4. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/features/exception_handling/#1-functional-overview","title":"1. Functional Overview","text":"<p>At its core, the Exception Handling feature offers:</p> <p>\u2022 A type-safe hierarchy of exceptions (all inheriting from <code>RTError</code>) that encode where the failure happened and what the caller can do about it. \u2022 Rich, colorised terminal output that highlights root causes and, when available, actionable debugging notes. \u2022 Centralised, YAML-backed message/notes storage to keep human-readable text decoupled from the code. \u2022 Specialised helpers for Large Language Model (LLM) errors that capture historical context (<code>MessageHistory</code>) for post-mortem analysis.</p>"},{"location":"api_reference/features/exception_handling/#11-unified-error-hierarchy","title":"1.1 Unified Error Hierarchy","text":"<p>The top-level <code>RTError</code> provides ANSI-colour helpers and establishes the \u201ccontract\u201d that every Railtracks exception abides by. All concrete subclasses (e.g. <code>NodeInvocationError</code>, <code>LLMError</code>) must inherit from it, ensuring that downstream code (CLI, web server, notebook widgets) can rely on a single catch-point.</p> <pre><code>import railtracks as rt\nfrom railtracks.exceptions import NodeInvocationError\n\ntry:\n    result = rt.call(graph)          # may raise\nexcept NodeInvocationError as exc:\n    rt.utils.logging.action.error(exc)   # uniform str(exc) is colourised &amp; note-aware\n</code></pre>"},{"location":"api_reference/features/exception_handling/#12-context-aware-debug-notes","title":"1.2 Context-Aware Debug Notes","text":"<p>Most exception constructors accept an optional <code>notes: list[str]</code>. These notes are appended (green) below the primary error (red) at render-time.</p> <pre><code>from railtracks.exceptions import NodeCreationError\n\nraise NodeCreationError(\n    \"Duplicate parameter names detected.\",\n    notes=[\n        \"Check that every parameter in `tool_params` is unique.\",\n        \"Hint: autogenerate param keys with `rt.llm.Tool.Parameter.auto_name()`.\"\n    ]\n)\n</code></pre> <p>Output (simplified):</p> <pre><code>\u274c Duplicate parameter names detected.\n   Tips to debug:\n   - Check that every parameter \u2026\n   - Hint: autogenerate \u2026\n</code></pre>"},{"location":"api_reference/features/exception_handling/#13-llm-failure-introspection","title":"1.3 LLM Failure Introspection","text":"<p><code>LLMError</code> records the reason plus an optional <code>MessageHistory</code>. Tools like the Profiler or the Debug UI can pretty-print that history for root-cause analysis.</p> <pre><code>from railtracks.exceptions import LLMError\nfrom railtracks.llm import MessageHistory\n\nhistory = MessageHistory.system(\"You are helpful\").user(\"Hi\")\nraise LLMError(\"Model quota exhausted\", history)\n</code></pre>"},{"location":"api_reference/features/exception_handling/#14-yaml-driven-message-catalogue","title":"1.4 YAML-Driven Message Catalogue","text":"<p>To avoid scattering magic strings, reusable messages live in <code>railtracks/exceptions/messages/exception_messages.yaml</code>. They can be fetched via <code>get_message()</code> / <code>get_notes()</code>:</p> <pre><code>from railtracks.exceptions.messages import (\n    ExceptionMessageKey as Key,\n    get_message, get_notes\n)\n\nmsg  = get_message(Key.OUTPUT_MODEL_REQUIRED_MSG)\nnotes = get_notes(Key.OUTPUT_MODEL_REQUIRED_NOTES)\n\nraise NodeCreationError(msg, notes)\n</code></pre>"},{"location":"api_reference/features/exception_handling/#2-external-contracts","title":"2. External Contracts","text":"<p>While most of the feature is internal, two artefacts are considered contracts for other systems or teams:</p>"},{"location":"api_reference/features/exception_handling/#21-message-key-catalogue","title":"2.1 Message Key Catalogue","text":"File Stability Purpose <code>railtracks/exceptions/messages/exception_messages.yaml</code> Stable Upstream UIs &amp; docs index keys (e.g. <code>OUTPUT_MODEL_REQUIRED_MSG</code>) to show translated copies. Do not rename keys without a deprecation cycle."},{"location":"api_reference/features/exception_handling/#22-cli-exit-codes","title":"2.2 CLI / Exit Codes","text":"<p>The Railtracks CLI converts any uncaught <code>RTError</code> into process exit code <code>2</code>. No other exception type should intentionally propagate beyond the boundary.</p> Command Exit Code Trigger <code>python -m railtracks_cli \u2026</code> <code>2</code> Any unhandled subclass of <code>RTError</code> <p>No environment variables or feature flags currently influence behaviour.</p>"},{"location":"api_reference/features/exception_handling/#3-design-and-architecture","title":"3. Design and Architecture","text":""},{"location":"api_reference/features/exception_handling/#31-architectural-patterns","title":"3.1 Architectural Patterns","text":"<p>\u2022 Single-root Exception Tree \u2013 simplifies <code>except RTError as e</code> catch-alls and keeps non-Railtracks errors distinguishable. \u2022 Colourised, Human-First Rendering \u2013 leverages ANSI codes defined in <code>RTError</code> to draw attention to actionable info. \u2022 Externalised Message Content \u2013 messages &amp; notes stored in YAML to enable localisation and runtime overrides without touching code. \u2022 Enrichment over Replacement \u2013 concrete classes wrap the base class string, never discard context, allowing nested try/except to add details.</p>"},{"location":"api_reference/features/exception_handling/#32-data-flow","title":"3.2 Data Flow","text":"<pre><code>flowchart TD\n    subgraph Runtime\n        A[Graph / Node\\nexecution] --&gt;|failure| B(NodeInvocationError)\n        A2[Node creation / validation] --&gt;|failure| C(NodeCreationError)\n        A3[LLM call] --&gt;|failure| D(LLMError)\n        A4[Coordinator timeout] --&gt;|timeout| E(GlobalTimeOutError)\n    end\n\n    B &amp; C &amp; D &amp; E --&gt; F(Logger / CLI)\n    subgraph Presentation\n        F --&gt;|str(exc)| G[[ANSI-colour\\nstring]]\n    end\n</code></pre>"},{"location":"api_reference/features/exception_handling/#33-key-trade-offs","title":"3.3 Key Trade-offs","text":"Decision Rationale Drawback ANSI colours in <code>__str__</code> Quick visual parsing in interactive shells. Non-TTY sinks (JSON logs, HTTP APIs) must strip escape codes. YAML message store Simplifies edits &amp; translations. Runtime overhead of loading YAML once; minimal. Hierarchy depth \u2248 1 Keeps surface simple (<code>RTError</code> children). Fewer semantic layers; rely on naming to convey nuance."},{"location":"api_reference/features/exception_handling/#34-alternatives-considered","title":"3.4 Alternatives Considered","text":"<ol> <li>Standard Python logging only \u2013 rejected: lacks structured, catchable types; weaker UX.  </li> <li>Error codes enumerated in code \u2013 rejected: harder to update, less translator-friendly than YAML.  </li> </ol>"},{"location":"api_reference/features/exception_handling/#4-related-files","title":"4. Related Files","text":""},{"location":"api_reference/features/exception_handling/#41-related-component-files","title":"4.1 Related Component Files","text":"<ul> <li><code>../components/exception_handling.md</code>: Low-level API &amp; implementation details for all exception classes.</li> <li><code>../components/model_error_handling.md</code>: LLM-specific extensions (<code>ModelError</code>, <code>FunctionCallingNotSupportedError</code>) that build upon this feature.</li> </ul>"},{"location":"api_reference/features/exception_handling/#42-related-feature-files","title":"4.2 Related Feature Files","text":"<ul> <li><code>../logging_profiling.md</code>: Shows how exceptions are captured, logged, and profiled across the system.</li> </ul>"},{"location":"api_reference/features/exception_handling/#43-external-dependencies","title":"4.3 External Dependencies","text":"<ul> <li><code>https://github.com/yaml/pyyaml</code>: Used to parse the YAML message catalogue.</li> </ul>"},{"location":"api_reference/features/exception_handling/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial extraction from component-level docs into formal feature documentation.</li> </ul>"},{"location":"api_reference/features/llm_integration/","title":"Llm integration","text":""},{"location":"api_reference/features/llm_integration/#llm-integration","title":"LLM Integration","text":"<p>End-to-end framework that embeds Large-Language-Model (LLM) capabilities into the Railtracks runtime\u2014covering messaging, model invocation, provider abstraction, prompt handling, error handling, and a minimal chat UI.</p> <p>Version: 0.0.1\u2003</p>"},{"location":"api_reference/features/llm_integration/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Functional Overview</li> <li>2. External Contracts</li> <li>3. Design and Architecture</li> <li>4. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/features/llm_integration/#1-functional-overview","title":"1. Functional Overview","text":"<p>The feature exposes a cohesive set of task-sets that, together, allow developers to:</p> <ol> <li>Build, inspect, and manipulate conversation histories.</li> <li>Invoke local or cloud LLMs\u2014synchronously, asynchronously, or in streaming mode.</li> <li>Extend support for new providers with minimal boiler-plate.</li> <li>Inject contextual data into prompts at runtime.</li> <li>Surface model errors in a structured way.</li> <li>Embed a reference browser-based chat UI for rapid prototyping and demos.</li> </ol> <p>The following subsections outline each task-set from a user\u2019s perspective.</p>"},{"location":"api_reference/features/llm_integration/#11-messaging-history-management","title":"1.1 Messaging &amp; History Management","text":"<p>Create strongly-typed messages, append them to a history, and pre-process them before sending to a model.</p> <pre><code>from railtracks.llm.message import UserMessage, AssistantMessage, SystemMessage\nfrom railtracks.llm.history import MessageHistory\nfrom railtracks.utils.prompt_injection import inject_values, ValueDict\n\nhistory = MessageHistory([\n    SystemMessage(\"You are a helpful assistant.\"),\n    UserMessage(\"What\u2019s the weather in Paris?\"),\n])\n\n# Dynamically inject runtime values into the prompt placeholders\ninject_values(history, ValueDict(city=\"Paris\"))\n\nprint(history.removed_system_messages())\n</code></pre> <p>For message semantics, roles, and helper methods see <code>components/llm_messaging.md</code>.</p>"},{"location":"api_reference/features/llm_integration/#12-model-invocation-chat-structured-streaming","title":"1.2 Model Invocation (Chat / Structured / Streaming)","text":"<p>Interact with any <code>ModelBase</code> implementation (local or cloud) using a uniform API.</p> <pre><code>from railtracks.llm.model import ModelBase\nfrom railtracks.llm.history import MessageHistory\nfrom railtracks.llm.models.api_providers.openai import OpenAILLM\n\n# \u2776 Pick a provider wrapper\nllm: ModelBase = OpenAILLM(model_name=\"gpt-3.5-turbo\")\n\n# \u2777 (Optional) attach hooks\nllm.add_pre_hook(lambda messages, **kw: print(\"\u2192\", messages))\nllm.add_post_hook(lambda resp, **kw: print(\"\u2190\", resp.message().content))\n\n# \u2778 Call any of the high-level helpers\nhistory = MessageHistory().append(UserMessage(\"Say hello!\"))\nresp   = llm.chat(messages=history)             # regular chat\nasync_resp = await llm.achat(messages=history)  # async\nstream = llm.stream_chat(messages=history)      # generator of chunks\n</code></pre> <p>Structured outputs via Pydantic schemas are handled through <code>structured()</code> / <code>astructured()</code>. Detailed API surface documented in <code>components/model_interaction.md</code>.</p>"},{"location":"api_reference/features/llm_integration/#13-provider-abstraction-cloud-local","title":"1.3 Provider Abstraction (Cloud &amp; Local)","text":"<p>Switch between different providers without changing call-sites.</p> <pre><code>from railtracks.llm.models.cloud.azureai import AzureAILLM\nfrom railtracks.llm.models.local.ollama import OllamaLLM\n\ncloud_llm  = AzureAILLM(\"gpt-35-turbo\")\nlocal_llm  = OllamaLLM(\"mistral\", domain=\"auto\")\n\nfor model in (cloud_llm, local_llm):\n    print(model.model_type(), \"\u2192\", model.chat(messages=history).message().content)\n</code></pre> <p>New providers only need to subclass <code>ProviderLLMWrapper</code>; see <code>components/api_provider_wrappers.md</code>.</p>"},{"location":"api_reference/features/llm_integration/#14-prompt-formatting-context-injection","title":"1.4 Prompt Formatting &amp; Context Injection","text":"<p>Fill placeholders in prompt templates or entire histories using runtime context.</p> <pre><code>from railtracks.utils.prompt_injection import fill_prompt\n\ntemplate = \"Hello {user_name}, today is {date}.\"\nruntime_values = ValueDict(user_name=\"Alice\", date=\"2024-05-12\")\nprint(fill_prompt(template, runtime_values))\n# \u2192 \"Hello Alice, today is 2024-05-12.\"\n</code></pre> <p>More examples &amp; gotchas in <code>components/prompt_formatting.md</code>.</p>"},{"location":"api_reference/features/llm_integration/#15-error-handling","title":"1.5 Error Handling","text":"<p>Catch and reason about model-specific failures via typed exceptions.</p> <pre><code>from railtracks.llm.models._model_exception_base import FunctionCallingNotSupportedError\n\ntry:\n    resp = cloud_llm.chat_with_tools(history, tools=[...])\nexcept FunctionCallingNotSupportedError as e:\n    logger.warning(\"Fallback to manual handling: %s\", e)\n</code></pre> <p>Error taxonomy is defined in <code>components/model_error_handling.md</code>.</p>"},{"location":"api_reference/features/llm_integration/#16-reference-chat-ui","title":"1.6 Reference Chat UI","text":"<p>Spin up a lightweight FastAPI server to interact with any <code>ModelBase</code> instance.</p> <pre><code>python -m railtracks.utils.visuals.browser.chat_ui\n# Navigate to http://localhost:8000 to start chatting\n</code></pre> <p>Integration guidance &amp; available REST endpoints in <code>components/chat_ui.md</code>.</p>"},{"location":"api_reference/features/llm_integration/#2-external-contracts","title":"2. External Contracts","text":""},{"location":"api_reference/features/llm_integration/#21-environment-variables","title":"2.1 Environment Variables","text":"Name Description Default <code>OPENAI_API_KEY</code> API key for OpenAI provider wrappers \u2014 <code>AZURE_OPENAI_ENDPOINT</code> &amp; <code>AZURE_OPENAI_KEY</code> Required for <code>AzureAILLM</code> \u2014 <code>OLLAMA_HOST</code> When using <code>OllamaLLM(domain=\"auto\")</code>, host of Ollama srv <code>localhost</code> <code>RAILTRACKS_PROMPT_INJECTION</code> Toggle context-prompt injection globally (<code>true</code>/<code>false</code>) <code>true</code> <p>Component-level docs specify additional env-vars where relevant.</p>"},{"location":"api_reference/features/llm_integration/#22-cli-entry-points","title":"2.2 CLI Entry Points","text":"Command Description Owned By <code>python -m railtracks.utils.visuals.browser.chat_ui</code> Launches reference chat UI Feature"},{"location":"api_reference/features/llm_integration/#23-events-pubsub","title":"2.3 Events &amp; Pub/Sub","text":"<p>This feature does not publish/subscribe to system-wide events directly; instead individual components (e.g. <code>pubsub_messaging</code>) handle that.</p>"},{"location":"api_reference/features/llm_integration/#3-design-and-architecture","title":"3. Design and Architecture","text":"<p>LLM Integration stitches together multiple slow-changing component contracts. The high-level design is shown in the following Mermaid diagram.</p> <pre><code>flowchart LR\n    subgraph \"User / Developer\"\n        A[MessageHistory] --&gt;|chat()/structured()| B(ModelBase)\n    end\n\n    %% Core\n    B --&gt; C[Provider Wrapper(cloud/local)]\n    C --&gt; D[LiteLLMWrapper] --&gt;|invokes| E[litellm\\n3rd-party]\n    B --&gt; F[Hook Pipeline]\n    B -.-&gt;|raises| G[ModelError Hierarchy]\n\n    %% Prompt handling\n    H[Prompt Formatter] --&gt; I[inject_values()]\n    I --&gt; A\n\n    %% UI\n    J[Chat UI (FastAPI)] --&gt; A\n    J --&gt; B\n\n    style B fill:#FFF5D1,stroke:#FFB700\n</code></pre> <p>Key points:</p> <ol> <li> <p>Abstraction Layers    \u2022 <code>Message</code>/<code>MessageHistory</code> \u2192 conversation semantics    \u2022 <code>ModelBase</code> \u2192 capabilities contract (sync, async, streaming, structured)    \u2022 <code>ProviderLLMWrapper</code> \u2192 provider-specific translation    \u2022 <code>LiteLLMWrapper</code> \u2192 leverage <code>litellm</code> for transport &amp; token counting.  </p> </li> <li> <p>Hook Pipeline    Pre-, post-, and exception-hooks provide cross-cutting concerns (logging, retry, redaction) without subclassing.</p> </li> <li> <p>Prompt Life-Cycle    At runtime, context variables are injected before the hook pipeline, guaranteeing downstream model calls see concrete strings.</p> </li> <li> <p>Error Boundaries    All provider/network errors are mapped to the <code>ModelError</code> hierarchy. The caller only imports component-specific exceptions when they need fine-grained handling.</p> </li> <li> <p>Extensibility    Adding a new provider = one subclass + optional overrides:    <pre><code>class GroqLLM(ProviderLLMWrapper):\n    @classmethod\n    def model_type(cls) -&gt; str: return \"Groq\"\n</code></pre>    No other code changes needed.</p> </li> <li> <p>UI Separation    The Chat UI remains an optional, stateless visualization layer. It communicates over HTTP POST + Server-Sent Events, keeping the core package headless.</p> </li> </ol>"},{"location":"api_reference/features/llm_integration/#31-performance-scalability-choices","title":"3.1 Performance &amp; Scalability Choices","text":"<ul> <li>Hook execution is synchronous by design to keep ordering deterministic; heavy work should happen in async hooks or external services.</li> <li>Large message histories are trimmed by upstream business logic (not by this feature) to avoid provider-side token limits.</li> <li>Streaming API surfaces a Python generator (<code>Response.streamer</code>) to allow back-pressure in caller code.</li> </ul>"},{"location":"api_reference/features/llm_integration/#32-rejected-alternatives","title":"3.2 Rejected Alternatives","text":"<ul> <li>Direct SDKs per provider \u2014 increased maintenance overhead; <code>litellm</code> already abstracts 10+ providers and is battle-tested.  </li> <li>Prompt injection via Jinja \u2014 brings template-rendering edge cases; key-only <code>string.Formatter</code> proved simpler and safer.</li> </ul>"},{"location":"api_reference/features/llm_integration/#4-related-files","title":"4. Related Files","text":""},{"location":"api_reference/features/llm_integration/#41-related-component-files","title":"4.1 Related Component Files","text":"<ul> <li><code>components/llm_messaging.md</code> \u2013 message &amp; history primitives.</li> <li><code>components/model_interaction.md</code> \u2013 <code>ModelBase</code> and hook pipeline.</li> <li><code>components/api_provider_wrappers.md</code> \u2013 cloud provider abstraction.</li> <li><code>components/cloud_model_wrappers.md</code> \u2013 Azure OpenAI wrapper specifics.</li> <li><code>components/local_model_wrappers.md</code> \u2013 Ollama/local wrapper specifics.</li> <li><code>components/prompt_injection.md</code> \u2013 context injection logic.</li> <li><code>components/prompt_formatting.md</code> \u2013 formatting helpers.</li> <li><code>components/model_error_handling.md</code> \u2013 custom exception hierarchy.</li> <li><code>components/chat_ui.md</code> \u2013 browser UI integration.</li> </ul>"},{"location":"api_reference/features/llm_integration/#42-related-feature-files","title":"4.2 Related Feature Files","text":"<p>No additional feature files are currently dependent on LLM Integration; other high-level features should link back here.</p>"},{"location":"api_reference/features/llm_integration/#43-external-dependencies","title":"4.3 External Dependencies","text":"<ul> <li><code>https://github.com/BerriAI/litellm</code> \u2013 provider-agnostic LLM client (MIT).</li> <li><code>https://fastapi.tiangolo.com</code> \u2013 powering the reference chat server.</li> </ul>"},{"location":"api_reference/features/llm_integration/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (2024-05-12) [<code>&lt;INITIAL&gt;</code>]: Initial public documentation.</li> </ul>"},{"location":"api_reference/features/logging_profiling/","title":"Logging profiling","text":""},{"location":"api_reference/features/logging_profiling/#logging-and-profiling","title":"Logging and Profiling","text":"<p>A unified observability layer that offers fine-grained logging and lightweight runtime profiling to help developers debug, audit, and optimise Railtracks applications.</p> <p>Version: 0.0.1\u2003</p>"},{"location":"api_reference/features/logging_profiling/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Functional Overview   \u2013 1.1 Task Set A: Runtime Logging   \u2013 1.2 Task Set B: Performance Profiling</li> <li>2. External Contracts</li> <li>3. Design and Architecture</li> <li>4. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/features/logging_profiling/#1-functional-overview","title":"1. Functional Overview","text":"<p>Logging and Profiling is intentionally split into two task sets:</p> Task Set Key Audience Typical Questions Answered Runtime Logging Developers &amp; Operators \u201cWhat happened?\u201d / \u201cWhy did this fail?\u201d Performance Profiling Performance Eng. &amp; Researchers \u201cHow long did each step take?\u201d <p>Below we show the most common entry points and usage patterns.  For complete parameter references, see the component documentation linked in each section.</p>"},{"location":"api_reference/features/logging_profiling/#11-task-set-a-runtime-logging","title":"1.1 Task-Set A \u2013 Runtime Logging","text":"<p>Railtracks exposes a single configuration surface\u2014<code>ExecutorConfig</code>\u2014and a namespaced logger hierarchy rooted at <code>RT</code>.  Together they provide out-of-box terminal logging, optional file logging, coloured output, and pluggability for custom handlers.</p> <pre><code>import railtracks as rt\nfrom railtracks.utils.logging.create import get_rt_logger\n\n################################################################################\n# 1\ufe0f\u20e3  Configure logging *once* at application start-up.                        #\n################################################################################\ncfg = rt.ExecutorConfig(\n    logging_setting=\"VERBOSE\",    # VERBOSE / REGULAR / QUIET / NONE\n    log_file=\"my_run.log\",        # optional file sink\n)\nrt.set_config(cfg)                # Apply globally (or use `with rt.Session` \u2026)\n\n################################################################################\n# 2\ufe0f\u20e3  Obtain loggers and write messages.                                       #\n################################################################################\nroot_logger = get_rt_logger()              # \"RT\"\nworker_logger = get_rt_logger(\"worker-42\") # \"RT.worker-42\"\n\nroot_logger.info(\"initialising\")\nworker_logger.debug(\"internal state=%s\", some_dict)\n\n################################################################################\n# 3\ufe0f\u20e3  Leverage structured action logs for graph execution.                     #\n################################################################################\nfrom railtracks.utils.logging.action import (\n    RequestCreationAction,\n    RequestSuccessAction,\n    RequestFailureAction,\n)\n\nroot_logger.info(RequestCreationAction(\n    parent_node_name=\"Coordinator\",\n    child_node_name=\"Chunker\",\n    input_args=(\"doc.txt\",),\n    input_kwargs={}\n).to_logging_msg())\n</code></pre> <p>Key capabilities \u2022  Four opinionated log levels (VERBOSE, REGULAR, QUIET, NONE) \u2022  Colourised console output via <code>ColorfulFormatter</code> for fast visual scanning \u2022  Optional timestamped file output (<code>log_file=\"*.log\"</code>) \u2022  Namespaced loggers (<code>RT.&lt;subsystem&gt;.&lt;module&gt;\u2026</code>) keep 3rd-party code isolated \u2022  Structured messages for node life-cycle events (see   <code>../components/logging_actions.md</code>)</p> <p>Further reading: <code>docs/observability/logging.md</code> for detailed workflow examples and guidance on forwarding logs to external providers (Sentry, Loggly, etc.).</p>"},{"location":"api_reference/features/logging_profiling/#12-task-set-b-performance-profiling","title":"1.2 Task-Set B \u2013 Performance Profiling","text":"<p>The Profiling sub-system delivers a zero-dependency mechanism to annotate and retrieve execution \u201cstamps\u201d.  Each <code>Stamp</code> captures:</p> <pre><code>time  \u2013 float   (epoch seconds)\nstep  \u2013 int     (logical order)\nid    \u2013 str     (human message, e.g., \"chunking complete\")\n</code></pre> <pre><code>from railtracks.utils.profiling import StampManager\n\n###############################################################################\n# 1\ufe0f\u20e3  Create a manager and record stamps.                                     #\n###############################################################################\nsm = StampManager()\n\nsm.create_stamp(\"Session start\")\ntokeniser_stamp = sm.create_stamp(\"Doc tokenised\")   # increments step each call\n\n###############################################################################\n# 2\ufe0f\u20e3  Grouped stamps: keep the same \u2018step\u2019 for related sub-events.            #\n###############################################################################\ngroup = sm.stamp_creator()          # freeze current step\ngroup(\"chunk-A embedded\")\ngroup(\"chunk-B embedded\")\n\n###############################################################################\n# 3\ufe0f\u20e3  Export data for visualisation / debugging.                              #\n###############################################################################\ntimeline = sm.all_stamps            # total ordering by step then time\nstep_map = sm.step_logs             # Dict[int, List[str]]\n</code></pre> <p>Because <code>StampManager</code> is thread-safe and serialisable (<code>__getstate__</code>, <code>__setstate__</code>), it can be propagated through distributed tasks or pickled into reports for post-hoc analysis.</p>"},{"location":"api_reference/features/logging_profiling/#2-external-contracts","title":"2. External Contracts","text":"Type Name / Endpoint Description Notes Environment Var N/A Logging level and file output are deliberately configured in-code to avoid unexpected production overrides. Explicit design choice\u2014see Design \u00a73.2 CLI <code>python -m railtracks \u2026</code> Inherits logging configuration from <code>ExecutorConfig</code> when initialising a CLI <code>Session</code>. See <code>features/cli_interface.md</code> Python API <code>get_rt_logger()</code> Returns a logger compliant with <code>logging.Logger</code>. Link: <code>../components/logger_creation.md</code> Event Structured log message string Consumers (AIOLogstash, Fluentd, etc.) may tail the file handler or stdout. The message grammar is defined in <code>logging.actions</code> <p>Should future integrations require environment toggles (e.g., <code>RT_LOG_LEVEL</code>), they must be added to this contract table and the configuration pipeline (see Design \u00a73.4).</p>"},{"location":"api_reference/features/logging_profiling/#3-design-and-architecture","title":"3. Design and Architecture","text":""},{"location":"api_reference/features/logging_profiling/#31-architectural-overview","title":"3.1 Architectural Overview","text":"<pre><code>flowchart TD\n    subgraph Logging\n        EC[ExecutorConfig] --&gt;|calls| LC[prepare_logger()]\n        LC --&gt; RTLogger[RT root logger]\n        RTLogger --&gt; CH[ConsoleHandler]\n        RTLogger --&gt; FH[FileHandler?]\n        RTLogger --&gt; SH[StructuredAction msgs]\n    end\n\n    subgraph Profiling\n        StampManager --&gt; Stamp\n        StampManager --&gt;|serialize| Reports\n    end\n\n    classDef dashed stroke-dasharray: 4 4\n    Reports-. external tooling .-&gt; Grafana\n</code></pre> <p>\u2022  Centralised bootstrap (<code>ExecutorConfig.prepare_logger</code>) ensures all code    paths share a single <code>logging.Logger</code> hierarchy. \u2022  Colourised console output implemented in   <code>ColorfulFormatter.format(record)</code>\u2014keeps ANSI details out of business logic. \u2022  Structured action logging decouples node life-cycle semantics from   presentation; the stringification (<code>to_logging_msg</code>) is the only   presentation-layer knowledge the logger needs. \u2022  Profiling is purposely orthogonal; it does not emit log lines but can be   consumed by the logging sub-system or by external visualisers.</p>"},{"location":"api_reference/features/logging_profiling/#32-guiding-principles","title":"3.2 Guiding Principles","text":"Principle Applied To Resulting Trade-off Minimal global mutable state Logger config is held in <code>logging</code> module only Simpler concurrency story; requires explicit early configuration Fail-fast misconfiguration Invalid <code>logging_setting</code> raises <code>ValueError</code> Safe defaults over silent errors Low overhead profiling <code>Stamp</code> is a dataclass; dict copies in getters O(1) creation, but large logs may require post-processing for memory Human-first logs Colourful, action verbs (CREATED/DONE/FAILED) Log parsing requires colour stripping when exporting"},{"location":"api_reference/features/logging_profiling/#33-alternative-designs-considered","title":"3.3 Alternative Designs Considered","text":"<ol> <li>Third-party logging frameworks (Loguru, structlog) \u2013 Rejected to avoid    additional transitive dependencies and keep interoperability with Python\u2019s    standard library.</li> <li><code>tracemalloc</code> / cProfile for profiling \u2013 Valuable for deep performance    analysis but too heavyweight for day-to-day operator needs.  <code>StampManager</code>    hits the 80-20 use-case.</li> </ol>"},{"location":"api_reference/features/logging_profiling/#34-extensibility-hooks","title":"3.4 Extensibility Hooks","text":"<p>\u2022  Custom Handlers \u2013 Import <code>logging.Handler</code> subclasses and attach them   after <code>prepare_logger()</code>:</p> <pre><code>from railtracks.utils.logging.create import get_rt_logger\nfrom mycompany.logging import SlackHandler\n\nlogger = get_rt_logger()\nlogger.addHandler(SlackHandler(token=\"\u2026\"))\n</code></pre> <p>\u2022  Formatter Plug-in \u2013 Replace <code>ColorfulFormatter</code> with any   <code>logging.Formatter</code> compliant object at runtime.</p> <p>\u2022  Profiling Aggregators \u2013 Stamps can be JSON-serialised and shipped to   observability back-ends (Grafana Tempo, Honeycomb, etc.).</p>"},{"location":"api_reference/features/logging_profiling/#4-related-files","title":"4. Related Files","text":""},{"location":"api_reference/features/logging_profiling/#41-related-component-files","title":"4.1 Related Component Files","text":"<ul> <li><code>../components/executor_configuration.md</code>   \u2013 Primary entry point for user-facing logging configuration (<code>ExecutorConfig</code>).</li> <li><code>../components/logging_configuration.md</code>   \u2013 Implementation details for colourised console, file handlers, and log levels.</li> <li><code>../components/logger_creation.md</code>   \u2013 Explains <code>get_rt_logger</code> and logger naming conventions.</li> <li><code>../components/logging_actions.md</code>   \u2013 Describes structured log action classes for request lifecycle events.</li> <li><code>../components/profiling.md</code>   \u2013 Covers <code>Stamp</code>, <code>StampManager</code>, and thread-safety guarantees.</li> </ul>"},{"location":"api_reference/features/logging_profiling/#42-related-feature-files","title":"4.2 Related Feature Files","text":"<ul> <li><code>./cli_interface.md</code>   \u2013 CLI sessions automatically pick up global logging settings.</li> <li><code>./state_management.md</code>   \u2013 Persisted state may include <code>StampManager</code> snapshots for offline analysis.</li> </ul>"},{"location":"api_reference/features/logging_profiling/#43-external-dependencies","title":"4.3 External Dependencies","text":"<ul> <li><code>https://pypi.org/project/colorama/</code>   \u2013 ANSI colour support for cross-platform console logging.</li> </ul>"},{"location":"api_reference/features/logging_profiling/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (2024-06-11) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial feature documentation.</li> </ul>"},{"location":"api_reference/features/mcp_integration/","title":"Mcp integration","text":""},{"location":"api_reference/features/mcp_integration/#mcp-integration","title":"MCP Integration","text":"<p>Provides first-class, bi-directional integration between RailTracks and the Model Context Protocol (MCP): consume any remote MCP tool as a RailTracks <code>Node</code>, or expose any RailTracks <code>Node</code> as a FastMCP-compatible tool.</p> <p>Version: 0.0.1</p>"},{"location":"api_reference/features/mcp_integration/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Functional Overview</li> <li>1.1 Consuming Remote MCP Tools</li> <li>1.2 Exposing RailTracks Nodes as MCP Tools</li> <li>1.3 Jupyter / Windows Compatibility</li> <li>2. External Contracts</li> <li>2.1 Supported Transports</li> <li>2.2 Environment Variables &amp; Tokens</li> <li>2.3 CLI Commands</li> <li>3. Design and Architecture</li> <li>3.1 Runtime Data-flow (RailTracks \u21d2 MCP)</li> <li>3.2 Runtime Data-flow (MCP \u21d2 RailTracks)</li> <li>3.3 Threading &amp; Event-Loop Model</li> <li>3.4 Key Design Decisions &amp; Trade-offs</li> <li>4. Related Files</li> <li>4.1 Related Component Files</li> <li>4.2 External Dependencies</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/features/mcp_integration/#1-functional-overview","title":"1. Functional Overview","text":"<p>The feature is organised around three developer task sets.</p>"},{"location":"api_reference/features/mcp_integration/#11-consuming-remote-mcp-tools","title":"1.1 Consuming Remote MCP Tools","text":"<p>Connect to any MCP server (HTTP Stream, SSE, or STDIO) and instantly convert its tools into importable RailTracks <code>Node</code> classes that can be invoked inside agents, graphs, batch pipelines, etc.</p> <pre><code>from railtracks.rt_mcp import MCPHttpParams\nfrom railtracks.rt_mcp.mcp_tool import connect_mcp\nfrom railtracks.nodes.easy_usage_wrappers.agent import Agent      # Example usage\n\n# 1) Establish a connection ---------------------------------------------------\nmcp_server = connect_mcp(\n    config=MCPHttpParams(\n        url=\"https://fetch.mcp.ai/streamable-http\",      # or \"\u2026/sse\"\n        headers={\"Authorization\": f\"Bearer {MCP_TOKEN}\"},\n    )\n)\n\n# 2) Inspect available tools --------------------------------------------------\nfor ToolNode in mcp_server.tools:\n    print(ToolNode.name())\n\n# 3) Use tools like ordinary Nodes -------------------------------------------\ntranslator = next(t for t in mcp_server.tools if t.name() == \"translate\")\nresult     = translator.prepare_tool({\"text\": \"Hello world\", \"target\": \"es\"}).invoke()\nprint(result)     # \u2192 \"Hola mundo\"\n</code></pre> <p>The <code>MCPServer</code> object keeps the underlying connection open; close it explicitly when finished to free resources:</p> <pre><code>mcp_server.close()\n</code></pre>"},{"location":"api_reference/features/mcp_integration/#12-exposing-railtracks-nodes-as-mcp-tools","title":"1.2 Exposing RailTracks Nodes as MCP Tools","text":"<p>Publish your own nodes so they can be consumed by any MCP-compatible client or LLM agent.</p> <pre><code>from railtracks.nodes.easy_usage_wrappers.function import function_node\nfrom railtracks.rt_mcp.node_to_mcp import create_mcp_server\n\n# 1) Wrap business logic into a RailTracks Node -------------------------------\n@function_node\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Return a + b.\"\"\"\n\n# 2) Create and launch a FastMCP server ---------------------------------------\nmcp = create_mcp_server(nodes=[add])\nmcp.run(host=\"0.0.0.0\", port=8080, transport=\"streamable-http\")\n</code></pre> <p>The server now advertises an MCP tool named <code>add</code> that accepts JSON input <code>{\"a\": &lt;int&gt;, \"b\": &lt;int&gt;}</code> and returns the sum.</p>"},{"location":"api_reference/features/mcp_integration/#13-jupyter-windows-compatibility","title":"1.3 Jupyter / Windows Compatibility","text":"<p>On Windows, MCP\u2019s internal process-creation code is incompatible with Jupyter\u2019s custom I/O streams. Importing <code>connect_mcp</code> (or explicitly calling <code>railtracks.rt_mcp.jupyter_compat.apply_patches()</code>) transparently monkey-patches MCP to work inside notebooks:</p> <pre><code>from railtracks.rt_mcp.jupyter_compat import apply_patches\napply_patches()   # Safe to call multiple times / no-op on non-Windows.\n</code></pre>"},{"location":"api_reference/features/mcp_integration/#2-external-contracts","title":"2. External Contracts","text":""},{"location":"api_reference/features/mcp_integration/#21-supported-transports","title":"2.1 Supported Transports","text":"Transport Endpoint Shape Notes Streamable-HTTP <code>POST /streamable-http</code> Bidirectional HTTP chunked stream. Server-Sent Events <code>GET  /sse</code> One HTTP request per invocation. STDIO Local executable exposing stdin/stdout Usually used for CLI wrappers. <p>All transports share the same JSON envelope defined by the MCP specification (see upstream MCP docs).</p>"},{"location":"api_reference/features/mcp_integration/#22-environment-variables-tokens","title":"2.2 Environment Variables &amp; Tokens","text":"Variable Purpose When Required <code>MCP_TOKEN</code> Bearer token added to <code>Authorization</code> header. Remote HTTP/SSE servers that require auth. Service-specific variables e.g. <code>OPENAI_API_KEY</code>, <code>NOTION_TOKEN</code>, etc. Only when the server needs them; not referenced by client-side code."},{"location":"api_reference/features/mcp_integration/#23-cli-commands","title":"2.3 CLI Commands","text":"<p>While this feature has no dedicated CLI, typical workflows rely on FastMCP\u2019s CLI for local servers:</p> <pre><code># Serve current project\u2019s tools on port 8080\npython -m mcp.server.fastmcp --host 0.0.0.0 --port 8080\n</code></pre>"},{"location":"api_reference/features/mcp_integration/#3-design-and-architecture","title":"3. Design and Architecture","text":""},{"location":"api_reference/features/mcp_integration/#31-runtime-data-flow-railtracks-mcp","title":"3.1 Runtime Data-flow (RailTracks \u21d2 MCP)","text":"<pre><code>flowchart TD\n    RT_Node[\"RailTracks Node(generated)\"] --&gt;|invoke()| MCPAsyncClient\n    MCPAsyncClient --&gt;|await| mcp.ClientSession\n    ClientSession --&gt; RemoteServer[\"Remote MCP Server\"]\n</code></pre> <ol> <li><code>connect_mcp</code> \u279c instantiates <code>MCPServer</code>, spinning up a background thread.</li> <li>The thread creates its own asyncio event loop and an <code>MCPAsyncClient</code>.</li> <li>Tools are lazily fetched (<code>list_tools</code>) and cached.</li> <li>Each <code>Tool</code> record from MCP is wrapped by <code>from_mcp</code> into a <code>Node</code>    subclass whose <code>invoke</code> submits an RPC via <code>call_tool</code>.</li> </ol>"},{"location":"api_reference/features/mcp_integration/#32-runtime-data-flow-mcp-railtracks","title":"3.2 Runtime Data-flow (MCP \u21d2 RailTracks)","text":"<pre><code>flowchart TD\n    ExternalClient[\"MCP Client / LLM\"] --&gt; FastMCP[\"FastMCP Server\"]\n    FastMCP --&gt;|dispatch tool fn| _create_tool_function\n    _create_tool_function --&gt;|await| railtracks.interaction.call\n    call --&gt; UserNode[\"User-defined Node\"]\n</code></pre> <p><code>create_mcp_server</code> converts each <code>Node</code> into an asynchronous function that: 1. Validates &amp; orders parameters via <code>_parameters_to_json_schema</code>. 2. Delegates execution to the Node\u2019s <code>prepare_tool</code> + <code>call</code> helper (ensuring    standardised logging/metrics).</p>"},{"location":"api_reference/features/mcp_integration/#33-threading-event-loop-model","title":"3.3 Threading &amp; Event-Loop Model","text":"Context Thread Event Loop Rationale Main application main any Runs user code, agents, etc. <code>MCPServer</code> background child private Keeps network I/O off the main loop; avoids deadlocks when the application already owns an asyncio loop (common in Streamlit, Jupyter). <code>FastMCP</code> server main uvicorn loop Follows FastAPI/Starlette conventions (FastMCP\u2019s implementation). <p>Cross-thread invocations use <code>asyncio.run_coroutine_threadsafe</code>, enforcing a timeout derived from <code>config.timeout</code>.</p>"},{"location":"api_reference/features/mcp_integration/#34-key-design-decisions-trade-offs","title":"3.4 Key Design Decisions &amp; Trade-offs","text":"Decision Motivation Trade-offs Background thread for <code>MCPServer</code> Transparent usage from both sync &amp; async user code Extra complexity; must marshal data across threads. Lazy tool list caching Minimise round-trips on each invocation Tools added to the remote server after initialisation will not appear unless the user reconnects. Monkey patch instead of fork/exec shim Keep Jupyter support zero-config Risk of upstream MCP internals changing; mitigated by patch guard checks. Parameter JSON-Schema translation Preserve Node parameter metadata inside FastMCP Requires dual maintenance of schema mapping helpers."},{"location":"api_reference/features/mcp_integration/#4-related-files","title":"4. Related Files","text":""},{"location":"api_reference/features/mcp_integration/#41-related-component-files","title":"4.1 Related Component Files","text":"<ul> <li><code>components/jupyter_compatibility.md</code>: Details monkey-patching strategy for notebook environments.</li> <li><code>components/mcp_client_server.md</code>: Explains the <code>MCPAsyncClient</code> / <code>MCPServer</code> internal design.</li> <li><code>components/mcp_tool_connection.md</code>: Documents the <code>connect_mcp</code> convenience wrapper.</li> <li><code>components/node_to_mcp_server.md</code>: Describes exporting Nodes via <code>create_mcp_server</code>.</li> </ul>"},{"location":"api_reference/features/mcp_integration/#42-external-dependencies","title":"4.2 External Dependencies","text":"<ul> <li><code>https://github.com/modelcontext/mcp</code>: Upstream MCP reference implementation.</li> <li><code>https://github.com/modelcontext/fastmcp</code>: FastAPI-based MCP server used by <code>node_to_mcp_server</code>.</li> </ul>"},{"location":"api_reference/features/mcp_integration/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial public documentation.</li> </ul>"},{"location":"api_reference/features/node_interaction/","title":"Node interaction","text":""},{"location":"api_reference/features/node_interaction/#node-interaction","title":"Node Interaction","text":"<p>Uniform, high-level APIs for invoking Railtracks nodes\u2014individually or in batches\u2014both synchronously and asynchronously, with first-class support for streaming intermediate outputs.</p> <p>Version: 0.0.1\u2003</p>"},{"location":"api_reference/features/node_interaction/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Functional Overview</li> <li>2. External Contracts</li> <li>3. Design and Architecture</li> <li>4. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/features/node_interaction/#1-functional-overview","title":"1. Functional Overview","text":"<p><code>railtracks.interaction</code> exposes four helper functions that cover the entire node-execution life-cycle:</p> Helper Purpose <code>await call()</code> Asynchronous single-node invocation (preferred). <code>call_sync()</code> Blocking wrapper around <code>call()</code> for non-async code paths. <code>await call_batch()</code> Parallel fan-out over multiple iterables. <code>await broadcast()</code> Stream a chunk to the current node\u2019s subscribers. <p>These helpers work inside or outside an active <code>rt.Session()</code>\u2014they handle context boot-strapping transparently.</p>"},{"location":"api_reference/features/node_interaction/#11-single-node-invocation","title":"1.1 Single Node Invocation","text":"<p>Run a node once, await its result.</p> <pre><code>import railtracks as rt\nfrom my_nodes import GreeterNode\n\nasync def main():\n    greeting = await rt.call(GreeterNode, \"Alice\")\n    print(greeting)               # \u2192 \"Hello Alice!\"\n</code></pre> <p><code>call()</code> returns a coroutine; schedule several of them to run concurrently:</p> <pre><code>tasks = [rt.call(GreeterNode, name) for name in [\"Ada\", \"Grace\", \"Linus\"]]\nresults = await asyncio.gather(*tasks)\n</code></pre>"},{"location":"api_reference/features/node_interaction/#synchronous-wrapper","title":"Synchronous Wrapper","text":"<p>For legacy / synchronous scripts:</p> <pre><code>result = rt.call_sync(GreeterNode, \"Bob\")\n</code></pre> <p>Attempting to invoke <code>call_sync()</code> inside an already running <code>asyncio</code> loop raises an explanatory <code>RuntimeError</code>.</p>"},{"location":"api_reference/features/node_interaction/#12-batch-invocation","title":"1.2 Batch Invocation","text":"<p>Map a node over N sets of arguments in parallel and preserve input order.</p> <pre><code>names = [\"Anna\", \"Ben\", \"Cara\"]\nresults = await rt.call_batch(GreeterNode, names)\n</code></pre> <p>Internally this is a thin wrapper around <code>asyncio.gather()</code> with <code>return_exceptions=True</code> by default, so exceptions are gathered rather than raised prematurely.</p>"},{"location":"api_reference/features/node_interaction/#13-streaming","title":"1.3 Streaming","text":"<p>Push partial results downstream\u2014ideal for token-by-token LLM streaming or status updates.</p> <pre><code>await rt.broadcast(\"chunk-1\")\nawait rt.broadcast(\"chunk-2\")\n</code></pre> <p>Consumers attach any callable (sync or async) via <code>ExecutorConfig.subscriber</code>; see <code>components/pubsub_messaging.md</code> for subscription utilities.</p>"},{"location":"api_reference/features/node_interaction/#14-session-less-usage","title":"1.4 Session-less Usage","text":"<p>All helpers auto-create a throw-away runtime when invoked outside a <code>rt.Session()</code>:</p> <pre><code># \u2705 valid \u2013 automatic Session()\nresult = rt.call_sync(GreeterNode, \"Eve\")\n</code></pre>"},{"location":"api_reference/features/node_interaction/#15-timeout-cancellation","title":"1.5 Timeout &amp; Cancellation","text":"<p><code>rt.set_config(timeout=...)</code> sets a global per-request timeout; hitting it raises <code>GlobalTimeOutError</code>. Inside a node you may still raise <code>asyncio.TimeoutError</code> for fine-grained control\u2014<code>call()</code> distinguishes the two cases.</p>"},{"location":"api_reference/features/node_interaction/#2-external-contracts","title":"2. External Contracts","text":"<p>Node Interaction itself does not expose HTTP endpoints or CLI commands; instead it relies on the Pub/Sub infrastructure it shares with the rest of Railtracks.</p>"},{"location":"api_reference/features/node_interaction/#21-pubsub-messages","title":"2.1 Pub/Sub Messages","text":"Message Type Produced When Docs <code>RequestCreation</code> A new node is about to start. <code>pubsub_messaging</code> <code>RequestSuccess</code>, <code>RequestFailure</code> Node finished (successfully / with error). \u2033 <code>Streaming</code> <code>broadcast()</code> is called. \u2033 <code>FatalFailure</code> Unrecoverable framework error detected. \u2033 <p>Subscribers typically register through <code>RTPublisher.listener()</code>.</p>"},{"location":"api_reference/features/node_interaction/#22-configuration-flags","title":"2.2 Configuration Flags","text":"Name Type Default Effect <code>ExecutorConfig.timeout</code> float <code>30.0</code> Hard deadline enforced by <code>call()</code> top-level wrapper. <code>ExecutorConfig.subscriber</code> callable \u2014 Callback receiving <code>Streaming</code> messages. <p>Configure via <code>rt.set_config(...)</code> before spawning nodes.</p>"},{"location":"api_reference/features/node_interaction/#3-design-and-architecture","title":"3. Design and Architecture","text":"<p>Node Interaction is purposely small but sits at the nexus of multiple slow-changing contracts (context, pub/sub, executor config). The following diagram highlights the control flow when a top-level <code>call()</code> is issued.</p> <pre><code>sequenceDiagram\n    participant User\n    participant call() as call()/call_batch()\n    participant Context as Context(central.py)\n    participant Pub as RTPublisher\n    participant Coord as Task Coordinator\n    participant Node as Target Node\n\n    User-&gt;&gt;call(): invoke\n    call()-&gt;&gt;Context: ensure Session &amp; Publisher\n    activate Pub\n    call()-&gt;&gt;Pub: publish RequestCreation\n    Pub-&gt;&gt;Coord: (subscription) new task\n    par Node execution\n        Coord-&gt;&gt;Node: invoke\n        Node--&gt;&gt;Pub: Streaming (optional)\n    end\n    Node--&gt;&gt;Coord: result / exception\n    Coord--&gt;&gt;Pub: RequestSuccess / RequestFailure\n    Pub--&gt;&gt;call(): message filtered via request_id\n    deactivate Pub\n    call()--&gt;&gt;User: final result or raised error\n</code></pre>"},{"location":"api_reference/features/node_interaction/#31-key-design-decisions-trade-offs","title":"3.1 Key Design Decisions &amp; Trade-offs","text":"<ol> <li> <p>Single Entry Surface    Four public helpers cover 99 % of invocation patterns; remaining power-user needs (e.g., custom execution strategies) live in <code>features/task_execution.md</code>.</p> </li> <li> <p>Context-Aware Behaviour </p> </li> <li>No context \u2192 spin up temporary runner so library remains usable in REPL scripts.  </li> <li>Active but inactive context (i.e., top-level node) \u2192 <code>_start()</code> performs publisher life-cycle &amp; timeout enforcement.  </li> <li> <p>Active and running context (nested node) \u2192 <code>_run()</code> skips extra book-keeping for efficiency.</p> </li> <li> <p>Message Filtering    To avoid cross-talk with concurrent requests, <code>call()</code> installs a per-request <code>listener()</code> whose predicate matches the generated <code>request_id</code>.</p> </li> <li> <p>Timeout Handling    Differentiates framework timeout (<code>GlobalTimeOutError</code>) from user-land <code>asyncio.TimeoutError</code>, preserving semantics.</p> </li> <li> <p>Sync Wrapper Safety <code>call_sync()</code> detects an already running loop and fails fast, preventing dead-locks that plague na\u00efve <code>asyncio.run()</code> wrappers.</p> </li> <li> <p>Batch Determinism    Results keep input ordering even though workers run concurrently; callers thus avoid cumbersome index mapping.</p> </li> <li> <p>Streaming Simplicity <code>broadcast()</code> is a deliberate one-liner that just publishes <code>Streaming</code>\u2014no extra abstractions until proven necessary.</p> </li> </ol>"},{"location":"api_reference/features/node_interaction/#32-rejected-alternatives","title":"3.2 Rejected Alternatives","text":"Alternative Reason for Rejection Exposing raw <code>RTPublisher</code> APIs to users Too low-level; duplicates logic for request tracking. Auto-retrying failed nodes in <code>call_batch</code> Responsibility better handled by orchestration layer; keeps this component stateless. Allowing <code>call_sync()</code> inside event loop Leads to blocking calls that stall the loop; explicit error is safer."},{"location":"api_reference/features/node_interaction/#4-related-files","title":"4. Related Files","text":""},{"location":"api_reference/features/node_interaction/#41-related-component-files","title":"4.1 Related Component Files","text":"<ul> <li><code>components/node_interaction.md</code> \u2013 API reference &amp; low-level implementation.</li> <li><code>components/pubsub_messaging.md</code> \u2013 message types and publisher utilities used under the hood.</li> <li><code>components/context_management.md</code> \u2013 context &amp; executor configuration accessed by helpers.</li> <li><code>components/task_execution.md</code> \u2013 coordinator and execution strategies that run the actual work.</li> </ul>"},{"location":"api_reference/features/node_interaction/#42-related-feature-files","title":"4.2 Related Feature Files","text":"<ul> <li><code>features/task_execution.md</code> \u2013 drives job scheduling and integrates with Node Interaction.</li> <li><code>features/state_management.md</code> \u2013 persists node results and context if <code>ExecutorConfig.save_state=True</code>.</li> </ul>"},{"location":"api_reference/features/node_interaction/#43-external-dependencies","title":"4.3 External Dependencies","text":"<p>None beyond Python \u22653.9 standard library; all intra-framework communication leverages Railtracks\u2019 own Pub/Sub implementation.</p>"},{"location":"api_reference/features/node_interaction/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (2024-06-05) [<code>&lt;INITIAL&gt;</code>]: Initial public documentation.</li> </ul>"},{"location":"api_reference/features/node_management/","title":"Node management","text":""},{"location":"api_reference/features/node_management/#node-management","title":"Node Management","text":"<p>Provides the core abstractions and infrastructure for creating, executing, serialising and debugging Railtracks \u201cnodes\u201d\u2014small, composable units of asynchronous work.</p> <p>Version: 0.0.1\u2003</p>"},{"location":"api_reference/features/node_management/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Functional Overview</li> <li>2. External Contracts</li> <li>3. Design and Architecture</li> <li>4. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/features/node_management/#1-functional-overview","title":"1. Functional Overview","text":"<p>Railtracks workflows are expressed as graphs of nodes. The Node Management feature offers a complete, opinionated life-cycle for those nodes:</p> <ol> <li>Definition \u2013 subclass <code>railtracks.nodes.Node</code> or use higher-level helpers (e.g. <code>NodeBuilder</code> or function-wrappers) to declare new node types.  </li> <li>Creation \u2013 validate the new class with <code>node_creation.validation</code> and optionally register it with a registry.  </li> <li>Invocation \u2013 execute a node asynchronously (<code>await node.tracked_invoke()</code>) while automatically capturing debug &amp; latency data.  </li> <li>Introspection \u2013 access <code>node.details</code> for rich runtime telemetry.  </li> <li>Serialisation \u2013 convert a live node into a light-weight <code>NodeState</code> object that can cross process or network boundaries, then re-hydrate it with <code>.instantiate()</code>.  </li> <li>Tool exposure \u2013 expose a node as an LLM-callable tool via the <code>ToolCallable</code> mix-in.</li> </ol> <p>The following sections group the most common developer tasks.</p>"},{"location":"api_reference/features/node_management/#11-create-a-simple-node","title":"1.1 Create a Simple Node","text":"<pre><code>from railtracks.nodes import Node\n\nclass Adder(Node[int]):\n    @classmethod\n    def name(cls) -&gt; str:\n        return \"Adder\"\n\n    def __init__(self, a: int, b: int):\n        super().__init__()\n        self.a, self.b = a, b\n\n    async def invoke(self) -&gt; int:\n        return self.a + self.b\n</code></pre>"},{"location":"api_reference/features/node_management/#12-execute-with-latency-tracking","title":"1.2 Execute With Latency Tracking","text":"<pre><code>node = Adder(3, 4)\nresult = await node.tracked_invoke()\nprint(result)                             # 7\nprint(node.details[\"latency\"].total_time) # seconds (float)\n</code></pre>"},{"location":"api_reference/features/node_management/#13-pass-nodes-across-process-boundaries","title":"1.3 Pass Nodes Across Process Boundaries","text":"<pre><code>from railtracks.nodes import NodeState\n\nstate = NodeState(node)         # serialisable!\ntransport(state)                # send over IPC / network\nnew_node = state.instantiate()  # get a working copy\n</code></pre>"},{"location":"api_reference/features/node_management/#14-build-variants-programmatically","title":"1.4 Build Variants Programmatically","text":"<pre><code>from railtracks.nodes import NodeBuilder\n\nCustomAdder = (\n    NodeBuilder(Adder, class_name=\"BigAdder\")\n    .override_attrs(name=lambda cls: \"Big Adder\")\n    .build()\n)\n</code></pre>"},{"location":"api_reference/features/node_management/#15-expose-as-an-llm-tool","title":"1.5 Expose as an LLM Tool","text":"<pre><code>class AdderTool(Adder):\n    @classmethod\n    def tool_info(cls):\n        from railtracks.llm.tools import Tool, Parameter\n        return Tool(\n            name=cls.name(),\n            description=\"Adds two integers\",\n            parameters=[\n                Parameter(name=\"a\", type=\"int\", description=\"first operand\"),\n                Parameter(name=\"b\", type=\"int\", description=\"second operand\"),\n            ],\n        )\n</code></pre>"},{"location":"api_reference/features/node_management/#2-external-contracts","title":"2. External Contracts","text":"<p>Although node management is an internal runtime concern, other systems can interact with it via several stable surfaces.</p>"},{"location":"api_reference/features/node_management/#21-cli","title":"2.1 CLI","text":"Command Description <code>railtracks run &lt;NodeClass&gt; [args\u2026]</code> Spawn a node in an isolated worker. <code>railtracks inspect &lt;state.json&gt;</code> Re-hydrate a <code>NodeState</code> file and print debug info. <p>(See <code>components/cli_entry_point.md</code> for the full specification.)</p>"},{"location":"api_reference/features/node_management/#22-environment-variables","title":"2.2 Environment Variables","text":"Name Default Purpose <code>RT_NODE_DEBUG</code> <code>false</code> When <code>true</code>, each node captures verbose debug data. <code>RT_NODE_SERIALISATION_FMT</code> <code>json</code> Wire-format for <code>NodeState</code> objects."},{"location":"api_reference/features/node_management/#23-event-bus-messages","title":"2.3 Event Bus Messages","text":"<p>If <code>features/task_execution.md</code> is enabled, each <code>tracked_invoke</code> emits:</p> Topic Payload <code>node.started</code> <code>{uuid, name, params}</code> <code>node.finished</code> <code>{uuid, latency_ms, output_summary}</code> <code>node.failed</code> <code>{uuid, latency_ms, exception_type}</code>"},{"location":"api_reference/features/node_management/#3-design-and-architecture","title":"3. Design and Architecture","text":""},{"location":"api_reference/features/node_management/#31-core-abstractions","title":"3.1 Core Abstractions","text":"Class / Module Responsibility <code>railtracks.nodes.Node</code> Abstract base; enforces async <code>invoke</code>, debug hooks, and tool-integration. <code>NodeState</code> Portable, serialisable snapshot of a node. <code>DebugDetails</code>, <code>LatencyDetails</code> Typed containers recorded inside each node\u2019s <code>details</code> dict. <code>NodeBuilder</code> (<code>components/node_building.md</code>) Meta-programming helper that fabricates new subclasses at runtime. Validation modules (<code>node_creation.validation</code>, <code>node_invocation.validation</code>) Static and runtime validation to keep nodes well-formed."},{"location":"api_reference/features/node_management/#why-async-wrapping","title":"Why Async Wrapping?","text":"<p><code>Node.__init_subclass__</code> dynamically wraps the subclass\u2019 <code>invoke</code> method:</p> <pre><code>if not asyncio.iscoroutinefunction(invoke):\n    invoke = asyncio.to_thread(invoke)\n</code></pre> <p>This allows library authors to write sync code without sacrificing the async contract expected by the wider execution engine.</p>"},{"location":"api_reference/features/node_management/#toolcallable-mix-in","title":"ToolCallable Mix-in","text":"<p><code>Node</code> inherits from <code>ToolCallable</code>. This single-inheritance trick means every node can optionally advertise itself as an LLM \u201ctool\u201d by implementing <code>tool_info</code> and <code>prepare_tool</code>, without duplicating boiler-plate.</p>"},{"location":"api_reference/features/node_management/#32-data-control-flow","title":"3.2 Data &amp; Control Flow","text":"<pre><code>flowchart TD\n    subgraph Worker-Process\n        A[Instantiate Node] --&gt;|tracked_invoke| B[Record start time]\n        B --&gt; C[invoke()]\n        C --&gt;|await| D[return output]\n        D --&gt; E[Compute latency]\n        E --&gt; F[Populate DebugDetails]\n        F --&gt; G[Return output to caller]\n    end\n    G --&gt; H[Coordinator / Caller]\n</code></pre>"},{"location":"api_reference/features/node_management/#33-serialisation-strategy","title":"3.3 Serialisation Strategy","text":"<ol> <li><code>NodeState(node)</code> performs a deepcopy of the node (ensures pass-by-value).  </li> <li>The instance is pickled / JSON-encoded (configurable).  </li> <li>On the remote side <code>NodeState.instantiate()</code> returns a shallow reference copy\u2014fast, avoids replaying constructor logic.</li> </ol> <p>Trade-off: deep-copy serialises all attributes, so extremely large tensors or file handles should be stored in external object stores and fetched lazily.</p>"},{"location":"api_reference/features/node_management/#34-failure-semantics","title":"3.4 Failure Semantics","text":"<p>\u2022 Any unhandled exception in <code>invoke</code> bubbles up unchanged; <code>tracked_invoke</code> still records latency and attaches the exception type to <code>details</code>. \u2022 Nodes guarantee at-least-once execution semantics; the surrounding task-execution feature is responsible for retries and idempotency.</p>"},{"location":"api_reference/features/node_management/#35-rejected-alternatives","title":"3.5 Rejected Alternatives","text":"Alternative Reason Rejected Enforcing <code>async def</code> at author-time Too intrusive for simple sync functions; runtime wrapping is safer. ORMs / Dataclasses for serialisation Added heavy dependencies and poorer forward-compatibility."},{"location":"api_reference/features/node_management/#4-related-files","title":"4. Related Files","text":""},{"location":"api_reference/features/node_management/#41-related-component-files","title":"4.1 Related Component Files","text":"<ul> <li><code>components/node_management.md</code>: Detailed component-level API and examples of the <code>Node</code> base class.  </li> <li><code>components/node_building.md</code>: Runtime subclass generation.  </li> <li><code>components/node_state_management.md</code>: Low-level serialisation helpers.  </li> <li><code>components/node_creation_validation.md</code> &amp; <code>components/node_invocation_validation.md</code>: Static and runtime validators.  </li> <li><code>components/response_handling.md</code>: Response objects returned by many node subclasses.  </li> <li><code>components/tool_callable_node.md</code>: Base class that integrates nodes with LLM tool-calling.</li> </ul>"},{"location":"api_reference/features/node_management/#42-related-feature-files","title":"4.2 Related Feature Files","text":"<ul> <li><code>features/node_interaction.md</code>: Higher-level API for orchestrating multiple nodes.  </li> <li><code>features/task_execution.md</code>: Distributed executor which relies heavily on Node Management.  </li> <li><code>features/llm_integration.md</code>: How the LLM subsystem makes use of nodes like <code>TerminalLLM</code>.  </li> </ul>"},{"location":"api_reference/features/node_management/#43-external-dependencies","title":"4.3 External Dependencies","text":"<ul> <li><code>typing-extensions</code> \u2013 <code>Self</code>, <code>TypeVarTuple</code>, etc.  </li> <li>Python \u2265 3.10 \u2013 structural pattern-matching used in validation modules.</li> </ul>"},{"location":"api_reference/features/node_management/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (2024-04-30) [<code>&lt;INITIAL_COMMIT&gt;</code>]: Initial extraction from monolith into standalone feature document.</li> </ul>"},{"location":"api_reference/features/rag_system/","title":"Rag system","text":""},{"location":"api_reference/features/rag_system/#rag-system","title":"RAG System","text":"<p>Provides a plug-and-play Retrieval-Augmented Generation pipeline for chunking, embedding, storing, and semantically searching large collections of documents.</p> <p>Version: 0.0.1\u2003</p>"},{"location":"api_reference/features/rag_system/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Functional Overview</li> <li>2. External Contracts</li> <li>3. Design and Architecture</li> <li>4. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/features/rag_system/#1-functional-overview","title":"1. Functional Overview","text":"<p>The RAG System feature bundles several lower-level components into a cohesive workflow that:</p> <ol> <li>Ingests raw text or files.</li> <li>Splits the text into overlapping chunks.</li> <li>Generates dense embeddings for every chunk.</li> <li>Stores the embeddings (and rich metadata) in a vector store.</li> <li>Performs similarity search to retrieve the k most relevant chunks for an arbitrary user query.</li> </ol> <p>The end result is a simple API that lets application developers add \u201cchat-with-my-data\u201d or \u201csemantic search\u201d capabilities without worrying about the details of chunk sizes, embedding models, or vector-store plumbing.</p>"},{"location":"api_reference/features/rag_system/#11-quickstart-all-in-one-helper","title":"1.1 Quickstart (all-in-one helper)","text":"<p>The <code>railtracks.prebuilt.rag_node.rag_node</code> helper wraps the entire flow in a single function node that can be mounted inside a RailTracks graph.</p> <pre><code>from railtracks.prebuilt.rag_node import rag_node\n\ndocuments = [\n    \"The mitochondrion is the powerhouse of the cell.\",\n    \"ATP is synthesized via oxidative phosphorylation.\",\n]\n\nsearch_node = rag_node(\n    documents,\n    embed_model=\"text-embedding-3-small\",\n    token_count_model=\"gpt-4o\",\n    chunk_size=1_000,\n    chunk_overlap=200,\n)\n\nprint(search_node(\"What produces ATP?\")[0].record.text)\n# \u279c \"ATP is synthesized via oxidative phosphorylation.\"\n</code></pre> <p>Internally this helper:</p> <ol> <li>Creates a <code>rag.rag_core.RAG</code> instance.  </li> <li>Calls <code>embed_all()</code> to preprocess and index the documents.  </li> <li>Returns a callable node that runs <code>RAG.search()</code> for every invocation.</li> </ol>"},{"location":"api_reference/features/rag_system/#12-fine-grained-control","title":"1.2 Fine-grained Control","text":"<p>If you need to swap out the vector store, change similarity metrics, or stream documents gradually, use <code>rag.rag_core.RAG</code> directly.</p> <pre><code>from railtracks.rag.rag_core import RAG\n\nrag = RAG(\n    docs=[\"some text\", \"\u2026\"],\n    embed_config={\"model\": \"text-embedding-3-small\"},\n    store_config={\"backend\": \"memory\", \"metric\": \"cosine\"},\n    chunk_config={\"chunk_size\": 1500, \"chunk_overlap\": 300, \"model\": \"gpt-3.5-turbo\"},\n)\n\nrag.embed_all()                               # \u2b05\ufe0f ingest + index\nhits = rag.search(\"semantic query\", top_k=5)  # \u2b05\ufe0f retrieve\nfor hit in hits:\n    print(hit.score, hit.record.text)\n</code></pre>"},{"location":"api_reference/features/rag_system/#13-incremental-updates","title":"1.3 Incremental Updates","text":"<p><code>RAG</code> exposes the underlying <code>vector_store</code> so you can add new <code>VectorRecord</code>s or delete outdated ones without rebuilding the entire index:</p> <pre><code>new_id = rag.vector_store.add(\n    [\"fresh note about ATP\"],\n    embed=True,\n    metadata=[{\"source\": \"notes.md\"}],\n)[0]\n\nrag.vector_store.delete([new_id])\n</code></pre>"},{"location":"api_reference/features/rag_system/#2-external-contracts","title":"2. External Contracts","text":"<p>The RAG System is a purely in-process Python feature and does not expose HTTP endpoints or CLI commands. The only external contract is the requirement for an embedding provider that follows the <code>litellm</code> interface.</p>"},{"location":"api_reference/features/rag_system/#21-environment-variables","title":"2.1 Environment Variables","text":"Name Purpose Default <code>OPENAI_API_KEY</code> Used by <code>EmbeddingService</code> when no <code>api_key</code> passed. None"},{"location":"api_reference/features/rag_system/#22-configuration-objects","title":"2.2 Configuration Objects","text":"<p>All tunables are passed in as plain Python dictionaries:</p> <ul> <li><code>embed_config</code> \u2192 forwarded to <code>components/embedding_service.md</code> </li> <li><code>chunk_config</code> \u2192 forwarded to <code>components/chunking_service.md</code> </li> <li><code>store_config</code> \u2192 forwarded to <code>components/vector_store_factory.md</code></li> </ul>"},{"location":"api_reference/features/rag_system/#3-design-and-architecture","title":"3. Design and Architecture","text":"<p>At a high-level the feature implements the canonical RAG pipeline:</p> <pre><code>flowchart LR\n    A[Raw Docs / Paths] --&gt;|Text| B[TextChunkingService]\n    B --&gt;|Chunks| C[EmbeddingService]\n    C --&gt;|Vectors\\n+ Metadata| D[VectorStore (In-Memory)]\n    E[Query] --&gt;|Embed| C\n    C --&gt;|Query Vector| F[Similarity Search]\n    F --&gt;|Top-k SearchResult| G[Caller / LLM]\n</code></pre>"},{"location":"api_reference/features/rag_system/#31-component-boundaries","title":"3.1 Component Boundaries","text":"Stage Component (doc) Responsibility Chunking <code>components/chunking_service.md</code> Overlapping splits by char/token. Embedding <code>components/embedding_service.md</code> Batch requests to <code>litellm.embedding</code>. Storage + Search <code>components/vector_store_base.md</code>  \u2190 factory / memory CRUD, similarity search, persistence. Data Abstraction <code>components/text_object_management.md</code> Holds raw content, chunk list, embeddings, metadata hash. Utilities <code>components/rag_utilities.md</code>, <code>components/vector_store_utilities.md</code> Token counting, file-ops, vector math, stable hashing, UUIDs. Orchestration <code>components/rag_core.md</code> Glue code that wires every sub-component into a single fa\u00e7ade. Pre-built Node <code>components/rag_node.md</code> Exposes a single RailTracks node for graph composition."},{"location":"api_reference/features/rag_system/#32-key-design-decisions-trade-offs","title":"3.2 Key Design Decisions &amp; Trade-offs","text":"<ol> <li> <p>In-memory first:    The default backend is <code>InMemoryVectorStore</code> for zero-dependency prototyping. Disk persistence is provided via simple pickle-based <code>persist()</code> / <code>load()</code>. A future roadmap item (v0.1) is to add FAISS / Qdrant backends via the factory.</p> </li> <li> <p>Overlapping chunks:    A default <code>chunk_overlap</code> of 20 % (200 tokens for a 1 000 token chunk) was chosen to reduce boundary information loss at the cost of minor duplication in the store.</p> </li> <li> <p>Cosine similarity + normalization:    Normalization is automatically turned on when the metric is <code>cosine</code>, ensuring distance computations remain numerically stable and comparable.</p> </li> <li> <p>Loose coupling via factory functions:    Both the vector store and chunking strategy can be swapped by injecting a different <code>backend</code> or <code>strategy</code> callable respectively\u2014no changes needed in <code>RAG</code>.</p> </li> <li> <p>Batch-size-agnostic embeddings: <code>EmbeddingService.embed()</code> internally chunks long input lists into 8-item batches to stay within OpenAI\u2019s recommended payload sizes, but keeps the public signature simple.</p> </li> </ol>"},{"location":"api_reference/features/rag_system/#33-rejected-alternatives","title":"3.3 Rejected Alternatives","text":"<ul> <li>Rigid pipeline class: An earlier design hard-coded the ingestion order inside a monolithic class. This was replaced by the current thin fa\u00e7ade + injectable services to encourage experimentation (e.g., multi-modal embeddings).  </li> <li>Auto-token-chunking via GPT-4: Considered using GPT-4 to dynamically decide split points (<code>chunk_smart</code>). Ultimately postponed due to latency/cost concerns; placeholder method remains for future work.</li> </ul>"},{"location":"api_reference/features/rag_system/#4-related-files","title":"4. Related Files","text":""},{"location":"api_reference/features/rag_system/#41-related-component-files","title":"4.1 Related Component Files","text":"<ul> <li><code>components/rag_node.md</code>: one-line RailTracks node wrapper around <code>RAG</code>.</li> <li><code>components/rag_core.md</code>: orchestrator that drives chunking, embedding, storage, and search.</li> <li><code>components/chunking_service.md</code>: text splitting strategies.</li> <li><code>components/embedding_service.md</code>: litellm embedding interface.</li> <li><code>components/vector_store_base.md</code>: abstract CRUD + search API for any vector store backend.</li> <li><code>components/vector_store_factory.md</code>: factory that selects <code>InMemoryVectorStore</code> by default.</li> <li><code>components/in_memory_vector_store.md</code>: default, dependency-free backend.</li> <li><code>components/text_object_management.md</code>: canonical representation of a document + metadata.</li> <li><code>components/rag_utilities.md</code>: tokenization &amp; file helpers.</li> <li><code>components/vector_store_utilities.md</code>: vector math, UUID, hashing.</li> </ul>"},{"location":"api_reference/features/rag_system/#42-external-dependencies","title":"4.2 External Dependencies","text":"<ul> <li><code>https://github.com/BerriAI/litellm</code>: lightweight wrapper used for both embeddings and tokenization.</li> <li><code>https://pypi.org/project/tiktoken/</code>: fast tokenizer leveraged by <code>LORAGTokenizer</code>.</li> </ul>"},{"location":"api_reference/features/rag_system/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (2024-06-07) [<code>&lt;INIT&gt;</code>]: Initial specification of the RAG System feature.</li> </ul>"},{"location":"api_reference/features/state_management/","title":"State management","text":""},{"location":"api_reference/features/state_management/#state-management","title":"State Management","text":"<p>End-to-end facilities for capturing, mutating, and querying the entire runtime state of a RailTracks execution\u2014including nodes, requests, timestamps, graph relationships, and serialization to external systems.</p> <p>Version: 0.0.1\u2003</p>"},{"location":"api_reference/features/state_management/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Functional Overview</li> <li>2. External Contracts</li> <li>3. Design and Architecture</li> <li>4. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/features/state_management/#1-functional-overview","title":"1. Functional Overview","text":"<p>The State Management feature is the canonical source of truth for what is happening inside RailTracks at any moment. It combines multiple cooperating components\u2014<code>Forest</code>, <code>NodeForest</code>, <code>RequestForest</code>, <code>ExecutionInfo</code>, <code>RTState</code>, utility helpers, and custom (de)serialization\u2014to deliver:</p> <p>\u2022 Historical object tracking (\u201ctime-travel\u201d) \u2022 Request lifecycle orchestration and visibility \u2022 Graph-based introspection of nodes \u2194 requests \u2194 timestamps \u2022 Snapshotting for UIs / analytics \u2022 Safe, JSON-serializable exports for remote consumers</p>"},{"location":"api_reference/features/state_management/#11-object-history-time-travel","title":"1.1 Object History &amp; \u201cTime-Travel\u201d","text":"<p>At the lowest level, immutable objects are stored in a <code>Forest</code> indexed by identifier. Developers can rewind the heap to any logical step using <code>Forest.time_machine()</code>.</p> <pre><code>from railtracks.state.forest import Forest, AbstractLinkedObject\nfrom railtracks.utils.profiling import Stamp\n\nclass ConfigSnapshot(AbstractLinkedObject):\n    ...\n\nsnapshots = Forest[ConfigSnapshot]()\nstamp1 = Stamp(step=1, identifier=\"init\")\nsnapshots._update_heap(ConfigSnapshot(\"cfg\", stamp1, parent=None))\n\n# Later \u2026\nsnapshots.time_machine(step=1)        # Heap is now identical to step == 1\n</code></pre>"},{"location":"api_reference/features/state_management/#12-request-lifecycle-tracking","title":"1.2 Request Lifecycle Tracking","text":"<p>Requests between nodes are first-class citizens represented by <code>RequestForest</code>. Creating, updating, and querying requests gives developers full visibility into in-flight work.</p> <pre><code>from railtracks.state.request import RequestForest\nfrom railtracks.utils.profiling import Stamp\n\nrf = RequestForest()\nreq_id = rf.create(\n    identifier=\"my_req\", source_id=None, sink_id=\"node_a\",\n    input_args=(), input_kwargs={}, stamp=Stamp()\n)\n\n# Mark as finished later on\nrf.update(req_id, output=\"ok \u2705\", stamp=Stamp())\n</code></pre>"},{"location":"api_reference/features/state_management/#13-execution-snapshots-graph-visualisation","title":"1.3 Execution Snapshots &amp; Graph Visualisation","text":"<p><code>ExecutionInfo</code> bundles all active <code>NodeForest</code>, <code>RequestForest</code>, and <code>StampManager</code> data into a single immutable snapshot. Its <code>.graph_serialization()</code> helper turns the snapshot into a UI-friendly JSON graph.</p> <pre><code>from railtracks.state.info import ExecutionInfo\n\nsnapshot = ExecutionInfo.create_new()\njson_graph = snapshot.graph_serialization()        # &lt;- ready for front-end\n</code></pre>"},{"location":"api_reference/features/state_management/#14-runtime-orchestration-rtstate","title":"1.4 Runtime Orchestration (<code>RTState</code>)","text":"<p><code>RTState</code> is the orchestrator that:</p> <ol> <li>Creates nodes + requests (<code>_create_node_and_request</code>)</li> <li>Executes tasks asynchronously via the Coordinator</li> <li>Mutates forests on success / failure</li> <li>Publishes Pub/Sub events for observability</li> </ol> <pre><code>state = RTState(\n    execution_info=ExecutionInfo.create_new(),\n    executor_config=my_cfg,\n    coordinator=my_coordinator,\n    publisher=my_pub\n)\n\nawait state.call_nodes(\n    parent_node_id=None,\n    request_id=None,\n    node=MyNode,          # subclass of railtracks.nodes.Node\n    args=(), kwargs={}\n)\n</code></pre>"},{"location":"api_reference/features/state_management/#15-sub-state-extraction-for-debugging","title":"1.5 Sub-State Extraction for Debugging","text":"<p><code>create_sub_state_info()</code> (see <code>State Utilities</code>) lets you isolate just the downstream branch of a node or request\u2014perfect for error isolation or focused UI views.</p> <pre><code>from railtracks.state.utils import create_sub_state_info\n\nsub_nodes, sub_reqs = create_sub_state_info(\n    node_heap=snapshot.node_forest.heap(),\n    request_heap=snapshot.request_forest.heap(),\n    parent_ids=\"root_request_id\"\n)\n</code></pre>"},{"location":"api_reference/features/state_management/#2-external-contracts","title":"2. External Contracts","text":""},{"location":"api_reference/features/state_management/#21-pubsub-messages","title":"2.1 Pub/Sub Messages","text":"<p><code>RTState</code> publishes the following message types (see <code>railtracks/pubsub/messages.py</code>):</p> Message When Emitted Owned By <code>RequestCreation</code> Before a node is executed RT State Management <code>RequestSuccess</code> / <code>RequestFailure</code> After task completion Same <code>FatalFailure</code> Fatal, system-terminating error occurred Same <p>Down-stream systems (CLI visualiser, HTTP server, etc.) subscribe to these messages to update live dashboards.</p>"},{"location":"api_reference/features/state_management/#22-configuration-flags","title":"2.2 Configuration &amp; Flags","text":"Name Default Purpose <code>executor_config.end_on_error</code> <code>False</code> If <code>True</code>, any error triggers a <code>FatalFailure</code> broadcast and shutdown. <p>No dedicated environment variables are required; all knobs reside in <code>ExecutorConfig</code>.</p>"},{"location":"api_reference/features/state_management/#3-design-and-architecture","title":"3. Design and Architecture","text":""},{"location":"api_reference/features/state_management/#31-high-level-view","title":"3.1 High-Level View","text":"<pre><code>graph TD\n    RTState --&gt;|creates/updates| NodeForest &amp; RequestForest\n    NodeForest --&gt;|immutable nodes| ExecutionInfo\n    RequestForest --&gt;|edges| ExecutionInfo\n    ExecutionInfo --&gt;|serialize| RTJSONEncoder\n    RTState --&gt;|Pub/Sub| RTPublisher\n</code></pre> <ol> <li>Forests provide O(1) access to \u201chead\u201d objects + linked-list history.  </li> <li>RTState is a fa\u00e7ade combining execution, exception handling, and forest mutation.  </li> <li>ExecutionInfo is the read-model exported to UIs / tests.  </li> <li>Serialization via <code>RTJSONEncoder</code> guarantees that complex objects (<code>Edge</code>, <code>Vertex</code>, <code>Stamp</code>, Pydantic models, \u2026) round-trip to JSON.</li> </ol>"},{"location":"api_reference/features/state_management/#32-core-design-decisions","title":"3.2 Core Design Decisions","text":"Decision Rationale Trade-offs Immutable linked objects (<code>parent</code> chain) Makes \u201ctime-travel\u201d trivial; no deep copies during reads More memory usage vs. in-place mutation Re-entrant lock per <code>Forest</code> Simple thread-safety without requiring external synchronisation Global write lock per heap limits parallel write throughput Asynchronous task execution (via <code>Coordinator</code>) High throughput, supports external LLM calls Requires careful exception propagation Single custom JSON encoder Centralises (de)serialization rules Needs manual upkeep when adding new types"},{"location":"api_reference/features/state_management/#33-alternatives-considered","title":"3.3 Alternatives Considered","text":"<p>\u2022 Full event-sourcing (append-only log) \u2013 rejected for initial simplicity; could be layered in later. \u2022 Relational DB storage \u2013 overkill for in-memory, single-process execution; would hurt latency.</p>"},{"location":"api_reference/features/state_management/#4-related-files","title":"4. Related Files","text":""},{"location":"api_reference/features/state_management/#41-related-component-files","title":"4.1 Related Component Files","text":"<ul> <li><code>../components/state_management.md</code>: Low-level <code>Forest</code> mechanics and history management.</li> <li><code>../components/node_state_management.md</code>: Node storage &amp; vertex conversion.</li> <li><code>../components/request_management.md</code>: Request graph and status helpers.</li> <li><code>../components/execution_info.md</code>: Snapshot model and graph export.</li> <li><code>../components/rt_state_management.md</code>: Orchestrator for live executions.</li> <li><code>../components/serialization.md</code>: <code>RTJSONEncoder</code> and encoder helper functions.</li> <li><code>../components/state_utilities.md</code>: Sub-state slicing helpers.</li> <li><code>../components/graph_serialization.md</code>: <code>Edge</code> and <code>Vertex</code> data models.</li> </ul>"},{"location":"api_reference/features/state_management/#42-external-dependencies","title":"4.2 External Dependencies","text":"<ul> <li><code>pydantic</code> \u2013 required for serialising <code>BaseModel</code> objects.</li> <li><code>mermaid</code> \u2013 diagrams in docs only; no runtime dependency.</li> </ul>"},{"location":"api_reference/features/state_management/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (2024-05-29) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial extraction from codebase into formal documentation.</li> </ul>"},{"location":"api_reference/features/task_execution/","title":"Task execution","text":""},{"location":"api_reference/features/task_execution/#task-execution","title":"Task Execution","text":"<p>Enables reliable, observable, and pluggable execution of RailTracks nodes via synchronous or asynchronous strategies, while surfacing lifecycle events through the Pub/Sub system.</p> <p>Version: 0.0.1\u2003</p>"},{"location":"api_reference/features/task_execution/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Functional Overview</li> <li>2. External Contracts</li> <li>3. Design and Architecture</li> <li>4. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/features/task_execution/#1-functional-overview","title":"1. Functional Overview","text":"<p>The Task Execution feature is responsible for turning a logical node invocation (e.g. <code>rt.call_sync(MyNode, \"hello\")</code>) into concrete work that is executed, monitored, and reported back to the caller.  It is intentionally decoupled from the node graph and from I/O concerns so that new execution strategies (threads, processes, remote workers, etc.) can be added without changing user-facing code.</p> <p>Key responsibilities:</p> <ol> <li>Create an executable <code>Task</code> object that wraps a node invocation.</li> <li>Select and apply an <code>ExecutionStrategy</code> (<code>asyncio</code>, <code>thread</code>, <code>process</code>, \u2026).</li> <li>Track job metadata (start/end times, status) through <code>CoordinatorState</code>.</li> <li>Emit lifecycle messages (<code>RequestSuccess</code>, <code>RequestFailure</code>, \u2026) on the    Pub/Sub bus for observability and downstream consumers.</li> <li>Offer a simple fa\u00e7ade (<code>Session</code> helpers <code>call_sync</code>, <code>call_async</code>) for    library users.</li> </ol>"},{"location":"api_reference/features/task_execution/#11-executing-a-node-synchronous-helper","title":"1.1 Executing a Node (synchronous helper)","text":"<pre><code>import railtracks as rt\n\n# A session implicitly wires up a Coordinator and Publisher for us.\nwith rt.Session() as run:\n    result = rt.call_sync(rt.nodes.MyNode, \"Hello world!\")\nprint(result)\n</code></pre> <p>Behind the scenes:</p> <pre><code>Session \u2500\u2500\u25ba Coordinator \u2500\u2500\u25ba AsyncioExecutionStrategy \u2500\u2500\u25ba Task.invoke()\n         \u2502               \u2502                           \u2502\n         \u2502               \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Pub/Sub messages \u25c4\u2518\n         \u2514\u2500\u2500 ExecutionInfo / RTState updated\n</code></pre>"},{"location":"api_reference/features/task_execution/#12-manual-coordination","title":"1.2 Manual Coordination","text":"<p>Advanced users can bypass the high-level helpers to exert fine-grained control.</p> <pre><code>from railtracks.execution.coordinator import Coordinator\nfrom railtracks.execution.execution_strategy import AsyncioExecutionStrategy\nfrom railtracks.execution.task import Task\nfrom railtracks.pubsub.publisher import RTPublisher\n\npublisher = RTPublisher()\ncoordinator = Coordinator(execution_modes={\"async\": AsyncioExecutionStrategy()})\ncoordinator.start(publisher)            # attach subscriber\n\ntask = Task(request_id=\"abc123\", node=my_node)\nresponse = await coordinator.submit(task, mode=\"async\")  # returns RequestSuccess/Failure\n</code></pre>"},{"location":"api_reference/features/task_execution/#13-streaming-observability","title":"1.3 Streaming &amp; Observability","text":"<p>Attach a broadcast subscriber to stream real-time events:</p> <pre><code>from railtracks.pubsub import stream_subscriber\n\ndef ui_handler(text: str):\n    print(text)\n\nwith rt.Session(broadcast_callback=ui_handler) as run:\n    rt.call_sync(rt.nodes.ChatNode, \"Hi!\")\n</code></pre> <p>The callback receives pretty-formatted log lines produced from <code>RequestCompletionMessage.log_message()</code>.</p>"},{"location":"api_reference/features/task_execution/#2-external-contracts","title":"2. External Contracts","text":"<p>The feature lives entirely inside the Python package; there are no public HTTP endpoints.  The following contracts are relied upon by other features:</p>"},{"location":"api_reference/features/task_execution/#21-pubsub-topics","title":"2.1 Pub/Sub Topics","text":"Message Class Description Published By <code>railtracks.pubsub.messages.RequestSuccess</code> Node finished successfully. <code>AsyncioExecutionStrategy</code> <code>RequestFailure</code> Node raised an exception. <code>AsyncioExecutionStrategy</code> <code>RequestCreation</code> A new task was created (future extension). (planned) <code>FatalFailure</code> Irrecoverable coordinator failure. Coordinator <p>Consumers: Logging/Observability, UI streaming, Session Management (saves state), and any user-supplied callback registered through <code>stream_subscriber()</code>.</p>"},{"location":"api_reference/features/task_execution/#22-environment-variables-flags","title":"2.2 Environment Variables &amp; Flags","text":"Name Default Purpose <code>RAILTRACKS_EXECUTION_MODE</code> <code>async</code> Experimental override to force a global execution mode (not yet stable)."},{"location":"api_reference/features/task_execution/#3-design-and-architecture","title":"3. Design and Architecture","text":"<p>The Task Execution feature is a thin orchestration layer intentionally kept separate from graph-building concerns so that execution tactics can evolve.</p> <pre><code>graph TD\n  subgraph Feature Boundary\n    Session --&gt;|creates| Coordinator\n    CoordinatorState -.-&gt; Coordinator\n    Coordinator --&gt;|delegates| Strategy[ExecutionStrategy]\n    Strategy --&gt; Task\n    Task --&gt; NodeInvoke[Node.tracked_invoke()]\n  end\n\n  NodeInvoke --&gt;|result| PubSubMessage[RequestSuccess / Failure]\n  PubSubMessage -- broadcast --&gt; RTPublisher\n  RTPublisher -- subscriber --&gt; Coordinator\n  Coordinator --&gt;|\"end_job()\"| CoordinatorState\n</code></pre>"},{"location":"api_reference/features/task_execution/#31-core-abstractions","title":"3.1 Core Abstractions","text":"<p>\u2022 Task \u2013 Immutable wrapper pairing <code>request_id</code> with a <code>Node</code> instance.   Provides <code>invoke()</code> that injects context (<code>update_parent_id</code>) then calls   <code>Node.tracked_invoke()</code>, thereby ensuring audit logging is captured.</p> <p>\u2022 ExecutionStrategy (Strategy Pattern) \u2013 Defines how a task runs.   Current concrete class: <code>AsyncioExecutionStrategy</code>.  Future: <code>Threaded</code>,   <code>Process</code>, distributed workers.</p> <p>\u2022 Coordinator (Command Invoker) \u2013 Receives tasks, selects a strategy, tracks   state, and listens to completion messages so that it can mark jobs closed.</p> <p>\u2022 CoordinatorState / Job \u2013 In-memory, append-only record of job lifecycle   used for diagnostics (<code>Session.info</code>) and persisted on run teardown.</p>"},{"location":"api_reference/features/task_execution/#32-trade-offs-rationale","title":"3.2 Trade-offs &amp; Rationale","text":"Decision Reasoning / Impact Asyncio-first execution Optimal for IO-bound LLM/API calls, simplest to implement; lacks GIL parallelism Strategy Pattern instead of if/else flags Cleaner extension point; avoids Coordinator bloat Pub/Sub for internal events Decouples execution from logging, UI, persistence; minimal runtime overhead No cross-thread execution yet Prevents tricky contextvars migration; revisit after stable single-event loop"},{"location":"api_reference/features/task_execution/#33-rejected-alternatives","title":"3.3 Rejected Alternatives","text":"<ul> <li>Celery / external queue \u2013 too heavy-weight for in-process graphs.</li> <li>Direct Node invocation in Session \u2013 would mix concerns and block future   distributed execution.</li> </ul>"},{"location":"api_reference/features/task_execution/#4-related-files","title":"4. Related Files","text":""},{"location":"api_reference/features/task_execution/#41-related-component-files","title":"4.1 Related Component Files","text":"<ul> <li><code>../components/task_execution.md</code> \u2013 Low-level API reference for Coordinator, Task, and Execution Strategy.</li> <li><code>../components/pubsub_messaging.md</code> \u2013 Defines message classes and publisher utilities used for lifecycle events.</li> <li><code>../components/session_management.md</code> \u2013 Describes how <code>Session</code> bootstraps a publisher and coordinator.</li> </ul>"},{"location":"api_reference/features/task_execution/#42-related-feature-files","title":"4.2 Related Feature Files","text":"<ul> <li><code>../features/state_management.md</code> \u2013 Persists <code>CoordinatorState</code> and <code>ExecutionInfo</code> to disk.</li> <li><code>../features/logging_profiling.md</code> \u2013 Subscribes to Pub/Sub messages for structured logs and performance metrics.</li> </ul>"},{"location":"api_reference/features/task_execution/#43-external-dependencies","title":"4.3 External Dependencies","text":"<ul> <li><code>https://docs.python.org/3/library/asyncio.html</code> \u2013 Standard library used by <code>AsyncioExecutionStrategy</code>.</li> </ul>"},{"location":"api_reference/features/task_execution/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial extraction from monolithic   documentation, added Architectural diagrams and External Contracts section.</li> </ul>"},{"location":"api_reference/features/tool_management/","title":"Tool management","text":""},{"location":"api_reference/features/tool_management/#tool-management","title":"Tool Management","text":"<p>Provides end-to-end \u201ctool calling\u201d support: from automatically turning a Python function or MCP-supplied tool into an internal <code>Tool</code> object, to serialising that object into a LiteLLM-compatible JSON-Schema, to routing the model\u2019s <code>tool_call</code> response back into executable Python code. In short, this feature is the glue that lets an LLM discover, request, and run business-logic functions at runtime.</p> <p>Version: 0.0.1</p>"},{"location":"api_reference/features/tool_management/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Functional Overview </li> <li>2. External Contracts </li> <li>3. Design and Architecture </li> <li>4. Related Files </li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/features/tool_management/#1-functional-overview","title":"1. Functional Overview","text":"<p>From a package user\u2019s perspective, Tool Management offers two main task sets: (1) registering tools, and (2) invoking an LLM with those tools enabled.</p> <pre><code>flowchart LR\n    subgraph Dev\n        A[Write function or import MCP tool]\n        B[Tool.from_function / Tool.from_mcp]\n        C[Tool instance]\n    end\n    subgraph Runtime\n        C --&gt; D[LiteLLMWrapper._chat_with_tools()]\n        D --&gt;|serialised JSON-Schema| E[litellm.completion]\n        E --&gt;|tool_call| F[ToolCall objects]\n        F --&gt; G[execute Python function]\n    end\n</code></pre>"},{"location":"api_reference/features/tool_management/#11-registering-a-python-function-as-a-tool","title":"1.1  Registering a Python Function as a Tool","text":"<pre><code>from railtracks.llm.tools import Tool\n\ndef translate(text: str, target_lang: str = \"fr\") -&gt; str:\n    \"\"\"\n    Translate `text` into the target language.\n\n    Args:\n        text: The text to translate.\n        target_lang (str): Destination language (ISO-639-1 code).\n    \"\"\"\n    # \u2026 implementation \u2026\n    return \"bonjour\"\n\ntranslation_tool = Tool.from_function(translate)\n</code></pre> <p>Behind the scenes:</p> <ol> <li><code>Tool.from_function()</code> introspects the function\u2019s type annotations.  </li> <li><code>railtracks.llm.tools.docstring_parser.parse_docstring_args()</code> harvests    per-parameter descriptions from the docstring.  </li> <li>Each parameter is converted into a concrete <code>Parameter</code> subclass via a chain    of <code>ParameterHandler</code>s (<code>PydanticModelHandler</code>, <code>SequenceParameterHandler</code>,    etc.).</li> </ol>"},{"location":"api_reference/features/tool_management/#12-exposing-tools-to-an-llm-and-handling-calls","title":"1.2  Exposing Tools to an LLM and Handling Calls","text":"<pre><code>from railtracks.llm.models import LiteLLMWrapper\nfrom railtracks.llm.message import UserMessage, MessageHistory\n\nmodel = LiteLLMWrapper(\"openai/gpt-4o-mini\")\n\nmessages = MessageHistory(\n    UserMessage(content=\"Translate 'hello' into French and Spanish.\")\n)\n\nresponse = model.chat_with_tools(messages, tools=[translation_tool])\n\nmatch response.message.content:\n    case list(tool_calls):            # LLM requested one or more tools\n        for call in tool_calls:       # call: railtracks.llm.content.ToolCall\n            if call.name == translation_tool.name:\n                result = translate(**call.arguments)\n                # \u2026 emit ToolMessage(result) back to the model \u2026\n    case _:\n        print(response.message.content)  # Plain assistant text\n</code></pre>"},{"location":"api_reference/features/tool_management/#13-importing-an-existing-mcp-tool","title":"1.3  Importing an Existing MCP Tool","text":"<pre><code>from railtracks.llm.tools import Tool\nfrom railtracks.rt_mcp import get_example_mcp_tool  # hypothetical helper\n\nmcp_tool_handle = get_example_mcp_tool()\ntool = Tool.from_mcp(mcp_tool_handle)\n</code></pre> <p><code>Tool.from_mcp()</code> validates the MCP input schema (<code>type == \"object\"</code>) and recursively converts it into internal <code>Parameter</code> objects via <code>schema_parser.parse_json_schema_to_parameter()</code>.</p>"},{"location":"api_reference/features/tool_management/#2-external-contracts","title":"2. External Contracts","text":"<p>Although Tool Management owns no HTTP endpoints, it defines binary contracts between otherwise-decoupled subsystems.</p>"},{"location":"api_reference/features/tool_management/#21-json-schema-sent-to-litellm","title":"2.1 JSON-Schema Sent to litellm","text":"<p><code>railtracks.llm.models._litellm_wrapper._to_litellm_tool()</code> emits the exact schema accepted by litellm.completion.</p> <p>Key guarantees:</p> Field Format / Source <code>function.name</code> <code>Tool.name</code> \u2013 must be lower_snake_case. <code>function.description</code> <code>Tool.detail</code> \u2013 first paragraph of docstring. <code>function.parameters</code> Draft-07 JSON-Schema produced from <code>Parameter</code> <p>Breaking this schema will break all downstream calls.</p>"},{"location":"api_reference/features/tool_management/#22-environment-variables-flags","title":"2.2 Environment Variables &amp; Flags","text":"<p>None at the moment. (The surrounding LLM integration may rely on <code>OPENAI_API_KEY</code>, but that is tracked in <code>features/llm_integration.md</code>.)</p>"},{"location":"api_reference/features/tool_management/#3-design-and-architecture","title":"3. Design and Architecture","text":""},{"location":"api_reference/features/tool_management/#31-core-data-model","title":"3.1  Core Data Model","text":"Concept Responsibility <code>Tool</code> Quasi-immutable value object. Holds <code>name</code>, <code>detail</code>, and a <code>Set[Parameter]</code> or raw JSON <code>Parameter</code> family Declarative type system (<code>str</code>, <code>int</code>, union, arrays, nested objects/<code>Pydantic</code>) <code>ParameterHandler</code> Strategy pattern that converts a Python annotation into a concrete <code>Parameter</code> <code>ToolCall</code> Runtime request from model (<code>id</code>, <code>name</code>, <code>arguments</code>) <p>Key principles:</p> <ol> <li>Source-of-truth invariants \u2013 the original Python function signature is    not mutated; <code>Tool</code> objects are created, never modified.</li> <li>Graceful degradation \u2013 when unable to parse a type (e.g., builtin C    function without signature), <code>ToolCreationError</code> is raised with    actionable \u201ctips to debug\u201d.</li> <li>Zero implementation coupling \u2013 <code>Tool</code> knows nothing about LiteLLM;    conversion happens in <code>_litellm_wrapper.py</code> to avoid back-references.</li> </ol>"},{"location":"api_reference/features/tool_management/#32-parameter-inference-pipeline","title":"3.2  Parameter-Inference Pipeline","text":"<pre><code>graph TD\n    A[inspect.signature] --&gt; B{annotation type?}\n    B --&gt;|Pydantic model| C[PydanticModelHandler]\n    B --&gt;|Sequence / list| D[SequenceParameterHandler]\n    B --&gt;|Union| E[UnionParameterHandler]\n    B --&gt;|Fallback| F[DefaultParameterHandler]\n    C &amp; D &amp; E &amp; F --&gt; G[Parameter instance]\n    G --&gt; H{array wrapper?}\n    H --&gt;|yes| I[ArrayParameter]\n</code></pre> <p>Trade-offs:</p> <p>\u2022 Performance vs flexibility \u2013 we do the heavy regex/docstring parsing once   at startup, trading some memory for runtime speed. \u2022 Static vs dynamic typing \u2013 Pydantic models are accepted verbatim to avoid   duplicating validation logic.</p>"},{"location":"api_reference/features/tool_management/#33-error-handling-strategy","title":"3.3  Error-Handling Strategy","text":"<p><code>ToolCreationError</code> derives from <code>RTLLMError</code> and attaches a <code>notes</code> list printed in green to standard error. This avoids noisy stack traces while still surfacing actionable guidance.</p> <p>Rejected alternative: returning <code>None</code> silently \u2013 deemed too brittle.</p>"},{"location":"api_reference/features/tool_management/#34-execution-flow-at-runtime","title":"3.4  Execution Flow at Runtime","text":"<ol> <li>Developer registers tool(s).  </li> <li>Wrapper converts tools to LiteLLM JSON schema and calls LLM.  </li> <li>LLM returns <code>tool_call</code>(s) \u2192 marshalled into <code>ToolCall</code> dataclass.  </li> <li>Caller inspects each <code>ToolCall</code> and dispatches to the actual Python    function (often via a <code>ToolCallable</code> node helper).  </li> <li>Optionally, the result is sent back to the model in a <code>ToolMessage</code>,    enabling multi-step tool chains.</li> </ol> <p>Concurrency: The <code>Tool</code> object is read-only after creation and therefore thread-safe. The caller must still ensure the wrapped function is safe to execute in the chosen concurrency model (async/thread/process).</p>"},{"location":"api_reference/features/tool_management/#4-related-files","title":"4. Related Files","text":""},{"location":"api_reference/features/tool_management/#41-related-component-files","title":"4.1 Related Component Files","text":"<ul> <li><code>components/tool_management.md</code> \u2013 houses   lower-level implementation details and Public API stubs.  </li> <li><code>components/tool_parsing.md</code> \u2013 deep-dives   into docstring/JSON-Schema parsing logic used by this feature.</li> </ul>"},{"location":"api_reference/features/tool_management/#42-key-code-files-linked-indirectly-through-components","title":"4.2 Key Code Files (linked indirectly through components)","text":"<ul> <li><code>railtracks/llm/tools/tool.py</code> \u2013 <code>Tool</code>, <code>ToolCreationError</code>, <code>from_function</code>,   <code>from_mcp</code>.  </li> <li><code>railtracks/llm/tools/docstring_parser.py</code> \u2013 regex utilities.  </li> <li><code>railtracks/llm/tools/parameter_handlers.py</code> \u2013 strategy implementations.  </li> <li><code>railtracks/llm/models/_litellm_wrapper.py</code> \u2013 <code>_to_litellm_tool</code>,   <code>_chat_with_tools</code>, <code>ToolCall</code> deserialisation.</li> </ul>"},{"location":"api_reference/features/tool_management/#43-external-dependencies","title":"4.3 External Dependencies","text":"<ul> <li><code>litellm</code> \u2013 upstream library whose   schema we target when sending tools to an LLM.  </li> <li><code>pydantic</code> \u2013 leveraged for complex, nested   parameter validation (see <code>PydanticModelHandler</code>).</li> </ul>"},{"location":"api_reference/features/tool_management/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version \u2013 first complete   end-to-end documentation of the Tool Management feature.</li> </ul>"},{"location":"api_reference/features/validation/","title":"Validation","text":""},{"location":"api_reference/features/validation/#validation","title":"Validation","text":"<p>Guarantees the structural integrity, safety, and correctness of nodes and their interactions by validating code and runtime inputs before execution.</p> <p>Version: 0.0.1\u2003</p>"},{"location":"api_reference/features/validation/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Functional Overview</li> <li>2. External Contracts</li> <li>3. Design and Architecture</li> <li>4. Related Files</li> <li>CHANGELOG</li> </ul>"},{"location":"api_reference/features/validation/#1-functional-overview","title":"1. Functional Overview","text":"<p>The Validation feature is the project\u2019s first line of defence against mis-configuration and malformed data. It is split into two complementary task sets:</p> <ol> <li>Node Creation Validation \u2013 runs at build time (graph construction, tool registration, etc.).</li> <li>Node Invocation Validation \u2013 runs at execution time (every node call).</li> </ol> <p>Together they prevent subtle bugs from propagating into live executions, surface actionable error messages, and shield large language models (LLMs) from invalid prompts or unsafe schemas.</p>"},{"location":"api_reference/features/validation/#11-node-creation-validation","title":"1.1 Node Creation Validation","text":"<p>Covers static and semi-static checks that can be performed before a node is ever invoked.</p> <p>Key responsibilities \u2022 Ensure a node\u2019s public function does not receive raw <code>dict</code> / mutable mapping types \u2022 Validate that required <code>@classmethod</code> APIs are implemented correctly \u2022 Verify tool metadata (params, details, pretty name, <code>SystemMessage</code>, max tool calls, \u2026) \u2022 Confirm connected nodes belong to the declared <code>Node</code> hierarchy \u2022 Guard against malformed JSON Schema definitions produced by <code>pydantic</code> models</p> <pre><code>from railtracks.validation.node_creation.validation import (\n    validate_function,\n    validate_tool_metadata,\n)\n\n# 1\ufe0f\u20e3  Function signature validation\ndef unsafe_fn(a: int, b: dict):  # \u274c mutable dict\n    ...\n\nvalidate_function(unsafe_fn)  # \u279c raises NodeCreationError at build time\n\n\n# 2\ufe0f\u20e3  Tool metadata validation\ntool_params  = [{'name': 'query'}, {'name': 'source'}]\ntool_details = {'description': 'RAG search'}\nsystem_msg   = SystemMessage(content=\"You are a helpful assistant.\")\n\nvalidate_tool_metadata(\n    tool_params     = tool_params,\n    tool_details    = tool_details,\n    system_message  = system_msg,\n    pretty_name     = \"Search Tool\",\n    max_tool_calls  = 3,\n)\n</code></pre> <p>For deep-dive information see <code>components/node_creation_validation.md</code>.</p>"},{"location":"api_reference/features/validation/#12-node-invocation-validation","title":"1.2 Node Invocation Validation","text":"<p>Covers dynamic checks that can only be evaluated when the node is about to run.</p> <p>Key responsibilities \u2022 Message history integrity (instances of <code>llm.Message</code>, presence of system message, etc.) \u2022 Availability of an <code>llm.ModelBase</code> instance \u2022 Enforcing (or warning about) <code>max_tool_calls</code></p> <pre><code>from railtracks.validation.node_invocation.validation import (\n    check_message_history,\n    check_llm_model,\n    check_max_tool_calls,\n)\nfrom railtracks.llm import Message, MessageHistory, ModelBase\n\nhistory = MessageHistory([\n    Message(role=\"user\", content=\"Hi!\"),\n])\n\ncheck_message_history(history, system_message=\"You are ChatGPT\")  # \u2705 safe\n\nllm_model = None\ncheck_llm_model(llm_model)  # \u279c raises NodeInvocationError\n\ncheck_max_tool_calls(0)     # \u2705 allowed (no tool calls)\n</code></pre> <p>For deep-dive information see <code>components/node_invocation_validation.md</code>.</p>"},{"location":"api_reference/features/validation/#2-external-contracts","title":"2. External Contracts","text":"<p>Validation operates purely in-process; it does not expose HTTP / gRPC endpoints or CLIs. However, consumers must understand the following contracts because violating them triggers exceptions.</p>"},{"location":"api_reference/features/validation/#21-exceptions-raised","title":"2.1 Exceptions Raised","text":"Exception Class Typical Triggers Fatal? <code>railtracks.exceptions.errors.NodeCreationError</code> Invalid function sig, duplicate param names, classmethod missing, \u2026 Yes <code>railtracks.exceptions.errors.NodeInvocationError</code> Missing model, wrong message type, negative <code>max_tool_calls</code>, \u2026 Yes/No (see <code>fatal</code> flag) <p>Both exception classes surface internationalised messages and remediation notes retrieved from <code>railtracks.exceptions.messages.exception_messages</code>.</p>"},{"location":"api_reference/features/validation/#22-warnings-emitted","title":"2.2 Warnings Emitted","text":"<p>Some non-fatal conditions emit a <code>warnings.warn</code> instead (e.g., unlimited tool calls). These can be upgraded to errors by enabling Python\u2019s warning filters:</p> <pre><code>python -W \"error::UserWarning\" your_app.py\n</code></pre>"},{"location":"api_reference/features/validation/#3-design-and-architecture","title":"3. Design and Architecture","text":"<p>Validation is intentionally lightweight; its only dependency is <code>pydantic</code> for model inspection. Nevertheless, a few architectural decisions are critical for maintainability.</p>"},{"location":"api_reference/features/validation/#31-two-tier-validation-layer","title":"3.1 Two-Tier Validation Layer","text":"<pre><code>flowchart LR\n    subgraph Build-Time\n        direction TB\n        VC[Node CreationValidation]\n    end\n    subgraph Run-Time\n        direction TB\n        VI[Node InvocationValidation]\n    end\n    VC -- upon node.build() --&gt; VI\n</code></pre> <ol> <li> <p>Node Creation Validation (VC)    \u2022 Runs once per node definition.    \u2022 Throws hard errors because configuration mistakes should fail fast and fail early.</p> </li> <li> <p>Node Invocation Validation (VI)    \u2022 Executes every time the node is called.    \u2022 May raise fatal errors or warnings, depending on whether the problem is recoverable.</p> </li> </ol> <p>This separation avoids redundant work (static checks do not run repeatedly) while still protecting runtime interactions.</p>"},{"location":"api_reference/features/validation/#32-exception-message-catalogue","title":"3.2 Exception &amp; Message Catalogue","text":"<p>All validation functions never hard-code strings; instead they call:</p> <pre><code>from railtracks.exceptions.messages.exception_messages import (\n    get_message, get_notes, ExceptionMessageKey\n)\n</code></pre> <p>Advantages \u2022 Centralised message editing / localisation \u2022 Consistent wording across the entire project \u2022 Easy unit-test assertions by key</p>"},{"location":"api_reference/features/validation/#33-extensibility","title":"3.3 Extensibility","text":"<p>\u2022 Add new checks simply by appending helper functions that raise <code>NodeCreationError</code> or <code>NodeInvocationError</code>. \u2022 Disable individual warnings via <code>warnings.filterwarnings</code>. \u2022 Override behaviour (e.g., auto-correct rather than fail) by wrapping validation calls in custom orchestration code.</p>"},{"location":"api_reference/features/validation/#34-rejected-alternatives","title":"3.4 Rejected Alternatives","text":"Option Reason Rejected Using <code>typing.Annotated</code> metadata to implicitly encode validation rules Too implicit; developers failed to discover mistakes until runtime. Central, monolithic <code>validate()</code> function Violated Single Responsibility, hard to unit-test individual concerns."},{"location":"api_reference/features/validation/#4-related-files","title":"4. Related Files","text":""},{"location":"api_reference/features/validation/#41-related-component-files","title":"4.1 Related Component Files","text":"<ul> <li><code>../components/node_creation_validation.md</code>: Static checks executed during node construction.</li> <li><code>../components/node_invocation_validation.md</code>: Dynamic checks executed during node calls.</li> </ul>"},{"location":"api_reference/features/validation/#42-related-feature-files","title":"4.2 Related Feature Files","text":"<p>(None yet)</p>"},{"location":"api_reference/features/validation/#43-external-dependencies","title":"4.3 External Dependencies","text":"<ul> <li><code>pydantic</code>: Used for recursive field inspection of output models.</li> </ul>"},{"location":"api_reference/features/validation/#changelog","title":"CHANGELOG","text":"<ul> <li>v0.0.1 (YYYY-MM-DD) [<code>&lt;COMMIT_HASH&gt;</code>]: Initial version.</li> </ul>"},{"location":"llm_support/config/","title":"\u2699\ufe0f Configuration","text":"<p>Railtracks provides flexible configuration options to customize the behavior of your agent executions. You can control timeouts, logging, error handling, and more through a simple configuration system.</p>"},{"location":"llm_support/config/#configuration-methods","title":"\ud83d\udd27 Configuration Methods","text":"<p>Configuration parameters follow a specific precedence order, allowing you to override settings at different levels:</p> <ol> <li>Session Constructor Parameters - Highest priority</li> <li>Global Configuration (<code>rt.set_config()</code>) - Medium priority  </li> <li>Default Values - Lowest priority</li> </ol>"},{"location":"llm_support/config/#available-configuration-parameters","title":"\ud83d\udccb Available Configuration Parameters","text":""},{"location":"llm_support/config/#core-execution-settings","title":"Core Execution Settings","text":"<ul> <li><code>timeout</code> (<code>float</code>): Maximum seconds to wait for a response to your top-level request</li> <li><code>end_on_error</code> (<code>bool</code>): Stop execution when an exception is encountered</li> <li><code>run_identifier</code> (<code>str | None</code>): Unique identifier for the run (random if not provided)</li> </ul>"},{"location":"llm_support/config/#logging-configuration","title":"Logging Configuration","text":"<ul> <li><code>logging_setting</code> (allowable_log_levels | None): Level of logging detail.  Here are the <code>allowable_log_levels</code> options:<ul> <li><code>VERBOSE</code></li> <li><code>REGULAR</code></li> <li><code>QUIET</code></li> <li><code>NONE</code></li> </ul> </li> <li><code>log_file</code> (<code>str | os.PathLike | None</code>): File path for log output (None = no file logging)</li> </ul>"},{"location":"llm_support/config/#advanced-settings","title":"Advanced Settings","text":"<ul> <li><code>context</code> (<code>Dict[str, Any]</code>): Global context variables for execution</li> <li><code>broadcast_callback</code> (<code>Callable</code>): Callback function for broadcast messages</li> <li><code>prompt_injection</code> (<code>bool</code>): Automatically inject prompts from context variables</li> <li><code>save_state</code> (<code>bool</code>): Save execution state to <code>.railtracks</code> directory</li> </ul>"},{"location":"llm_support/config/#default-values","title":"\ud83c\udfaf Default Values","text":"<pre><code># Default configuration values\ntimeout = 150.0                   # seconds\nend_on_error = False              # continue on errors\nlogging_setting = \"REGULAR\"       # standard logging level\nlog_file = None                   # no file logging\nbroadcast_callback = None         # no broadcast callback\nrun_identifier = None             # random identifier generated\nprompt_injection = True           # enable prompt injection\nsave_state = True                 # save execution state\n</code></pre>"},{"location":"llm_support/config/#method-1-session-constructor","title":"\ud83d\udee0\ufe0f Method 1: Session Constructor","text":"<p>Configure settings when creating a session for your agent execution:</p> <pre><code>import railtracks as rt\n\n# Configure for a partiular session execution\nwith rt.session(\n    timeout=300.0,\n    end_on_error=True,\n    logging_setting=\"DEBUG\",\n    log_file=\"execution.log\",\n    run_identifier=\"my-custom-run-001\",\n    prompt_injection=False,\n    save_state=False,\n    context={\"user_name\": \"Alice\", \"environment\": \"production\"}\n    ):\n    response = rt.call_sync(\n        my_agent,\n        \"Hello world!\",\n    )\n</code></pre>"},{"location":"llm_support/config/#method-2-global-configuration","title":"\ud83c\udf10 Method 2: Global Configuration","text":"<p>Set configuration globally using <code>rt.set_config()</code>. This must be called before any <code>rt.call()</code> or <code>rt.call_sync()</code> operations:</p> <pre><code>import railtracks as rt\n\n# Set global configuration\nrt.set_config(\n    timeout=200.0,\n    logging_setting=\"DEBUG\",\n    log_file=\"app_logs.log\",\n    end_on_error=True,\n    context={\"app_version\": \"1.2.3\"}\n)\n\n# Now all subsequent calls will use these settings\nresponse1 = rt.call_sync(agent1, \"First request\")\nresponse2 = rt.call_sync(agent2, \"Second request\")\n</code></pre>"},{"location":"llm_support/config/#configuration-precedence","title":"\ud83c\udf9a\ufe0f Configuration Precedence","text":"<p>When the same parameter is set in multiple places, Railtracks uses this priority order:</p> <pre><code>import railtracks as rt\n\n# 1. Set global config (medium priority)\nrt.set_config(timeout=100.0, logging_setting=\"REGULAR\")\n\n# 2. Session overrides global config (highest priority)\nwith rt.session(\n    timeout=300.0,        # This overrides the global timeout=100.0\n    end_on_error=True     # This uses session-level setting\n    # logging_setting not specified, so uses global \"REGULAR\"\n):\n    response = rt.call_sync(\n        my_agent,\n        \"Hello!\",\n    )\n\n# Final effective configuration:\n# - timeout: 300.0 (from session constructor)\n# - end_on_error: True (from session constructor)  \n# - logging_setting: \"REGULAR\" (from global config)\n# - All other parameters use default values\n</code></pre>"},{"location":"llm_support/config/#best-practices","title":"\ud83d\udca1 Best Practices","text":""},{"location":"llm_support/config/#development-vs-production","title":"\ud83c\udfd7\ufe0f Development vs Production","text":"<pre><code>import railtracks as rt\nimport os\n\n# Configure based on environment\nif os.getenv(\"ENVIRONMENT\") == \"production\":\n    rt.set_config(\n        timeout=300.0,\n        logging_setting=\"REGULAR\",\n        log_file=\"production.log\",\n        end_on_error=False,\n        save_state=True\n    )\nelse:\n    rt.set_config(\n        timeout=60.0,\n        logging_setting=\"DEBUG\", \n        log_file=\"debug.log\",\n        end_on_error=True,\n        save_state=False\n    )\n</code></pre>"},{"location":"llm_support/config/#debugging-configuration","title":"\ud83d\udd0d Debugging Configuration","text":"<pre><code>import railtracks as rt\n\n# Enhanced debugging setup\nrt.set_config(\n    logging_setting=\"DEBUG\",\n    log_file=\"debug_session.log\",\n    end_on_error=True,           # Stop on first error\n    save_state=True,             # Save state for inspection\n    run_identifier=\"debug-001\"   # Easy identification\n)\n\ndef debug_callback(message: str):\n    print(f\"\ud83d\udd0d Broadcast: {message}\")\n\nwith rt.session(\n    broadcast_callback=debug_callback,\n):\n    response = rt.call_sync(\n        my_agent,\n        \"Debug this workflow\",\n    )\n</code></pre>"},{"location":"llm_support/config/#important-notes","title":"\ud83d\udea8 Important Notes","text":"<ul> <li><code>rt.set_config()</code> must be called before any agent execution</li> <li>Session constructor parameters always take highest precedence</li> <li>Configuration is global and affects all subsequent agent calls</li> <li>Default values are used for any unspecified parameters</li> </ul>"},{"location":"llm_support/prompts/","title":"Prompts and Context Injection","text":"<p>Prompts are a fundamental part of working with LLMs in the RailTracks framework. This guide explains how to create dynamic prompts that use our context injection feature to make your prompts more flexible and powerful.</p>"},{"location":"llm_support/prompts/#understanding-prompts-in-railtracks","title":"Understanding Prompts in RailTracks","text":"<p>In RailTracks, prompts are provided as system messages or user messages when interacting with LLMs. These messages guide the LLM's behavior and responses. For example:</p> <pre><code>import railtracks as rt\nfrom railtracks.llm import OpenAILLM\n\nencoder_agent = rt.library.terminal_llm(\n    name=\"Encoder\",\n    system_message=\"You are an encoder that converts text to base64 encoding.\",\n    llm_model=OpenAILLM(\"gpt-4o\"),\n)\n</code></pre>"},{"location":"llm_support/prompts/#context-injection","title":"Context Injection","text":"<p>RailTracks provides a powerful feature called \"context injection\" (also referred to as \"prompt injection\") that allows you to dynamically insert values from the global context into your prompts. This makes your prompts more flexible and reusable across different scenarios.</p>"},{"location":"llm_support/prompts/#how-context-injection-works","title":"How Context Injection Works","text":"<ol> <li>Define placeholders in your prompts using curly braces: <code>{variable_name}</code></li> <li>Set values in the RailTracks context (see Context Management for details)</li> <li>When the prompt is processed, the placeholders are replaced with the corresponding values from the context</li> </ol>"},{"location":"llm_support/prompts/#example","title":"Example","text":"<pre><code>import railtracks as rt\nfrom railtracks.llm import OpenAILLM, MessageHistory, UserMessage\n\n# Define a prompt with placeholders\nsystem_message = \"You are a {role} assistant specialized in {domain}.\"\n\n# Create an LLM node with this prompt\nassistant = rt.library.terminal_llm(\n    name=\"Assistant\",\n    system_message=system_message,\n    llm_model=OpenAILLM(\"gpt-4o\"),\n)\n\n# Run with context values\nwith rt.Session(context={\"role\": \"technical\", \"domain\": \"Python programming\"}):\n    response = rt.call_sync(assistant, user_input=\"Help me understand decorators.\")\n</code></pre> <p>In this example, the system message will be expanded to: \"You are a technical assistant specialized in Python programming.\"</p>"},{"location":"llm_support/prompts/#enabling-and-disabling-context-injection","title":"Enabling and Disabling Context Injection","text":"<p>Context injection is enabled by default but can be disabled if needed:</p> <pre><code>import railtracks as rt\nfrom railtracks.llm import MessageHistory, UserMessage\n\n# Create a node with a prompt containing placeholders\nmy_node = rt.agent_node(\n    name=\"Example\",\n    system_message=\"You are a {variable} assistant.\",\n    llm_model=rt.llm.OpenAILLM(\"gpt-4o\"),\n)\n\n# Disable context injection for a specific run\nwith rt.Session(\n        context={\"variable\": \"value\"},\n        prompt_injection=False\n):\n    # Context injection will not occur in this run\n    user_message = MessageHistory([UserMessage(\"Hello\")])\n    response = rt.call_sync(my_node, user_input=user_message)\n</code></pre> <p>This may be useful when formatting prompts that should not change based on the context.</p>"},{"location":"llm_support/prompts/#escaping-placeholders","title":"Escaping Placeholders","text":"<p>If you need to include literal curly braces in your prompt without triggering context injection, you can escape them by doubling the braces:</p> <pre><code># This will not be replaced with a context value\nsystem_message = \"Use the {{variable}} placeholder in your code.\"\n</code></pre>"},{"location":"llm_support/prompts/#debugging-prompts","title":"Debugging Prompts","text":"<p>If your prompts aren't producing the expected results:</p> <ol> <li>Check context values: Ensure the context contains the expected values for your placeholders</li> <li>Verify prompt injection is enabled: Check that <code>prompt_injection=True</code> in your <code>ExecutorConfig</code></li> <li>Look for syntax errors: Ensure your placeholders use the correct format <code>{variable_name}</code></li> </ol>"},{"location":"llm_support/prompts/#advanced-usage","title":"Advanced Usage","text":""},{"location":"llm_support/prompts/#message-level-control","title":"Message-Level Control","text":"<p>Context injection can be controlled at the message level using the <code>inject_prompt</code> parameter:</p> <pre><code>from railtracks.llm import Message\n\n# This message will have context injection applied\nsystem_msg = Message(role=\"system\", content=\"You are a {role}.\", inject_prompt=True)\n\n# This message will not have context injection applied\nuser_msg = Message(role=\"user\", content=\"Tell me about {topic}.\", inject_prompt=False)\n</code></pre> <p>This can be useful when you want to control which messages should have context injected and which should not.  As an example, in a Math Assistant, you might want to inject context into the system message, but not the user message that may contain latex that has <code>{}</code> characters. To prevent formatting issues, you can set <code>inject_prompt=False</code> for the user message.</p>"},{"location":"llm_support/providers/","title":"\ud83c\udf10 Supported Providers","text":"<p>We currently support connecting to different available LLMs through the following providers:</p> <ul> <li>OpenAI - GPT models</li> <li>Anthropic - Claude models</li> <li>Gemini - Google's Gemini models</li> <li>Azure AI Foundry - Azure-hosted models</li> <li>Ollama - Local and self-hosted models</li> </ul> <p>This allows you to use the same codebase to interact with different LLMs, making it easy to switch providers or use multiple providers in parallel, completely abstracting the underlying API differences.</p>"},{"location":"llm_support/providers/#quick-start-examples","title":"\ud83d\udccb Quick Start Examples","text":"<p>Take a look at the examples below to see how using different providers look for achieving the same task.</p> OpenAIAnthropicGeminiAzure AI FoundryOllama <pre><code>import railtracks as rt\n\nGeneralAgent = rt.agent_node(\n    llm_model=rt.llm.OpenAILLM(\"gpt-4o\"),\n    system_message=\"You are a general-purpose AI assistant.\"\n)\n</code></pre> <pre><code>import railtracks as rt\n\nGeneralAgent = rt.agent_node(\n    llm_model=rt.llm.AnthropicLLM(\"claude-sonnet-4\"),\n    system_message=\"You are a general-purpose AI assistant.\"\n)\n</code></pre> <pre><code>import railtracks as rt\n\nGeneralAgent = rt.agent_node(\n    llm_model=rt.llm.GeminiLLM(\"gemini-2.5-flash\"),\n    system_message=\"You are a general-purpose AI assistant.\"\n)\n</code></pre> <pre><code>import railtracks as rt\n\nGeneralAgent = rt.agent_node(\n    llm_model=rt.llm.AzureAILLM(\"azure_ai/deepseek-r1\"),\n    system_message=\"You are a general-purpose AI assistant.\"\n)\n</code></pre> <pre><code>import railtracks as rt\n\nGeneralAgent = rt.agent_node(\n    llm_model=rt.llm.OllamaLLM(\"deepseek-r1:8b\"),\n    system_message=\"You are a general-purpose AI assistant.\"\n)\n</code></pre> <p>API Keys</p> <p>Make sure you set the appropriate environment variable keys for your specific provider. By default, RailTracks uses the <code>dotenv</code> framework to load environment variables from a <code>.env</code> file. Please refer to the API Reference for each provider to see the required environment variables.</p> <p>Tool Calling Capabilities</p> <p>If you want to use tool calling capabilities by passing the <code>tool_nodes</code> parameter to the <code>agent_node</code>, you can do so with any of the above providers. However, you need to ensure that the provider and the specific LLM model you are using support tool calling.</p>"},{"location":"llm_support/providers/#writing-custom-llm-providers","title":"\ud83d\udd27 Writing Custom LLM Providers","text":"<p>We hope to cover most of the common and widely used LLM providers, but if you need to use a provider that is not currently supported, you can implement your own LLM provider by subclassing <code>LLMProvider</code> and implementing the required methods. </p> <p>For our implementation, we have benefited from the amazing LiteLLM \ud83d\ude80 framework, which provides excellent multi-provider support.</p> <p>Custom Provider Documentation</p> <p>Please refer to the Custom LLM Provider documentation for detailed instructions on how to implement your own provider.</p>"},{"location":"observability/chat/","title":"Local Chat","text":""},{"location":"observability/chat/#overview","title":"Overview","text":"<p>Conversational interactions with agents is currently one of the most common methods. This guide will go over how to quickly visualize and come up with a proof of concept assessing how the potential user experience would look using <code>RailTracks</code>.</p> <pre><code>import random\nimport datetime\n\nimport railtracks as rt\n\ndef get_todays_date(tell_time: bool, tell_location: bool):\n    \"\"\"\n    Returns the correct date once called. Time can also be provided.\n\n    Args:\n        tell_time (bool): if set to True will also return time\n    \"\"\"\n\n    if tell_location:\n        raise ValueError(\"Location is not supported in this example.\")\n\n    if tell_time:\n        if random.random() &lt; 0.8:\n            raise RuntimeError(\"Random exception occurred!\")\n        return str(datetime.datetime.now())\n    else:\n        return str(datetime.date.today())\n\nINSTRUCTION =\"\"\"\nYou are a helpful agent that can analyze images and answer questions about them.\n\"\"\"\n\nChatBot = rt.chatui_node(\n    port=5000,\n    auto_open=True,\n    llm_model=rt.llm.OpenAILLM(\"gpt-4o\"),\n    pretty_name=\"ChatBot\",\n    system_message=rt.llm.SystemMessage(\n        INSTRUCTION\n    ),\n    tool_nodes={\n        get_todays_date,\n    },\n)\n\nwith rt.Session(timeout=600) as session:\n    resp = rt.call_sync(\n        ChatBot,\n        rt.llm.MessageHistory(),\n    )\n</code></pre>"},{"location":"observability/chat/#walkthrough","title":"Walkthrough","text":"<p>The above code defines a simple tool that provides the agent with either date or datetime together and randomly throws a bug. This is done on purpose to demonstrate the tool view and illustration of different invocation outcomes.</p> <p>By using the INSERT NAME here, you can easily do this. Upon running the above code, the following window will open: </p> <p>Here you can see the conversation history with the agent markdown formatted. Additionally, you can also use the <code>Invoked Tools</code> tab to visualize the input/output of the tools the agent has used. </p> <p>While this is meant to be used locally, the styling of the UI can be adjusted by changing the files <code>chat.css</code> and <code>chat.html</code>.</p>"},{"location":"observability/error_handling/","title":"\ud83d\udea8 Error Handling","text":"<p>RailTracks (RT) provides a comprehensive error handling system designed to give developers clear, actionable feedback when things go wrong. The framework uses a hierarchy of specialized exceptions that help you understand exactly what went wrong and where.</p>"},{"location":"observability/error_handling/#error-hierarchy","title":"\ud83c\udfd7\ufe0f Error Hierarchy","text":"<p>All RailTracks errors inherit from the base <code>RTError</code> class, which provides colored console output and structured error reporting.</p> <pre><code>RTError (base)\n\u251c\u2500\u2500 NodeCreationError\n\u251c\u2500\u2500 NodeInvocationError\n\u251c\u2500\u2500 LLMError\n\u251c\u2500\u2500 GlobalTimeOutError\n\u251c\u2500\u2500 ContextError\n\u2514\u2500\u2500 FatalError\n</code></pre>"},{"location":"observability/error_handling/#error-types","title":"\ud83c\udfaf Error Types","text":""},{"location":"observability/error_handling/#internally-raised-errors","title":"\ud83d\udd27 Internally Raised Errors","text":"<p>These errors are automatically raised by RailTracks when issues occur during execution. All inherit from <code>RTError</code> and provide colored terminal output with debugging information.</p> <ul> <li><code>NodeCreationError</code> \u2699\ufe0f - Raised during node setup and validation</li> <li><code>NodeInvocationError</code> \u26a1 - Raised during node execution (has <code>fatal</code> flag)</li> <li><code>LLMError</code> \ud83e\udd16 - Raised during LLM operations (includes <code>message_history</code>)</li> <li><code>GlobalTimeOutError</code> \u23f0 - Raised when execution exceeds timeout</li> <li><code>ContextError</code> \ud83c\udf10 - Raised for context-related issues</li> </ul> <p>All internal errors include helpful debugging notes and formatted error messages to guide troubleshooting.</p>"},{"location":"observability/error_handling/#user-raised-errors","title":"\u26a0\ufe0f User-Raised Errors","text":"<p><code>FatalError</code> \ud83d\udc80 - The only error type designed for developers to raise manually when encountering unrecoverable situations.</p> <p>Usage: <pre><code>from rt.exceptions import FatalError\n\ndef my_critical_function():\n    if critical_condition_failed:\n        raise FatalError(\"Critical system state compromised\")\n</code></pre></p>"},{"location":"observability/error_handling/#error-handling-patterns","title":"\ud83d\udd04 Error Handling Patterns","text":""},{"location":"observability/error_handling/#basic-error-handling","title":"\ud83d\udd28 Basic Error Handling","text":"<pre><code>import railtracks as rt\nfrom rt.exceptions import NodeInvocationError, LLMError\n\ntry:\n    result = await rt.call(node, user_input=\"Tell me about machine learning\")\nexcept NodeInvocationError as e:\n    if e.fatal:\n        # Fatal errors should stop execution\n        logger.error(f\"Fatal node error: {e}\")\n        raise\n    else:\n        # Non-fatal errors can be handled gracefully\n        logger.warning(f\"Node error (recoverable): {e}\")\n        # Implement retry logic or fallback\n\nexcept LLMError as e:\n    logger.error(f\"LLM operation failed: {e.reason}\")\n    # Maybe retry with different parameters\n    # Or fallback to a simpler approach\n</code></pre>"},{"location":"observability/error_handling/#comprehensive-error-handling","title":"\ud83d\udd0d Comprehensive Error Handling","text":"<pre><code>import railtracks as rt\nfrom rt.exceptions import (\n    RTError, NodeCreationError, NodeInvocationError, \n    LLMError, GlobalTimeOutError, ContextError, FatalError\n)\n\ntry:\n    # Setup phase\n    node = rt.agent_node(\n        llm_model=rt.llm.OpenAI(\"gpt-4o\"),\n        system_message=\"You are a helpful assistant\",\n    )\n\n    # Configure timeout\n    rt.set_config(timeout=60.0)\n\n    # Execution phase\n    result = await rt.call(node, user_input=\"Explain quantum computing\")\n\nexcept NodeCreationError as e:\n    # Configuration or setup issue\n    logger.error(\"Node setup failed - check your configuration\")\n    print(e)  # Shows debugging tips\n\nexcept NodeInvocationError as e:\n    # Runtime execution issue\n    if e.fatal:\n        logger.error(\"Fatal execution error - stopping\")\n        raise\n    else:\n        logger.warning(\"Recoverable execution error\")\n        # Implement recovery strategy\n\nexcept LLMError as e:\n    # LLM-specific issue\n    logger.error(f\"LLM error: {e.reason}\")\n    if e.message_history:\n        # Analyze conversation for debugging\n        save_debug_history(e.message_history)\n\nexcept GlobalTimeOutError as e:\n    # Execution took too long\n    logger.error(f\"Execution timed out after {e.timeout}s\")\n    # Maybe increase timeout or optimize graph\n\nexcept ContextError as e:\n    # Context management issue\n    logger.error(\"Context error - check your context setup\")\n    print(e)  # Shows debugging tips\n\nexcept FatalError as e:\n    # User-defined critical error\n    logger.critical(f\"Fatal error: {e}\")\n    # Implement emergency shutdown procedures\n\nexcept RTError as e:\n    # Catch any other RT errors\n    logger.error(f\"RailTracks error: {e}\")\n\nexcept Exception as e:\n    # Non-RT errors\n    logger.error(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"observability/error_handling/#error-recovery-strategies","title":"\ud83d\udd04 Error Recovery Strategies","text":""},{"location":"observability/error_handling/#retry-with-backoff","title":"\ud83d\udd01 Retry with Backoff","text":"<pre><code>import asyncio\nimport railtracks as rt\nfrom rt.exceptions import NodeInvocationError, LLMError\n\nasync def call_with_retry(node, user_input, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return await rt.call(node, user_input=user_input)\n        except (NodeInvocationError, LLMError) as e:\n            if attempt == max_retries - 1:\n                raise  # Last attempt, re-raise\n\n            wait_time = 2 ** attempt  # Exponential backoff\n            logger.warning(f\"Attempt {attempt + 1} failed, retrying in {wait_time}s\")\n            await asyncio.sleep(wait_time)\n</code></pre>"},{"location":"observability/error_handling/#graceful-degradation","title":"\ud83d\udee1\ufe0f Graceful Degradation","text":"<pre><code>import railtracks as rt\nfrom rt.exceptions import NodeInvocationError\n\nasync def call_with_fallback(primary_node, fallback_node, user_input):\n    try:\n        return await rt.call(primary_node, user_input=user_input)\n    except NodeInvocationError as e:\n        if not e.fatal:\n            logger.info(\"Primary execution failed, trying fallback\")\n            return await rt.call(fallback_node, user_input=user_input)\n        raise\n</code></pre>"},{"location":"observability/error_handling/#best-practices","title":"\u2705 Best Practices","text":""},{"location":"observability/error_handling/#1-handle-errors-at-the-right-level","title":"1. \ud83d\udcdd Handle Errors at the Right Level","text":"<ul> <li>Handle <code>NodeCreationError</code> during setup/configuration</li> <li>Handle <code>NodeInvocationError</code> during execution with appropriate recovery</li> <li>Handle <code>LLMError</code> with retry logic and fallbacks</li> <li>Let <code>FatalError</code> bubble up to stop execution</li> </ul>"},{"location":"observability/error_handling/#2-use-error-information","title":"2. \ud83d\udcca Use Error Information","text":"<ul> <li>Check the <code>fatal</code> flag on <code>NodeInvocationError</code></li> <li>Examine <code>message_history</code> in <code>LLMError</code> for debugging</li> <li>Read the <code>notes</code> property for debugging tips</li> </ul>"},{"location":"observability/error_handling/#3-implement-appropriate-recovery","title":"3. \ud83d\udd27 Implement Appropriate Recovery","text":"<ul> <li>Retry transient errors (network issues, rate limits)</li> <li>Fallback for recoverable errors</li> <li>Fail fast for configuration errors</li> <li>Log appropriately for debugging</li> </ul>"},{"location":"observability/error_handling/#4-monitor-and-alert","title":"4. \ud83d\udcc8 Monitor and Alert","text":"<p>For detailed logging and monitoring strategies, see Logging.</p>"},{"location":"observability/error_handling/#5-testing-error-scenarios","title":"5. \ud83e\uddea Testing Error Scenarios","text":"<pre><code>import pytest\nimport railtracks as rt\nfrom rt.exceptions import NodeInvocationError\n\ndef test_node_error_handling():\n    with pytest.raises(NodeInvocationError) as exc_info:\n        # Test code that should raise NodeInvocationError\n        pass\n\n    assert not exc_info.value.fatal  # Test specific properties\n    assert \"expected error message\" in str(exc_info.value)\n</code></pre>"},{"location":"observability/error_handling/#debugging-tips","title":"\ud83d\udd0d Debugging Tips","text":"<ol> <li>Enable Debug Logging: RailTracks errors include colored output and debugging notes</li> <li>Check Error Properties: Many errors include additional context (notes, message_history, etc.)</li> <li>Use Message History: LLMError includes conversation context for debugging</li> <li>Examine Stack Traces: RT errors preserve the full stack trace for debugging</li> <li>Test Error Scenarios: Write tests that verify your error handling works correctly</li> </ol> <p>The RailTracks error system is designed to fail fast when appropriate, provide clear feedback, and enable robust error recovery strategies.</p>"},{"location":"observability/logging/","title":"\ud83e\udeb5 Logging","text":"<p>Railtracks provides built-in logging to help track the execution of your flows. Logs are automatically generated and can be viewed in the terminal or saved to a file.</p> <pre><code>[+3.525  s] RT          : INFO     - START CREATED Github Agent\n[+8.041  s] RT          : INFO     - Github Agent CREATED create_issue\n[+8.685  s] RT          : INFO     - create_issue DONE\n[+14.333 s] RT          : INFO     - Github Agent CREATED assign_copilot_to_issue\n[+14.760 s] RT          : INFO     - assign_copilot_to_issue DONE\n[+17.540 s] RT          : INFO     - Github Agent CREATED assign_copilot_to_issue\n[+18.961 s] RT          : INFO     - assign_copilot_to_issue DONE\n[+23.401 s] RT          : INFO     - Github Agent DONE\n</code></pre>"},{"location":"observability/logging/#configuring-logging","title":"\u2699\ufe0f Configuring Logging","text":""},{"location":"observability/logging/#logging-levels","title":"\ud83d\udd22 Logging Levels","text":"<p>Railtracks supports four logging levels:</p> <ol> <li><code>VERBOSE</code>: Includes all logs, including <code>DEBUG</code>.</li> <li><code>REGULAR</code>: (Default) Includes <code>INFO</code> and above. Ideal for local development.</li> <li><code>QUIET</code>: Includes <code>WARNING</code> and above. Recommended for production.</li> <li><code>NONE</code>: Disables all logging.</li> </ol> <pre><code>import railtracks as rt\n\nrt.ExecutorConfig(logging_setting=\"VERBOSE\")\nrt.ExecutorConfig(logging_setting=\"REGULAR\")\nrt.ExecutorConfig(logging_setting=\"QUIET\")\nrt.ExecutorConfig(logging_setting=\"NONE\")\n</code></pre>"},{"location":"observability/logging/#logging-handlers","title":"\ud83d\udce6 Logging Handlers","text":""},{"location":"observability/logging/#console-handler","title":"\ud83d\udda5\ufe0f Console Handler","text":"<p>By default, logs are printed to <code>stdout</code> and <code>stderr</code>.</p>"},{"location":"observability/logging/#file-handler","title":"\ud83d\uddc2\ufe0f File Handler","text":"<p>To save logs to a file, pass a <code>log_file</code> parameter to the config:</p> <pre><code>import railtracks as rt\n\nrt.ExecutorConfig(log_file=\"my_logs.log\")\n</code></pre>"},{"location":"observability/logging/#custom-handlers","title":"\ud83d\udee0\ufe0f Custom Handlers","text":"<p>Railtracks uses the standard Python <code>logging</code> module with the <code>RT</code> prefix. You can attach custom handlers:</p> <pre><code>import logging\n\nclass CustomHandler(logging.Handler):\n    def emit(self, record):\n        # Custom logging logic\n        pass\n\nlogger = logging.getLogger()\nlogger.addHandler(CustomHandler())\n</code></pre>"},{"location":"observability/logging/#example-usage","title":"\ud83e\uddea Example Usage","text":"<p>You can configure logging globally or per-run.</p>"},{"location":"observability/logging/#global-configuration","title":"\ud83c\udf10 Global Configuration","text":"<pre><code>import railtracks as rt\n\nrt.set_config(\n    rt.ExecutorConfig(\n        logging_setting=\"VERBOSE\",\n        log_file=\"my_logs.log\"\n    )\n)\n</code></pre> <p>Applies to all flows.</p>"},{"location":"observability/logging/#scoped-configuration","title":"\ud83d\udd12 Scoped Configuration","text":"<pre><code>import railtracks as rt\n\nwith rt.Session(\n        rt.ExecutorConfig(\n            logging_setting=\"VERBOSE\",\n            log_file=\"my_logs.log\"\n        )\n) as runner:\n    # Your code here\n    pass\n</code></pre> <p>Applies only within the context of the <code>Runner</code>.</p>"},{"location":"observability/logging/#forwarding-logs-to-external-services","title":"\ud83c\udf0d Forwarding Logs to External Services","text":"<p>You can forward logs to services like Loggly, Sentry, or Conductr by attaching custom handlers. Refer to each provider's documentation for integration details.</p> ConductrLogglySentry <pre><code>import logging\n# Example integration TBD\n\nlogger = logging.getLogger(\"RT\")\n</code></pre> <pre><code>import logging\nfrom loggly.handlers import HTTPSHandler\n\nLOGGLY_TOKEN = 'YOUR_LOGGLY_TOKEN'\nhttps_handler = HTTPSHandler(\n    url=f'https://logs-01.loggly.com/inputs/{LOGGLY_TOKEN}/tag/python'\n)\n\nlogger = logging.getLogger()\nlogger.addHandler(https_handler)\n</code></pre> <pre><code>import sentry_sdk\n\nsentry_sdk.init(\n    dsn=\"https://examplePublicKey@o0.ingest.sentry.io/0\",\n    send_default_pii=True,  # Collects additional metadata\n)\n</code></pre>"},{"location":"observability/logging/#log-message-examples","title":"\ud83e\uddfe Log Message Examples","text":"\ud83d\udc1e DEBUG Messages Type Example Runner Created <code>RT.Runner   : DEBUG    - Runner &lt;RUNNER_ID&gt; is initialized</code> Node Created <code>RT.Publisher: DEBUG    - RequestCreation(current_node_id=&lt;PARENT_NODE_ID&gt;, new_request_id=&lt;REQUEST_ID&gt;, running_mode=async, new_node_type=&lt;NODE_NAME&gt;, args=&lt;INPUT_ARGS&gt;, kwargs=&lt;INPUT_KWARGS&gt;)</code> Node Completed <code>RT.Publisher: DEBUG    - &lt;NODE_NAME&gt; DONE with result &lt;RESULT&gt;</code> \u2139\ufe0f INFO Messages Type Example Initial Request <code>RT          : INFO     - START CREATED &lt;NODE_NAME&gt;</code> Invoking Nodes <code>RT          : INFO     - &lt;PARENT_NODE_NAME&gt; CREATED &lt;CHILD_NODE_NAME&gt;</code> Node Completed <code>RT          : INFO     - &lt;NODE_NAME&gt; DONE</code> Run Data Saved <code>RT.Runner   : INFO     - Saving execution info to .railtracks\\&lt;RUNNER_ID&gt;.json</code> \u26a0\ufe0f WARNING Messages Type Example Overwriting File <code>RT.Runner   : WARNING  - File .railtracks\\&lt;RUNNER_ID&gt;.json already exists, overwriting...</code> \u274c ERROR Messages Type Example Node Failed <code>RT          : ERROR    - &lt;NODE_NAME&gt; FAILED</code>"},{"location":"observability/streaming/","title":"\ud83d\udeb0 Streaming","text":"<p>Streaming lets you monitor your agents' progress in real time by sending live updates during execution. This can be useful for:</p> <ul> <li>Displaying progress in a UI or dashboard</li> <li>Logging intermediate steps for debugging</li> <li>Triggering alerts based on runtime events</li> </ul> <p>Railtracks supports basic data streaming, enabling you to receive these updates via a callback function.</p>"},{"location":"observability/streaming/#usage","title":"\u2699\ufe0f Usage","text":"<p>To enable streaming, provide a callback function to the <code>subscriber</code> parameter in <code>ExecutorConfig</code>. This function will receive streaming updates:</p> <pre><code>import railtracks as rt\n\n\ndef example_streaming_handler(data):\n    print(f\"Received data: {data}\")\n\n\nrt.ExecutorConfig(broadcast_callback=example_streaming_handler)\n</code></pre> <p>With streaming enabled, call <code>rt.stream(...)</code> inside any function decorated with <code>@rt.to_node</code> to send updates:</p> <pre><code>import railtracks as rt\n\n\n@rt.function_node\ndef example_node(data: list[str]):\n    rt.broadcast(f\"Handling {len(data)} items\")\n</code></pre> <p>Warning</p> <p>Currently, only string messages can be streamed.</p>"},{"location":"observability/visualization/","title":"\ud83d\udc40 Visualization","text":"<p>One of the number one complaints when working with LLMs is that they can be a black box. Agentic applications exacerbate this problem by adding even more complexity. RailTracks aims to make it easier than ever to visualize your runs.</p>"},{"location":"observability/visualization/#local-development-visualization","title":"\ud83d\udcbb Local Development Visualization","text":"<p>RailTracks comes with a built-in visualization tool that allows you to see your runs once they have completed.</p>"},{"location":"observability/visualization/#usage","title":"Usage","text":"<p>First install the CLI tool if you haven't already: <pre><code>pip install railtracks-cli\n</code></pre></p> <pre><code>railtracks init\n</code></pre> <p>This will create a <code>.railtracks</code> directory in your current working directory setting up the web app. You can then run your application</p> <pre><code>railtracks viz\n</code></pre> <p>This should open up a web browser window where you can see your runs once they have finished.</p> <p></p> <p>Saving State</p> <p>By default, all of your runs will be saved to the <code>.railtracks</code> directory. If you don't want things saved, you can set the flag to <code>False</code>:</p> <pre><code>import railtracks as rt\n\nrt.set_config(save_state=False)\n# or if you want it scoped to a session\nwith rt.Session(save_state=False): ...\n</code></pre>"},{"location":"observability/visualization/#remote-visualization","title":"\u2601\ufe0f Remote Visualization","text":"<p>However, local viewing is only the beginning of the challenges that face any Agent developer. When you deploy your application how can you understand what is going on in your Agent?</p> <p>This product is coming soon \ud83d\ude04</p>"},{"location":"system_internals/concepts/","title":"Core Concepts","text":""},{"location":"system_internals/concepts/#node-based-execution-model","title":"Node-Based Execution Model","text":"<ul> <li>Nodes: Atomic units of work (functions, AI agents, tools)</li> <li>Execution Graph: Dynamic tree built during runtime</li> <li>Call Semantics: How nodes invoke other nodes</li> </ul>"},{"location":"system_internals/concepts/#immutable-state-architecture","title":"Immutable State Architecture","text":"<ul> <li>Forest Data Structures: Why trees, not graphs</li> <li>Temporal Tracking: Time-travel debugging capabilities</li> <li>State Snapshots: Point-in-time execution views</li> </ul>"},{"location":"system_internals/concepts/#event-driven-coordination","title":"Event-Driven Coordination","text":"<ul> <li>Request Lifecycle: Creation \u2192 Execution \u2192 Completion</li> <li>Message Types: Success, Failure, Streaming, Fatal</li> <li>Loose Coupling: Components communicate via events only</li> </ul>"},{"location":"system_internals/concepts/#session-isolation","title":"Session Isolation","text":"<ul> <li>Context Boundaries: Each session is independent</li> <li>Resource Management: Automatic cleanup and lifecycle</li> <li>Configuration Scoping: Settings apply per-session</li> </ul>"},{"location":"system_internals/coordinator/","title":"\ud83c\udfaf Coordinator","text":""},{"location":"system_internals/coordinator/#overview","title":"\ud83c\udf1f Overview","text":"<p>The <code>Coordinator</code> is the central component responsible for invoking and managing the execution of tasks within the Railtracks system. It acts as the concrete invoker, receiving tasks and delegating them to the appropriate execution strategies. It ensures that every task is tracked from submission to completion, maintaining a comprehensive state of all ongoing and completed jobs.</p>"},{"location":"system_internals/coordinator/#key-components","title":"\ud83d\udd27 Key Components","text":""},{"location":"system_internals/coordinator/#coordinator_1","title":"\ud83c\udfae <code>Coordinator</code>","text":"<p>This class orchestrates task execution. It maintains the system's state via <code>CoordinatorState</code>, uses different <code>AsyncioExecutionStrategy</code> implementations to run tasks, and listens for task completion events through the pub/sub system to keep the state up-to-date.</p>"},{"location":"system_internals/coordinator/#coordinatorstate","title":"\ud83d\udcca <code>CoordinatorState</code>","text":"<p>A state container that holds a list of all <code>Job</code> objects. It tracks every task that is currently running or has been completed, providing a complete history of work handled by the <code>Coordinator</code>.</p>"},{"location":"system_internals/coordinator/#job","title":"\ud83d\udcdd <code>Job</code>","text":"<p>Represents a single unit of work. A <code>Job</code> is created when a task is submitted, and its lifecycle is tracked from an <code>opened</code> to a <code>closed</code> state. It records the task's identifiers, status, result, and timing information, offering a detailed view of each task's execution.</p>"},{"location":"system_internals/coordinator/#asyncioexecutionstrategy","title":"\u26a1 <code>AsyncioExecutionStrategy</code>","text":"<p>An execution strategy that uses asyncio for task execution. This strategy provides async-await style execution for tasks, allowing for efficient concurrent processing without the need for threads or processes. It handles task invocation, result processing, and error handling while publishing completion messages through the pub/sub system.</p>"},{"location":"system_internals/coordinator/#execution-flow","title":"\ud83d\udd04 Execution Flow","text":"<p>The execution of a task follows a well-defined sequence of events, ensuring reliable processing and state management:</p> <ol> <li>Submission: A task is submitted to the system via a call to <code>Coordinator.submit(task, mode)</code> where <code>mode</code> is the key for which <code>TaskExecutionStrategy</code> to be used.</li> <li>Job Creation: The <code>Coordinator</code> uses its member <code>CoordinatorState</code> object's <code>add_job</code> method which creates a <code>Job</code> instance for the submitted <code>Task</code> initialized with a status of <code>opened</code> and a start time.</li> <li>Delegation: The <code>Coordinator</code> determines the correct <code>TaskExecutionStrategy</code> based on the task's configuration and delegates the execution to it.</li> <li>Asynchronous Execution: The execution strategy runs the task asynchronously, allowing the <code>Coordinator</code> to manage other tasks concurrently.</li> <li>Completion Notification: Upon completion, the <code>TaskExecutionStrategy</code> publishes a <code>RequestCompletionMessage</code> to the pub/sub system.</li> <li>Handling Completion: The <code>Coordinator</code>, being a subscriber to these messages, receives the notification in its <code>handle_item</code> method.</li> <li>Finalizing the Job: The <code>Coordinator</code> finds the corresponding <code>Job</code> in its <code>CoordinatorState</code> using the <code>request_id</code> from the message and updates its status to <code>closed</code>, recording the final result and end time.</li> </ol>"},{"location":"system_internals/coordinator/#diagrams","title":"\ud83d\udcca Diagrams","text":"<p>This diagram shows the sequence of interactions when a task is submitted and processed.</p> <p><pre><code>sequenceDiagram\n    participant A as Actor\n    participant C as Coordinator\n    participant CS as CoordinatorState\n    participant J as Job\n    participant TES as TaskExecutionStrategy\n    participant RT as RTPublisher\n\n    A-&gt;&gt;C: start(publisher)\n    A-&gt;&gt;C: submit(task)\n    C-&gt;&gt;RT: subscribe(callback)\n    C-&gt;&gt;CS: add_job(task)\n    CS-&gt;&gt;J: create_new(task)\n    J-&gt;&gt;CS: Job\n    C-&gt;&gt;TES: execute(task)\n    TES-&gt;&gt;C: RequestSuccess/Failure\n    TES-&gt;&gt;RT: publish(respone)\n\n    Note over RT: Coordinator is subscribed to RTPublisher and gets notified of the response\n</code></pre> Note: The Coordinator is subscribed to RTPublisher and gets notified of the response automatically through the pub/sub system.</p>"},{"location":"system_internals/node/","title":"Node Abstraction","text":""},{"location":"system_internals/node/#overview","title":"Overview","text":""},{"location":"system_internals/node/#overview_1","title":"Overview","text":"<p>Nodes are the main components of RailTracks core of our abstractions. Looking at the abstract class, we can see that each Node needs the following methods implemented: Nodes are the main components of RailTracks core of our abstractions. Looking at the abstract class, we can see that each Node needs the following methods implemented:</p>"},{"location":"system_internals/node/#execution-flow","title":"Execution Flow","text":"<p>After the creation of a Node, execution primarily happens through the <code>call</code> method, which is responsible for invoking the node's logic and handling its execution flow. The <code>call</code> method can be called synchronously or asynchronously, depending on the node's configuration and the execution strategy in use.</p> <p>The scenario for the execution of a node is one of the following cases:</p> <p>I. Standalone Execution</p> <p>II. Within a Session Context:</p> <pre><code>A. **Top Level Node Execution**: In this case, the node is executed as the main entry point of the session, and it has full access to the session context and its resources.\n\nB. **Node Invoking Another Node**: Here, the node acts as a caller to another node, passing the necessary context and parameters for the invoked node to execute.\n</code></pre>"},{"location":"system_internals/node/#i-standalone-execution","title":"I. Standalone Execution","text":"<p>This is the simplest case where a node is executed independently, without being part of a larger workflow or session. This is mainly when we want to quickly test a node's functionality or when the node is designed to be used in isolation and we do not care about tracking its execution history or state.</p> <pre><code>graph TD\n    call[rt.call] --&gt; session[Temporary Session Context]\n    session--&gt; start[rt.interaction._start]\n    start --&gt; |await|startPublisher[rt.context.central.activate_publisher]\n    start --&gt; execute[rt.interaction._execute]\n    startPublisher --&gt; getPublisher[rt.context.get_publisher]\n    execute --&gt; getPublisher[rt.context.get_publisher]\n    getPublisher --&gt; publisher[publisher]\n    publisher --&gt; |await| awaitPublish[publisher.publish]\n    publisher --&gt; |await| listen[publisher.listener]\n</code></pre> <p>Here is a breakdown of the flow:</p> <ol> <li>Call Invocation: The process begins with the invocation of the <code>call</code> method on a node, which is the entry point for executing the node's logic.</li> <li>Session Context Wrapper: Inside call, if we identify if no session context exists, we create a temporary session context wrapper to manage the execution environment.</li> <li>Start Interaction: The <code>rt.interaction._start</code> function is called to firstly initalize the publisher and then execute the node's logic.</li> <li>Publisher Activation: The publisher is activated to handle message publishing and subscribing.</li> <li>Node Execution: The node's logic is executed within the <code>_execute</code> function, which handles the actual processing of the node.</li> <li>Node Execution: The node's logic is executed within the <code>_execute</code> function, which publishes a <code>RequestCreation</code> message to signal the start of the node's execution and then \"listens\" for completion messages and returns the final result.</li> <li>Cleanup: After execution, the publisher is shutdown and we exit the temporary session context.</li> </ol>"},{"location":"system_internals/node/#ii-within-a-session-context","title":"II. Within a Session Context","text":"<p>The recommended and intended way to execute agents (which are often comprised of interactive nodes) is within a session context. This allows for better management of state, history, and interactions between nodes, also allowing for monitoring, deployment, visualization, and debugging of the entire workflow.</p> <p>The workflow for executing a node within a session context is quite similar to the standalone execution, but with the removal of creating the \"temporary session\" since upon entering the session context, we already have a session context available. However, we still need to use the <code>_start</code> method to initialize the publisher and execute the node's logic, which corresponds point II.A from the list above.</p> <p>II.B is a sub-category of the context having been activated already as well and we therefore simply use the go directly to the <code>_execute</code> method to execute the node's logic.</p>"},{"location":"system_internals/overview/","title":"RailTracks Internal Architecture Overview","text":"<p>Welcome to the internal architecture overview of RailTracks, our framework for building agentic workflows. This document provides a high-level understanding of how the system is structured and how its different components interact with each other.</p>"},{"location":"system_internals/overview/#execution-flow","title":"Execution Flow","text":""},{"location":"system_internals/overview/#railtracks-internal-architecture-overview_1","title":"RailTracks Internal Architecture Overview","text":"<p>Welcome to the internal architecture overview of RailTracks, our framework for building agentic workflows. This document provides a high-level understanding of how the system is structured and how its different components interact with each other.</p>"},{"location":"system_internals/overview/#execution-flow_1","title":"Execution Flow","text":"<ol> <li>Agent Creation: Users create an agent using <code>rt.agent_node(...)</code> customizing the various different available parameters such as <code>llm_model</code>, <code>system_message</code>, and <code>tool_nodes</code>. </li> <li>Session Initialization: The <code>Session</code> is initialized as a context manager, which sets up the necessary components like <code>RTPublisher</code>, <code>Coordinator</code>, and <code>RTState</code>.</li> <li>Agent Execution: Users run the agent by calling <code>call</code> or <code>call_sync</code> methods inside the context with the appropriate parameters given the setup of their agent.</li> <li>Agent Creation: Users create an agent using <code>rt.agent_node(...)</code> customizing the various different available parameters such as <code>llm_model</code>, <code>system_message</code>, and <code>tool_nodes</code>. </li> <li>Session Initialization: The <code>Session</code> is initialized as a context manager, which sets up the necessary components like <code>RTPublisher</code>, <code>Coordinator</code>, and <code>RTState</code>.</li> <li>Agent Execution: Users run the agent by calling <code>call</code> or <code>call_sync</code> methods inside the context with the appropriate parameters given the setup of their agent.</li> </ol> <pre><code>graph TD\n    A[Agent Creation] --&gt; |User Creates Agent| B[Session Initialization]\n    B --&gt; |Sets up Components and Context| C[Agent Execution]\n    C --&gt; |Runs Agent and Returns Results| D[Results]\n    B --&gt; |Saves the temporal State| E[RTState]\n    D --&gt; |Saves output| E[RTState]\n</code></pre>"},{"location":"system_internals/overview/#core-architectural-principles","title":"Core Architectural Principles","text":"<ul> <li>Immutable State Trees: All execution history preserved</li> <li>Event-Driven Communication: Pub/sub messaging between components  </li> <li>Graceful Error Recovery: Continue execution when possible and desirable</li> <li>Session-Scoped Isolation: Clean boundaries between workflow runs</li> </ul>"},{"location":"system_internals/overview/#developer-journey","title":"Developer Journey","text":"<ol> <li>Start Here: Core Concepts - Understand the mental model behind RailTracks</li> <li>Core Components<ul> <li>Session</li> <li>PubSub System</li> <li>Coordinator</li> <li>RTState</li> </ul> </li> </ol>"},{"location":"system_internals/pubsub/","title":"\ud83d\udce1 PubSub (Publisher-Subscriber) Documentation","text":""},{"location":"system_internals/pubsub/#overview","title":"\ud83d\udd0d Overview","text":"<p>The PubSub (Publisher-Subscriber) system is a messaging pattern that allows different parts of the RailTracks system to communicate asynchronously. Think of it like a radio station: publishers broadcast messages (like radio shows), and subscribers listen for messages they're interested in (like tuning into specific stations).</p>"},{"location":"system_internals/pubsub/#what-is-pubsub","title":"\ud83e\udd14 What is PubSub?","text":"<p>PubSub is a communication pattern where:</p> <ul> <li>Publishers send messages without knowing who will receive them</li> <li>Subscribers listen for messages they care about without knowing who sent them</li> <li>A message broker (in our case, the Publisher class) handles the routing</li> </ul> <p>This creates loose coupling between components - they don't need to know about each other directly.</p>"},{"location":"system_internals/pubsub/#what-are-callbacks","title":"\ud83d\udcde What are Callbacks?","text":"<p>A callback is a function that gets called automatically when something happens. In the context of PubSub:</p> <ul> <li>You give the system a function (the callback)</li> <li>The system calls your function when a relevant message arrives</li> <li>Your function receives the message as a parameter</li> </ul> <p>Example: <pre><code>def my_callback(message):\n    print(f\"Received: {message}\")\n\n# This callback will be called whenever a message is published\n</code></pre></p>"},{"location":"system_internals/pubsub/#core-components","title":"\ud83e\udde9 Core Components","text":""},{"location":"system_internals/pubsub/#1-messages-messagespy","title":"1. \ud83d\udce8 Messages (<code>messages.py</code>)","text":"<p>Messages are the data structures that flow through the PubSub system. All messages inherit from <code>RequestCompletionMessage</code>:</p>"},{"location":"system_internals/pubsub/#base-message-class","title":"\ud83c\udfd7\ufe0f Base Message Class","text":"<ul> <li><code>RequestCompletionMessage</code>: The foundation for all messages in the system</li> <li>Has a <code>log_message()</code> method for debugging</li> </ul>"},{"location":"system_internals/pubsub/#request-lifecycle-messages","title":"\ud83d\udd04 Request Lifecycle Messages","text":"<ul> <li><code>RequestCreation</code>: Sent when a new request is created<ul> <li>Contains: current node ID, new request ID, execution mode, node type, and arguments</li> </ul> </li> <li><code>RequestSuccess</code>: Sent when a request completes successfully<ul> <li>Contains: request ID, node state, and the result</li> </ul> </li> <li><code>RequestFailure</code>: Sent when a request fails during execution<ul> <li>Contains: request ID, node state, and the error</li> </ul> </li> <li><code>RequestCreationFailure</code>: Sent when request creation itself fails<ul> <li>Contains: request ID and the error</li> </ul> </li> </ul>"},{"location":"system_internals/pubsub/#special-messages","title":"\u26a1 Special Messages","text":"<ul> <li><code>FatalFailure</code>: Indicates an irrecoverable system failure</li> <li><code>Streaming</code>: Used for streaming data during execution<ul> <li>Contains: the streamed object and node ID</li> </ul> </li> </ul>"},{"location":"system_internals/pubsub/#2-publisher-publisherpy","title":"2. \ud83d\udce2 Publisher (<code>publisher.py</code>)","text":"<p>The Publisher is the central message broker that manages message distribution.</p>"},{"location":"system_internals/pubsub/#basic-publisher-features","title":"\ud83d\ude80 Basic Publisher Features","text":"<ul> <li>Asynchronous: Uses <code>asyncio</code> for non-blocking operations</li> <li>Ordered: Messages are processed in the order they arrive</li> <li>Thread-safe: Multiple components can publish simultaneously</li> </ul>"},{"location":"system_internals/pubsub/#key-methods","title":"\ud83d\udd11 Key Methods","text":"<p>\ud83c\udfac Starting and Stopping: <pre><code>publisher = Publisher()\nawait publisher.start()  # Start the message processing loop\nawait publisher.shutdown()  # Stop and cleanup\n</code></pre></p> <p>\ud83d\udce4 Publishing Messages: <pre><code>await publisher.publish(message)  # Send a message to all subscribers\n</code></pre></p> <p>\ud83d\udce5 Subscribing: <pre><code>def my_callback(message):\n    print(f\"Got message: {message}\")\n\nsubscriber_id = publisher.subscribe(my_callback, name=\"my_subscriber\")\n</code></pre></p> <p>\ud83d\udeab Unsubscribing: <pre><code>publisher.unsubscribe(subscriber_id)  # Remove a specific broadcast_callback\n</code></pre></p> <p>\ud83d\udd17 Workflow Example: <pre><code>import asyncio\nfrom railtracks.pubsub.publisher import Publisher\n\ndef callback(message: str):\n    \"\"\"\n    A simple callback function that processes incoming messages.\n    \"\"\"\n    print(f\"Received message: {message}\")\n\npublisher = Publisher()\nawait publisher.start()  # Start the publisher\nsubscriber_id = publisher.subscribe(callback, name=\"example_subscriber\")\nawait publisher.publish(\"Hello, World!\")  # Publish a message\n\nawait asyncio.sleep(1)  # Wait for the message to be processed\nawait publisher.shutdown()  # Stop the publisher\n</code></pre></p>"},{"location":"system_internals/pubsub/#advanced-features","title":"\ud83c\udfaf Advanced Features","text":"<p>\ud83d\udc42 Listeners: Listeners wait for a specific message that matches criteria: <pre><code># Wait for the first message that matches the filter\nresult = await publisher.listener(\n    message_filter=lambda msg: isinstance(msg, RequestSuccess),\n    result_mapping=lambda msg: msg.result,\n    listener_name=\"success_listener\"\n)\n</code></pre></p> <p>\ud83d\udd12 Context Manager Support: <pre><code>async with Publisher() as pub:\n    await pub.publish(message)\n# Publisher automatically shuts down when exiting the context\n</code></pre></p>"},{"location":"system_internals/pubsub/#3-subscriber-subscriberpy","title":"3. \ud83d\udc65 Subscriber (<code>subscriber.py</code>)","text":"<p>Contains utilities for creating specialized subscribers.</p>"},{"location":"system_internals/pubsub/#stream-subscriber","title":"\ud83c\udf0a Stream Subscriber","text":"<p>Converts streaming callbacks into proper message subscribers: <pre><code>def handle_stream_data(data):\n    print(f\"Streaming: {data}\")\n\nsubscriber = stream_subscriber(handle_stream_data)\npublisher.subscribe(subscriber)\n</code></pre></p>"},{"location":"system_internals/pubsub/#4-utilities-utilspy","title":"4. \ud83d\udee0\ufe0f Utilities (<code>utils.py</code>)","text":"<p>Helper functions for working with messages.</p>"},{"location":"system_internals/pubsub/#output-mapping","title":"\ud83d\uddfa\ufe0f Output Mapping","text":"<p>Converts message results into their final outputs or raises errors: <pre><code>try:\n    result = output_mapping(message)\n    print(f\"Success: {result}\")\nexcept Exception as error:\n    print(f\"Failed: {error}\")\n</code></pre></p>"},{"location":"system_internals/pubsub/#railtracks-publisher-rtpublisher","title":"\ud83c\udfaf RailTracks Publisher (RTPublisher)","text":"<p><code>RTPublisher</code> is a specialized publisher for the RailTracks system that:</p> <ul> <li>Automatically logs all messages for debugging</li> <li>Handles RailTracks specific message types</li> <li>Provides built-in error logging with stack traces</li> </ul> <pre><code>publisher = RTPublisher()\nawait publisher.start()\n</code></pre>"},{"location":"system_internals/pubsub/#common-usage-patterns","title":"\ud83c\udfa8 Common Usage Patterns","text":""},{"location":"system_internals/pubsub/#1-basic-message-publishing","title":"1. \ud83d\udcdd Basic Message Publishing","text":"<pre><code># Create and start publisher\nasync with RTPublisher() as publisher:\n    # Create a message\n    message = RequestSuccess(\n        request_id=\"123\",\n        node_state=some_node_state,\n        result=\"Hello World\"\n    )\n\n    # Publish it\n    await publisher.publish(message)\n</code></pre>"},{"location":"system_internals/pubsub/#2-subscribing-to-specific-message-types","title":"2. \ud83c\udfaf Subscribing to Specific Message Types","text":"<pre><code>def handle_success(message):\n    if isinstance(message, RequestSuccess):\n        print(f\"Request {message.request_id} succeeded with: {message.result}\")\n\ndef handle_failure(message):\n    if isinstance(message, RequestFailure):\n        print(f\"Request {message.request_id} failed: {message.error}\")\n\npublisher.subscribe(handle_success, \"success_handler\")\npublisher.subscribe(handle_failure, \"failure_handler\")\n</code></pre>"},{"location":"system_internals/pubsub/#3-waiting-for-specific-results","title":"3. \u23f0 Waiting for Specific Results","text":"<pre><code># Wait for a specific request to complete\nresult = await publisher.listener(\n    message_filter=lambda msg: (\n        isinstance(msg, (RequestSuccess, RequestFailure)) and \n        msg.request_id == \"my_request_123\"\n    ),\n    result_mapping=output_mapping,  # Convert to final result or raise error\n    listener_name=\"request_waiter\"\n)\n</code></pre>"},{"location":"system_internals/pubsub/#4-streaming-data","title":"4. \ud83c\udf0a Streaming Data","text":"<pre><code>def process_stream(data):\n    # Process each piece of streaming data\n    print(f\"Processing: {data}\")\n\n# Subscribe to streaming messages\nstream_handler = stream_subscriber(process_stream)\npublisher.subscribe(stream_handler, \"stream_processor\")\n</code></pre>"},{"location":"system_internals/pubsub/#error-handling","title":"\u26a0\ufe0f Error Handling","text":"<p>The PubSub system handles errors gracefully:</p> <ol> <li>Subscriber Errors: If a subscriber's callback fails, it's logged but doesn't affect other subscribers</li> <li>Publisher Errors: Fatal errors are communicated through <code>FatalFailure</code> messages</li> <li>Request Errors: Both creation and execution failures have specific message types</li> </ol>"},{"location":"system_internals/pubsub/#best-practices","title":"\u2705 Best Practices","text":"<ol> <li>Always use descriptive names for subscribers to aid debugging</li> <li>Handle errors in your callbacks - don't let them crash the system</li> <li>Use context managers when possible for automatic cleanup</li> <li>Filter messages efficiently - check message types early in your callbacks</li> <li>Unsubscribe when done to prevent memory leaks</li> <li>Use listeners for one-time responses instead of persistent subscribers</li> </ol>"},{"location":"system_internals/pubsub/#thread-safety-and-async-considerations","title":"\ud83e\uddf5 Thread Safety and Async Considerations","text":"<ul> <li>The Publisher uses <code>asyncio.Queue</code> for thread-safe message handling</li> <li>All operations are asynchronous - always use <code>await</code></li> <li>Messages are processed sequentially to maintain order</li> <li>The system gracefully handles shutdown during active operations</li> </ul>"},{"location":"system_internals/pubsub/#debugging-tips","title":"\ud83d\udc1b Debugging Tips","text":"<ol> <li>Enable debug logging to see all message flows</li> <li>Use meaningful subscriber names for easier log interpretation</li> <li>Check the <code>log_message()</code> output for standardized message descriptions</li> <li>Monitor the publisher's running state with <code>is_running()</code></li> <li>Use the built-in logging subscriber in RCPublisher for automatic message logging</li> </ol> <p>This PubSub system provides a robust foundation for decoupled communication throughout the Request Completion framework, allowing components to interact without tight coupling while maintaining reliability and observability.</p>"},{"location":"system_internals/session/","title":"Session","text":""},{"location":"system_internals/session/#overview","title":"Overview","text":"<p>The <code>Session</code> is the primary entry point and orchestrator for the entire Railtracks system. It serves as the main execution context that initializes, configures, and manages all core components required for running node-based workflows. The <code>Session</code> acts as a facade that brings together the <code>Coordinator</code>, <code>RTState</code>, <code>Pub/Sub</code> messaging, logging, and global context management into a unified, easy-to-use interface.</p>"},{"location":"system_internals/session/#overall-flow","title":"Overall Flow","text":"<ol> <li>Session Creation: A new <code>Session</code> is created, initializing all necessary components.</li> <li>Context Management: The <code>Session</code> manages the execution context, including variable scoping and state management.</li> <li>Workflow Execution: Users define and execute workflows within the <code>Session</code>, leveraging its orchestration capabilities.</li> <li>Result Handling: The <code>Session</code> collects and processes results from the workflow execution, providing a unified interface for accessing outcomes.</li> <li>Cleanup: Upon completion, the <code>Session</code> handles cleanup tasks, ensuring all resources are released properly.</li> </ol> <pre><code>graph TD\n    A[Session Initialization] --&gt; |Creates and Starts| B[RTPublisher]\n    A[Session Initialization] --&gt; |Creates| C[ExecutorConfig]\n    A[Session Initialization] --&gt; |Creates and Starts| D[Coordinator]\n    A[Session Initialization] --&gt; |Creates| E[RTState]\n    D[Coordinator] --&gt; |Subscribes to| B[RTPublisher]\n</code></pre>"},{"location":"system_internals/session/#publishersubscriber-integration","title":"Publisher/Subscriber Integration","text":"<p>The <code>Session</code> establishes a pub/sub messaging system where: - <code>Coordinator</code> subscribes to handle task completion messages - <code>RTState</code> subscribes to manage state updates - Optional user subscribers can be attached for streaming</p>"},{"location":"system_internals/session/#global-context-management","title":"Global Context Management","text":"<p>The <code>Session</code> manages global context through: - Registration of session ID, publisher, and configuration - Context variable scoping for nested executions - Cleanup to prevent context leakage between runs</p>"},{"location":"system_internals/session/#key-components","title":"Key Components","text":""},{"location":"system_internals/session/#session_1","title":"<code>Session</code>","text":"<p>The main orchestrator class responsible for system initialization, lifecycle management, and providing both synchronous and asynchronous execution interfaces. It encapsulates all system components and manages their interactions through a well-defined lifecycle.</p> <pre><code>classDiagram\n    class Session {\n        +executor_config: ExecutorConfig\n        +publisher: RTPublisher[RequestCompletionMessage]\n        +coordinator: Coordinator\n        +rc_state: RTState\n        +__init__(executor_config: ExecutorConfig, context: Dict[str, Any])\n        +__enter__() Session\n        +__exit__(exc_type, exc_val, exc_tb)\n        +run_sync(start_node, *args, **kwargs) ExecutionInfo\n        +run(start_node, *args, **kwargs) ExecutionInfo\n        +setup_subscriber()\n        +info() ExecutionInfo\n        +shutdown()\n    }\n</code></pre>"},{"location":"system_internals/session/#executorconfig","title":"<code>ExecutorConfig</code>","text":"<p>A configuration object that defines how the <code>Session</code> operates, including timeout settings, error handling behavior, logging configuration, and execution options. It provides comprehensive customization of the execution environment.</p> <pre><code>classDiagram\n    class ExecutorConfig {\n        +timeout: float\n        +end_on_error: bool\n        +logging_setting: str\n        +log_file: str | None\n        +subscriber: Callable | None\n        +run_identifier: str\n        +prompt_injection: bool\n        +save_state: bool\n        +__init__(timeout, end_on_error, logging_setting, log_file, subscriber, run_identifier, prompt_injection, save_state)\n    }\n</code></pre>"},{"location":"system_internals/session/#rtstate","title":"<code>RTState</code>","text":"<p>The central state management component that tracks execution progress, manages node lifecycles, handles exceptions, and coordinates between different system components. It maintains the complete execution history and current system state.</p> <pre><code>classDiagram\n    class RTState {\n        +execution_info: ExecutionInfo\n        +executor_config: ExecutorConfig\n        +coordinator: Coordinator\n        +publisher: RTPublisher\n        +__init__(execution_info, executor_config, coordinator, publisher)\n        +handle(item: RequestCompletionMessage)\n        +call_nodes(parent_node_id, request_id, node, args, kwargs)\n        +cancel(node_id: str)\n        +shutdown()\n        +info() ExecutionInfo\n        +get_info(ids: List[str]) ExecutionInfo\n    }\n</code></pre>"},{"location":"system_internals/session/#executioninfo","title":"<code>ExecutionInfo</code>","text":"<p>A comprehensive data structure that captures the complete state of a run, including all nodes, requests, execution paths, timing information, and results. It provides both runtime access and post-execution analysis capabilities.</p> <pre><code>classDiagram\n    class ExecutionInfo {\n        +request_heap: RequestForest\n        +node_heap: NodeForest\n        +stamper: StampManager\n        +__init__(request_heap, node_heap, stamper)\n        +create_new() ExecutionInfo\n        +to_graph() Tuple[List[Vertex], List[Edge]]\n        +graph_serialization() str\n        +get_info(ids: List[str]) ExecutionInfo\n        +answer() Any\n        +all_stamps() List[Stamp]\n    }\n</code></pre>"},{"location":"system_internals/session/#session-lifecycle","title":"Session Lifecycle","text":"<p>The <code>Session</code> follows a well-defined lifecycle that ensures proper initialization, execution, and cleanup:</p>"},{"location":"system_internals/session/#1-initialization-phase","title":"1. Initialization Phase","text":"<p>During initialization, the <code>Session</code>:</p> <ul> <li>Creates or uses provided <code>ExecutorConfig</code></li> <li>Initializes logging system based on configuration</li> <li>Creates <code>RTPublisher</code> for pub/sub messaging</li> <li>Instantiates <code>ExecutionInfo</code> for state tracking</li> <li>Creates <code>Coordinator</code> with execution strategies</li> <li>Initializes <code>RTState</code> to manage execution</li> <li>Registers global context variables</li> <li>Sets up subscriber connections</li> </ul>"},{"location":"system_internals/session/#2-execution-phase","title":"2. Execution Phase","text":"<p>During execution, the <code>Session</code>:</p> <ul> <li>Delegates node execution to the global <code>call</code> function</li> <li>Maintains execution state through <code>RTState</code></li> <li>Coordinates task execution via <code>Coordinator</code></li> <li>Publishes and handles completion messages via <code>RTPublisher</code></li> <li>Tracks all execution details in <code>ExecutionInfo</code></li> </ul>"},{"location":"system_internals/session/#3-cleanup-phase","title":"3. Cleanup Phase","text":"<p>During cleanup, the <code>Session</code>:</p> <ul> <li>Optionally saves execution state to disk (if <code>save_state=True</code>)</li> <li>Shuts down all execution strategies</li> <li>Detaches logging handlers</li> <li>Cleans up global context variables</li> <li>Releases system resources</li> </ul>"},{"location":"system_internals/session/#configuration-and-customization","title":"Configuration and Customization","text":"<p>The <code>Session</code> supports extensive customization through <code>ExecutorConfig</code>:</p> <ul> <li>Timeout Control: Set maximum execution time limits</li> <li>Error Handling: Configure whether to stop on first error or continue</li> <li>Logging: Control log levels and output destinations  </li> <li>State Persistence: Enable/disable saving execution state to disk</li> <li>Streaming: Attach custom subscribers for real-time monitoring</li> <li>Context Injection: Control global context variable behavior</li> </ul>"},{"location":"system_internals/session/#error-handling-and-recovery","title":"Error Handling and Recovery","text":"<p>The <code>Session</code> implements robust error handling:</p> <ul> <li>Graceful Degradation: Continues execution when possible</li> <li>Error Propagation: Properly bubbles up fatal errors</li> <li>State Preservation: Maintains execution state even during failures</li> <li>Cleanup Guarantees: Ensures proper resource cleanup in all scenarios</li> </ul>"},{"location":"tools_mcp/tools_mcp/","title":"\ud83d\udee0\ufe0f RailTracks Tools","text":"<p>Extend Your Agents' Capabilities</p> <p>Tools transform LLMs from chatbots into true agents that can take action in the world.</p> <p>RailTracks provides a comprehensive suite of tools that extend the capabilities of your agents, allowing them to interact with external systems and services. These tools are the \"hands and eyes\" of your agents, enabling them to perform actions beyond just generating text.</p>"},{"location":"tools_mcp/tools_mcp/#ways-to-get-tools","title":"\ud83d\udd0d Ways to Get Tools","text":"<p>RailTracks offers multiple ways to access and create tools:</p> <ol> <li>Built-in Tools - Use our pre-built tools for common tasks</li> <li>MCP Tools - Connect to Model Context Protocol servers for additional capabilities</li> <li>Create Your Own - Build custom tools for your specific needs</li> </ol> <p>For a conceptual overview of tools in RailTracks, see the Tools Guide.</p>"},{"location":"tools_mcp/tools_mcp/#available-tools","title":"\ud83e\uddf0 Available Tools","text":""},{"location":"tools_mcp/tools_mcp/#built-in-tools","title":"\ud83d\udcbb Built-in Tools","text":"Tool Description Use Case Python Execution Write and run Python code Data analysis, calculations, algorithmic tasks Local Shell Execute commands in your local environment File operations, system management, running scripts"},{"location":"tools_mcp/tools_mcp/#mcp-based-tools","title":"\ud83d\udd0c MCP-Based Tools","text":"Tool Description Use Case GitHub Interact with GitHub repositories Code management, issue tracking, PR reviews Notion Create and manage Notion pages Knowledge management, documentation, project planning Slack Send and receive Slack messages Team communication, notifications, updates Web Search Search the web and retrieve information Research, fact-checking, data gathering"},{"location":"tools_mcp/tools_mcp/#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>To use tools in your RailTracks agents, you'll typically follow these steps:</p> <ol> <li>Import the tools you want to use</li> <li>Create an agent that can access these tools</li> <li>Run the agent with your desired input</li> </ol> <pre><code>import railtracks as rt\nfrom railtracks.nodes.library import connect_mcp\nfrom railtracks.rt_mcp import MCPHttpParams\n\n# Get tools from an MCP server\nserver = connect_mcp(MCPHttpParams(url=\"https://remote.mcpservers.org/fetch/mcp\"))\ntools = server.tools\n\n# Create an agent with access to these tools\nagent = rt.library.tool_call_llm(\n    tool_nodes=tools,\n    name=\"Research Agent\",\n    system_message=\"Use the tools to find information.\",\n    llm_model=rt.llm.OpenAILLM(\"gpt-4o\"),\n)\n\n# Run the agent\nwith rt.Session():\n    result = rt.call_sync(\n        agent,\n        \"Find information about RailTracks\"\n    )\n</code></pre>"},{"location":"tools_mcp/tools_mcp/#next-steps","title":"\u27a1\ufe0f Next Steps","text":"<ul> <li>Learn about Model Context Protocol (MCP) for accessing even more tools</li> <li>Explore how to create your own custom tools</li> <li>Check out the tool-specific guides for detailed usage instructions</li> </ul>"},{"location":"tools_mcp/guides/github/","title":"Using GitHub MCP Server with RequestCompletion","text":"<p>To use the GitHub MCP server with RT, use the <code>from_mcp_server</code> utility to load tools directly from the MCP server. A valid GitHub Personal Access Token (PAT) is required, which in this example is provided via an environment variable.</p> <pre><code>import os\nfrom railtracks.rt_mcp import MCPHttpParams\nfrom railtracks.nodes.library.easy_usage_wrappers.mcp_tool import connect_mcp\n\nserver = connect_mcp(\n    MCPHttpParams(\n        url=\"https://api.githubcopilot.com/mcp/\",\n        headers={\n            \"Authorization\": f\"Bearer {os.getenv('GITHUB_PAT_TOKEN')}\",\n        },\n    )\n)\ntools = server.tools\n</code></pre> <p>At this point, the tools can be used the same as any other RT tool. See the following code as a simple example.</p> <pre><code>from railtracks.nodes.library.easy_usage_wrappers.tool_call_llm import tool_call_llm\nimport railtracks as rt\n\nagent = tool_call_llm(\n    connected_nodes={*tools},\n    system_message=\"\"\"You are a GitHub Copilot agent that can interact with GitHub repositories.\"\"\",\n    model=rt.llm.OpenAILLM(\"gpt-4o\"),\n)\n\nuser_prompt = \"\"\"Tell me about the RailtownAI/rc repository on GitHub.\"\"\"\nmessage_history = rt.llm.MessageHistory()\nmessage_history.append(rt.llm.UserMessage(user_prompt))\n\nwith rt.Session() as run:\n    result = run.run_sync(agent, message_history)\n\nprint(result.answer.content)\n</code></pre>"},{"location":"tools_mcp/guides/notion/","title":"Using Notion MCP Server with RT","text":"<p>To use Notion tools with RT, use the <code>from_mcp_server</code> utility to load tools directly from the MCP server. For this example, ensure you have a valid Notion API token set in the environment variables. To get the token, in Notion, go to Settings &gt; Connections &gt; Develop or manage integrations, and create a new integration, or get the token from an existing one.</p> <pre><code>import json\nimport os\nfrom railtracks.integrations.rt_mcp import MCPStdioParams, connect_mcp\n\nMCP_COMMAND = \"npx\"\nMCP_ARGS = [\"-y\", \"@notionhq/notion-mcp-server\"]\nNOTION_VERSION = \"2022-06-28\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {os.environ['NOTION_API_TOKEN']}\",\n    \"Notion-Version\": NOTION_VERSION\n}\n\nnotion_env = {\n    \"OPENAPI_MCP_HEADERS\": json.dumps(headers)\n}\n\nserver = connect_mcp(\n    MCPStdioParams(\n        command=MCP_COMMAND,\n        args=MCP_ARGS,\n        env=notion_env,\n    )\n)\ntools = server.tools\n</code></pre> <p>At this point, the tools can be used the same as any other RT tool. See the following code as a simple example.</p> <pre><code>import railtracks as rt\n\nagent = rt.agent_node(\n    tool_nodes={*tools},\n    system_message=\"\"\"You are a master Notion page designer. You love creating beautiful\n     and well-structured Notion pages and make sure that everything is correctly formatted.\"\"\",\n    llm_model=rt.llm.OpenAILLM(\"gpt-4o\"),\n)\n\nuser_prompt = \"\"\"Create a new page in Notion called 'Jokes' under the parent page \"Welcome to Notion!\" with a small joke at the top of the page.\"\"\"\nmessage_history = rt.llm.MessageHistory()\nmessage_history.append(rt.llm.UserMessage(user_prompt))\n\nwith rt.Session():\n    result = rt.call_sync(agent, message_history)\n\nprint(result.answer.content)\n</code></pre>"},{"location":"tools_mcp/guides/python_sandbox/","title":"Running python code with RailTracks","text":"<p>This is a simple guide to running Python code with RailTracks, using a Docker container as a sandboxed environment. Before running the code, make sure you have Docker installed and running on your machine.</p> <pre><code>import subprocess\nimport railtracks as rt\nfrom railtracks.nodes.library import tool_call_llm\n\n\ndef create_sandbox_container():\n    subprocess.run([\n        \"docker\", \"run\", \"-dit\", \"--rm\",\n        \"--name\", \"sandbox_chatbot_session\",\n        \"--memory\", \"512m\", \"--cpus\", \"0.5\",\n        \"python:3.12-slim\", \"python3\"\n    ])\n\n\ndef kill_sandbox():\n    subprocess.run([\"docker\", \"rm\", \"-f\", \"sandbox_chatbot_session\"])\n\n\ndef execute_code(code: str) -&gt; str:\n    exec_result = subprocess.run([\n        \"docker\", \"exec\", \"sandbox_chatbot_session\",\n        \"python3\", \"-c\", code\n    ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    return exec_result.stdout.decode() + exec_result.stderr.decode()\n\n\nagent = tool_call_llm(\n    tool_nodes={execute_code},\n    system_message=\"\"\"You are a master python programmer. To execute code, you have access to a sandboxed Python environment.\n    You can execute code in it using run_in_sandbox.\n    You can only see the output of the code if it is printed to stdout or stderr, so anything you want to see must be printed.\n    You can install packages with code like 'import os; os.system('pip install numpy')'\"\"\",\n    model=rt.llm.OpenAILLM(\"gpt-4o\"),\n)\n</code></pre> <p>When running the agent, use the <code>create_sandbox_container</code> function to start the Docker container before running the agent, and the <code>kill_sandbox</code> function to stop and remove the container after you're done. The following example shows how to use the agent to execute Python code in the sandboxed environment:</p> <pre><code>user_prompt = \"\"\"Create a 3x3 array of random numbers using numpy, and print the array and its mean\"\"\"\nmessage_history = rt.llm.MessageHistory()\nmessage_history.append(rt.llm.UserMessage(user_prompt))\n\nwith rt.Session(rt.ExecutorConfig(logging_setting=\"VERBOSE\")) as run:\n    create_sandbox_container()\n    try:\n        result = run.run_sync(agent, message_history)\n    finally:\n        kill_sandbox()\n\nprint(result.answer.content)\n</code></pre>"},{"location":"tools_mcp/guides/shell_bash/","title":"Using Shell as a tool with RequestCompletion","text":"<p>To allow for usage of shell as a tool, we can create a simple tool using <code>from_fuction</code>. The function could be modified to suit your needs, such as adding error handling or specific command restrictions. Below is a basic example of how to create a shell command execution tool using <code>subprocess</code> in Python.</p> <pre><code>import subprocess\nfrom railtracks.nodes.library import function_node\n\n\ndef run_shell(command: str) -&gt; str:\n    \"\"\"Run a bash command and return its output or error.\"\"\"\n    try:\n        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n        if result.returncode == 0:\n            return result.stdout.strip()\n        else:\n            return f\"Error: {result.stderr.strip()}\"\n    except Exception as e:\n        return f\"Exception: {str(e)}\"\n\n\nbash_tool = function_node(run_shell)\n</code></pre> <p>At this point, the tool can be used the same as any other RT tool. See the following code as a simple example.</p> <pre><code>import platform\n\nimport railtracks as rt\n\nagent = rt.agent_node(\n    tool_nodes={bash_tool},\n    system_message=f\"You are a useful helper that can run local shell commands. \"\n                   f\"You are on a {platform.system()} machine. Use appropriate shell commands to answer the user's questions.\",\n    llm_model=rt.llm.OpenAILLM(\"gpt-4o\"),\n)\n\nuser_prompt = \"\"\"What directories are in the current directory?\"\"\"\nmessage_history = rt.llm.MessageHistory()\nmessage_history.append(rt.llm.UserMessage(user_prompt))\n\nwith rt.Session(logging_setting=\"VERBOSE\"):\n    result = rt.call_sync(agent, message_history)\n\nprint(result.answer.content)\n</code></pre>"},{"location":"tools_mcp/guides/slack/","title":"Adding Slack integration with RT","text":"<p>To allow for Slack integration with RT, you need to first create a Slack app and at it to your Slack workspace - https://api.slack.com/apps.  Next, get the Slack team ID (It starts with T, such as \"T12345678\". You can also optionally specify the Slack channel IDs you want to restrict interaction to (ex. \"C87654321, C87654322\"). Finally, use the <code>from_mcp_server</code> utility to load tools directly from the MCP server.</p> <pre><code>import os\n\nfrom railtracks.integrations.rt_mcp import connect_mcp, MCPStdioParams\n\nMCP_COMMAND = \"npx\"\nMCP_ARGS = [\"-y\", \"@modelcontextprotocol/server-slack\"]\n\nslack_env = {\n    \"SLACK_BOT_TOKEN\": os.environ['SLACK_BOT_TOKEN'],\n    \"SLACK_TEAM_ID\": os.environ['SLACK_TEAM_ID'],\n    \"SLACK_CHANNEL_IDS\": os.environ['SLACK_CHANNEL_IDS'],\n}\n\nserver = connect_mcp(\n    MCPStdioParams(\n        command=MCP_COMMAND,\n        args=MCP_ARGS,\n        env=slack_env,\n    )\n)\ntools = server.tools\n</code></pre> <p>At this point, the tools can be used the same as any other RT tool. See the following code as a simple example.</p> <pre><code>import railtracks as rt\n\nagent = rt.agent_node(\n    tool_nodes={*tools},\n    system_message=\"\"\"You are a Slack agent that can interact with Slack channels.\"\"\",\n    llm_model=rt.llm.OpenAILLM(\"gpt-4o\"),\n)\n\nuser_prompt = \"\"\"Send a message to general saying \"Hello!\".\"\"\"\nmessage_history = rt.llm.MessageHistory()\nmessage_history.append(rt.llm.UserMessage(user_prompt))\n\nwith rt.Session(logging_setting=\"VERBOSE\"):\n    result = rt.call_sync(agent, message_history)\n\nprint(result.answer.content)\n</code></pre>"},{"location":"tools_mcp/guides/websearch_integration/","title":"Web Search Integration with RequestCompletion","text":"<p>This guide demonstrates how to create a web search integration for RequestCompletion (RC) using both MCP (Model Context Protocol) servers and custom Google API tools. This setup allows your AI agent to search the web and fetch content from URLs.</p>"},{"location":"tools_mcp/guides/websearch_integration/#prerequisites","title":"Prerequisites","text":"<p>Before implementing this integration, you'll need:</p> <ol> <li>Google Custom Search API credentials:</li> <li>Visit the Google Cloud Console</li> <li>Enable the Custom Search API</li> <li> <p>Create API credentials and a Custom Search Engine ID</p> </li> <li> <p>Environment variables:  <pre><code>GOOGLE_SEARCH_API_KEY=your_api_key_here\nGOOGLE_SEARCH_ENGINE_ID=your_search_engine_id_here\n</code></pre></p> </li> <li> <p>Required packages:  <pre><code>pip install railtracks python-dotenv aiohttp\n</code></pre></p> </li> </ol>"},{"location":"tools_mcp/guides/websearch_integration/#implementation","title":"Implementation","text":""},{"location":"tools_mcp/guides/websearch_integration/#step-1-import-dependencies-and-load-environment","title":"Step 1: Import Dependencies and Load Environment","text":"<pre><code>from dotenv import load_dotenv\nimport os\nfrom railtracks.nodes.library import connect_mcp, tool_call_llm\nimport railtracks as rt\nfrom railtracks.rt_mcp import MCPHttpParams\nimport aiohttp\nfrom typing import Dict, Any\n\nload_dotenv()\n</code></pre>"},{"location":"tools_mcp/guides/websearch_integration/#step-2-set-up-mcp-tools-for-url-fetching","title":"Step 2: Set Up MCP Tools for URL Fetching","text":"<p>The MCP server provides tools that can fetch and process content from URLs:</p> <p><pre><code># MCP Tools that can fetch data from URLs\nfetch_mcp_server = from_mcp_server(MCPHttpParams(url=\"https://remote.mcpservers.org/fetch/mcp\"))\nfetch_mcp_tools = fetch_mcp_server.tools\n</code></pre> Read more about the <code>from_mcp_server</code> utility TODO: change this link.  This connects to a remote MCP server that provides URL fetching capabilities.</p>"},{"location":"tools_mcp/guides/websearch_integration/#step-3-create-custom-google-search-tool","title":"Step 3: Create Custom Google Search Tool","text":"<pre><code>def _format_results(data: Dict[str, Any]) -&gt; Dict[str, Any]:\n   ...\n\n\n@rt.function_node\nasync def google_search(query: str, num_results: int = 3) -&gt; Dict[str, Any]:\n   \"\"\"\n   Tool for searching using Google Custom Search API\n\n   Args:\n       query (str): The search query\n       num_results (int): The number of results to return (max 5)\n\n   Returns:\n       Dict[str, Any]: Formatted search results\n   \"\"\"\n   params = {\n      'key': os.environ['GOOGLE_SEARCH_API_KEY'],\n      'cx': os.environ['GOOGLE_SEARCH_ENGINE_ID'],\n      'q': query,\n      'num': min(num_results, 5)  # Google API maximum is 5\n   }\n\n   async with aiohttp.ClientSession() as session:\n      try:\n         async with session.get(\"https://www.googleapis.com/customsearch/v1\", params=params) as response:\n            if response.status == 200:\n               data = await response.json()\n               return _format_results(data)\n            else:\n               error_text = await response.text()\n               raise Exception(f\"Google API error {response.status}: {error_text}\")\n      except Exception as e:\n         raise Exception(f\"Search failed: {str(e)}\")\n</code></pre>"},{"location":"tools_mcp/guides/websearch_integration/#step-4-create-and-use-the-search-agent","title":"Step 4: Create and Use the Search Agent","text":"<pre><code># Combine all tools\ntools = fetch_mcp_tools + [google_search]\n\n# Create the agent with search capabilities\nagent = tool_call_llm(\n    connected_nodes={*tools},\n    system_message=\"\"\"You are an information gathering agent that can search the web.\"\"\",\n    model=rt.llm.OpenAILLM(\"gpt-4o\"),\n)\n\n# Example usage\nuser_prompt = \"\"\"Tell me about Railtown AI.\"\"\"\nmessage_history = rt.llm.MessageHistory()\nmessage_history.append(rt.llm.UserMessage(user_prompt))\n\nresult = rt.call_sync(agent, message_history)\nprint(result)\n</code></pre>"},{"location":"tools_mcp/guides/websearch_integration/#how-it-works","title":"How It Works","text":"<ol> <li>Google Search Tool: Uses the Google Custom Search API to find relevant web pages based on user queries</li> <li>MCP Fetch Tools: Retrieves and processes content from the URLs found in search results</li> <li>Agent Integration: Combines both tools to create a comprehensive web search and content analysis system</li> </ol>"},{"location":"tools_mcp/mcp/MCP_tools_in_RT/","title":"\ud83d\udd27 Using MCP Tools in RailTracks","text":""},{"location":"tools_mcp/mcp/MCP_tools_in_RT/#overview","title":"\ud83d\udcdd Overview","text":"<p>Quick Summary</p> <p>RailTracks makes it easy to use any MCP-compatible tool with your agents. Just connect to an MCP server, get the tools, and start using them!</p> <p>RailTracks supports seamless integration with Model Context Protocol (MCP), allowing you to use any MCP-compatible tool as a native RailTracks Tool. This means you can connect your agents to a wide variety of external tools and data sources\u2014without having to implement the tool logic yourself. </p> <p>RailTracks handles the discovery and invocation of MCP tools, so you can focus on building intelligent agents.</p>"},{"location":"tools_mcp/mcp/MCP_tools_in_RT/#prerequisites","title":"\u2705 Prerequisites","text":"<p>Before You Begin</p> <p>Make sure you have the following set up before using MCP tools:</p> <ul> <li>RailTracks Framework installed (<code>pip install railtracks</code>)</li> <li>MCP package set up - Every MCP tool has different requirements (see specific tool documentation)</li> <li>Authentication credentials - Many MCP tools require API keys or OAuth tokens</li> </ul>"},{"location":"tools_mcp/mcp/MCP_tools_in_RT/#connecting-to-mcp-servers","title":"\ud83d\udd0c Connecting to MCP Servers","text":"<p>RailTracks supports two types of MCP servers:</p>"},{"location":"tools_mcp/mcp/MCP_tools_in_RT/#1-remote-http-servers","title":"\ud83c\udf10 1. Remote HTTP Servers","text":"<p>Use <code>MCPHttpParams</code> for connecting to remote MCP servers:</p> <pre><code>import os\nfrom railtracks.nodes.library import connect_mcp\nfrom railtracks.rt_mcp import MCPHttpParams\n\n# Connect to a remote MCP server\nfetch_server = connect_mcp(\n    MCPHttpParams(\n        url=\"https://remote.mcpservers.org/fetch/mcp\",\n        # Optional: Add authentication headers if needed\n        headers={\"Authorization\": f\"Bearer {os.getenv('API_KEY')}\"}\n    )\n)\n</code></pre>"},{"location":"tools_mcp/mcp/MCP_tools_in_RT/#2-local-command-line-servers","title":"\ud83d\udcbb 2. Local Command-Line Servers","text":"<p>Use <code>MCPStdioParams</code> for running local MCP servers:</p> <pre><code>from railtracks.nodes.library import connect_mcp\nfrom railtracks.rt_mcp import MCPStdioParams\n\n# Run a local MCP server (Time server example)\ntime_server = connect_mcp(\n    MCPStdioParams(\n        command=\"npx\",  # or other command to run the server\n        args=[\"mcp-server-time\"]\n    )\n)\n</code></pre>"},{"location":"tools_mcp/mcp/MCP_tools_in_RT/#using-mcp-tools-with-railtracks-agents","title":"\ud83e\udd16 Using MCP Tools with RailTracks Agents","text":"<p>Once you've connected to an MCP server, you can use the tools with your RailTracks agents:</p> <pre><code>import railtracks as rt\nfrom railtracks.nodes.library import connect_mcp\nfrom railtracks.rt_mcp import MCPHttpParams\n\n# Connect to an MCP server (example with Fetch server)\nfetch_server = connect_mcp(MCPHttpParams(url=\"https://remote.mcpservers.org/fetch/mcp\"))\ntools = fetch_server.tools  # List of RailTracks Tool Nodes\n\n# Create an agent that can use these tools\nagent = rt.library.tool_call_llm(\n    tool_nodes=tools,\n    name=\"Web Research Agent\",\n    system_message=\"Use the tools to research information online.\",\n    llm_model=rt.llm.OpenAILLM(\"gpt-4o\"),\n)\n\n# Use the agent\nwith rt.Session():\n    result = rt.call_sync(\n        agent,\n        \"Find information about RailTracks\"\n    )\n    print(result.content)\n</code></pre>"},{"location":"tools_mcp/mcp/MCP_tools_in_RT/#common-mcp-server-examples","title":"\ud83e\uddea Common MCP Server Examples","text":""},{"location":"tools_mcp/mcp/MCP_tools_in_RT/#fetch-server-url-content-retrieval","title":"\ud83c\udf10 Fetch Server (URL Content Retrieval)","text":"<pre><code>import railtracks as rt\nfrom railtracks.nodes.library import connect_mcp\nfrom railtracks.rt_mcp import MCPHttpParams\n\n# Connect to the Fetch MCP server\nfetch_server = connect_mcp(MCPHttpParams(url=\"https://remote.mcpservers.org/fetch/mcp\"))\nfetch_tools = fetch_server.tools\n</code></pre>"},{"location":"tools_mcp/mcp/MCP_tools_in_RT/#github-server","title":"\ud83d\udc19 GitHub Server","text":"<pre><code>import os\nimport railtracks as rt\nfrom railtracks.nodes.library import connect_mcp\nfrom railtracks.rt_mcp import MCPHttpParams\n\n# Connect to the GitHub MCP server\ngithub_server = connect_mcp(\n    MCPHttpParams(\n        url=\"https://api.githubcopilot.com/mcp/\",\n        headers={\n            \"Authorization\": f\"Bearer {os.getenv('GITHUB_PAT_TOKEN')}\",\n        },\n    )\n)\ngithub_tools = github_server.tools\n</code></pre>"},{"location":"tools_mcp/mcp/MCP_tools_in_RT/#notion-server","title":"\ud83d\udcd8 Notion Server","text":"<pre><code>import json\nimport os\nimport railtracks as rt\nfrom railtracks.nodes.library import connect_mcp\nfrom railtracks.rt_mcp import MCPStdioParams\n\n# Connect to the Notion MCP server\nnotion_server = connect_mcp(\n    MCPStdioParams(\n        command=\"npx\",\n        args=[\"-y\", \"@notionhq/notion-mcp-server\"],\n        env={\n            \"OPENAPI_MCP_HEADERS\": json.dumps({\n                \"Authorization\": f\"Bearer {os.environ['NOTION_API_TOKEN']}\",\n                \"Notion-Version\": \"2022-06-28\"\n            })\n        },\n    )\n)\nnotion_tools = notion_server.tools\n</code></pre>"},{"location":"tools_mcp/mcp/MCP_tools_in_RT/#combining-multiple-mcp-tools","title":"\ud83e\udde9 Combining Multiple MCP Tools","text":"<p>You can combine tools from different MCP servers to create powerful agents:</p> <pre><code>import railtracks as rt\nfrom railtracks.nodes.library import connect_mcp\nfrom railtracks.rt_mcp import MCPHttpParams, MCPStdioParams\nimport os\nimport json\n\n# Set up servers and get tools\nfetch_server = connect_mcp(MCPHttpParams(url=\"https://remote.mcpservers.org/fetch/mcp\"))\nfetch_tools = fetch_server.tools\n\ngithub_server = connect_mcp(\n    MCPHttpParams(\n        url=\"https://api.githubcopilot.com/mcp/\",\n        headers={\"Authorization\": f\"Bearer {os.getenv('GITHUB_PAT_TOKEN')}\"},\n    )\n)\ngithub_tools = github_server.tools\n\nnotion_server = connect_mcp(\n    MCPStdioParams(\n        command=\"npx\",\n        args=[\"-y\", \"@notionhq/notion-mcp-server\"],\n        env={\n            \"OPENAPI_MCP_HEADERS\": json.dumps({\n                \"Authorization\": f\"Bearer {os.environ['NOTION_API_TOKEN']}\",\n                \"Notion-Version\": \"2022-06-28\"\n            })\n        },\n    )\n)\nnotion_tools = notion_server.tools\n\n# Combine tools from multiple servers\nall_tools = fetch_tools + github_tools + notion_tools\n\n# Create an agent that can use all tools\nsuper_agent = rt.library.tool_call_llm(\n    tool_nodes=all_tools,\n    name=\"Multi-Tool Agent\",\n    system_message=\"Use the appropriate tools to complete tasks.\",\n    llm_model=rt.llm.OpenAILLM(\"gpt-4o\"),\n)\n</code></pre>"},{"location":"tools_mcp/mcp/MCP_tools_in_RT/#tool-specific-guides","title":"\ud83d\udcda Tool-Specific Guides","text":"<p>For detailed setup and usage instructions for specific MCP tools:</p> <ul> <li>GitHub Tool Guide</li> <li>Notion Tool Guide</li> <li>Slack Tool Guide</li> <li>Web Search Integration Guide</li> </ul>"},{"location":"tools_mcp/mcp/MCP_tools_in_RT/#related-topics","title":"\ud83d\udd17 Related Topics","text":"<ul> <li>What is MCP?</li> <li>RailTracks to MCP: Exposing RT Tools as MCP Tools</li> </ul>"},{"location":"tools_mcp/mcp/RTtoMCP/","title":"Exposing RT Tools as MCP Tools","text":""},{"location":"tools_mcp/mcp/RTtoMCP/#overview","title":"Overview","text":"<p>You can expose any RT Tool as an MCP-compatible tool, making it accessible to any MCP client or LLM agent that supports the Model Context Protocol (MCP). This allows you to share your custom RT logic with other frameworks, agents, or applications that use MCP.</p> <p>RC provides utilities to convert your Nodes into MCP tools and run a FastMCP server, so your tools are discoverable and callable via standard MCP transports (HTTP, SSE, stdio).</p>"},{"location":"tools_mcp/mcp/RTtoMCP/#prerequisites","title":"Prerequisites","text":"<ul> <li>RC Framework installed (<code>pip install railtracks</code>)</li> </ul>"},{"location":"tools_mcp/mcp/RTtoMCP/#basic-usage","title":"Basic Usage","text":""},{"location":"tools_mcp/mcp/RTtoMCP/#1-convert-rt-nodes-to-mcp-tools","title":"1. Convert RT Nodes to MCP Tools","text":"<p>Use the <code>create_mcp_server</code> utility to expose your RT nodes as MCP tools:</p> <pre><code>from railtracks.nodes.library import function_node\nfrom railtracks.integrations.rt_mcp.node_to_mcp import create_mcp_server\n\n\ndef add_nums_plus_ten(num1: int, num2: int):\n    \"\"\"Simple tool example.\"\"\"\n    return num1 + num2 + 10\n\n\nnode = function_node(add_nums_plus_ten)\n\n# Create and run the MCP server\nmcp = create_mcp_server([node], server_name=\"My MCP Server\")\nmcp.run(transport=\"streamable-http\", host=\"127.0.0.1\", port=8000)\n</code></pre> <p>This exposes your RT tool at <code>http://127.0.0.1:8000/mcp</code> for any MCP client.</p>"},{"location":"tools_mcp/mcp/RTtoMCP/#2-accessing-your-mcp-tools","title":"2. Accessing Your MCP Tools","text":"<p>Any MCP-compatible client or LLM agent can now discover and invoke your tool. As an example, you can use R C itself to try your tool:</p> <pre><code>from railtracks.nodes.library.mcp_tool import from_mcp_server\nfrom railtracks.integrations.rt_mcp.main import MCPHttpParams\n\nserver = from_mcp_server(MCPHttpParams(url=\"http://127.0.0.1:8000/mcp\"))\ntools = server.tools\n</code></pre>"},{"location":"tools_mcp/mcp/RTtoMCP/#advanced-topics","title":"Advanced Topics","text":"<ul> <li>Multiple Tools: Pass a list of Node classes to <code>create_mcp_server</code> to expose several tools.</li> <li>Transport Options: Use <code>streamable-http</code>, <code>sse</code>, or <code>stdio</code> as needed.</li> </ul>"},{"location":"tools_mcp/mcp/RTtoMCP/#related-topics","title":"Related Topics","text":"<ul> <li>What is MCP?</li> <li>Using MCP Tools in RT</li> </ul>"},{"location":"tools_mcp/mcp/mcp/","title":"\ud83d\udd0c Model Context Protocol (MCP)","text":""},{"location":"tools_mcp/mcp/mcp/#what-is-model-context-protocol-mcp","title":"\u2753 What is Model Context Protocol (MCP)?","text":"<p>MCP in a Nutshell</p> <p>Model Context Protocol (MCP) is an open standard that enables seamless integration between Large Language Models (LLMs) and external systems, including applications, data sources, and tools.</p> <p>Model Context Protocol (MCP) provides a unified interface, making it easy for LLMs to access context, perform actions, and interact with diverse environments. It standardizes how AI models communicate with external tools and services, similar to how REST APIs standardized web service communication.</p>"},{"location":"tools_mcp/mcp/mcp/#key-benefits-of-mcp","title":"\ud83c\udf1f Key Benefits of MCP","text":"<ul> <li>Standardized Integration: Connect to any MCP-compatible tool using the same interface</li> <li>Reduced Development Time: Use pre-built MCP tools instead of creating custom integrations</li> <li>Ecosystem Compatibility: Access a growing ecosystem of MCP-compatible tools and services</li> <li>Simplified Architecture: Uniform approach to tool integration reduces complexity</li> </ul>"},{"location":"tools_mcp/mcp/mcp/#using-mcp-tools-in-railtracks","title":"\ud83d\udd27 Using MCP Tools in RailTracks","text":"<p>RailTracks allows you to convert MCP tools into Tools that can be used by RailTracks agents just like any other Tool. We handle the conversion and server setup for you, so you can focus on building your agents without worrying about the underlying complexities of MCP.</p> <p>Quick Example</p> <pre><code>from railtracks.nodes.library import from_mcp_server\nfrom railtracks.rt_mcp import MCPHttpParams\n\n# Connect to a remote MCP server\nserver = from_mcp_server(MCPHttpParams(url=\"https://remote.mcpservers.org/fetch/mcp\"))\n\n# Get all available tools from the server\ntools = server.tools\n\n# Use these tools with your RailTracks agents\n</code></pre> <p>For a complete guide and more examples, see Using MCP Tools in RailTracks.</p>"},{"location":"tools_mcp/mcp/mcp/#railtracks-to-mcp","title":"\ud83d\udd04 RailTracks to MCP","text":"<p>RailTracks also provides a way to convert RailTracks Tools into MCP tools using FastMCP, allowing you to use your existing RailTracks tools in any MCP-compatible environment.</p> <p>This enables you to:</p> <ul> <li>Share your custom tools with the broader MCP ecosystem</li> <li>Use your tools in other MCP-compatible frameworks</li> <li>Create a unified toolset across different AI systems</li> </ul> <p>See the RailTracks to MCP page for more details on how to set this up.</p>"},{"location":"tools_mcp/mcp/mcp/#available-mcp-servers","title":"\ud83d\udccb Available MCP Servers","text":"<p>RailTracks supports pre-built integrations with various MCP servers, including:</p> MCP Server Description Setup Guide Websearch Retrieve and process content from URLs Guide GitHub Interact with GitHub repositories Guide Notion Create and manage Notion pages Guide Slack Send and receive Slack messages Guide"},{"location":"tools_mcp/tools/advanced_usage/","title":"\ud83e\udde0 Advanced Tooling","text":"<p>This page covers advanced patterns and techniques for working with tools in Railtracks, including tool behaviour encapsulation, conditional tool loading and complex agent compositions.</p>"},{"location":"tools_mcp/tools/advanced_usage/#tool-calling-agents-as-tools","title":"\ud83d\udd27 Tool Calling Agents as Tools","text":"<p>One powerful pattern is creating agents that can call multiple tools and then making those agents available as tools themselves. This creates a hierarchical structure where complex behaviors can be encapsulated and reused.</p> <p>Let's create a math calculator agent that has access to basic math operations and can be reused as a tool:</p> <pre><code>import railtracks as rt\nfrom railtracks.nodes.manifest import ToolManifest\nfrom railtracks.llm import Parameter\n\n# Define basic math operation tools\n@rt.function_node\ndef add(a: float, b: float) -&gt; float:\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\n@rt.function_node\ndef multiply(a: float, b: float) -&gt; float:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\n@rt.function_node\ndef divide(a: float, b: float) -&gt; float:\n    \"\"\"Divide two numbers.\"\"\"\n    if b == 0:\n        return \"Error: Cannot divide by zero\"\n    return a / b\n\n# Create the tool manifest for the calculator\ncalculator_manifest = ToolManifest(\n    description=\"A calculator agent that can perform mathematical calculations and solve math problems.\",\n    parameters=[\n        Parameter(name=\"math_problem\", description=\"The mathematical problem or calculation to solve.\", type=\"str\"),\n    ],\n)\n\n# Create the calculator agent\ncalculator_agent = rt.agent_node(\n    pretty_name=\"Calculator Agent\",\n    llm_model=rt.llm.OpenAILLM(\"gpt-4o\"),\n    system_message=\"You are a helpful calculator. Solve math problems step by step using the available math operations.\",\n    tool_nodes=[add, multiply, divide],\n    manifest=calculator_manifest,  # This makes the agent usable as a tool\n)\n</code></pre> <p>I. Invoking the Agent independently:</p> <p>This calculator can be invoked independently:</p> <pre><code># Invoke the calculator agent\nresult = rt.call_sync(calculator_agent, \"What is 3 + 4?\")\nprint(result.content)\n</code></pre> <p>II. Using the Agent as a Tool:</p> <p>This calculator can be used as a tool in other agents: <pre><code>import railtracks as rt\n\n@rt.function_node\ndef get_price_data(item: str) -&gt; dict:\n    \"\"\"Get pricing data for an item.\"\"\"\n    # Mock pricing data\n    prices = {\n        \"laptop\": {\"price\": 999.99, \"tax_rate\": 0.08},\n        \"phone\": {\"price\": 699.99, \"tax_rate\": 0.08},\n        \"tablet\": {\"price\": 449.99, \"tax_rate\": 0.08}\n    }\n    return prices.get(item, {\"price\": 0, \"tax_rate\": 0})\n\n# Create a shopping assistant that uses the calculator agent\nshopping_assistant = rt.agent_node(\n    pretty_name=\"Shopping Assistant\",\n    tool_nodes=[get_price_data, calculator_agent],  # Use the calculator agent as a tool\n    llm_model=rt.llm.OpenAILLM(\"gpt-4o\"),\n    system_message=\"You are a shopping assistant. Help users with pricing calculations including taxes, discounts, and totals.\"\n)\n\n# Demonstrate the agents working together\nresponse = rt.call_sync(\n    shopping_assistant,\n    \"I want to buy 3 laptops. Can you calculate the total cost including tax?\"\n)\nprint(response.content)\n</code></pre></p> <p>In this example:  1. The calculator agent encapsulates math operations and can solve complex calculations  2. The shopping assistant uses both the price lookup function and the calculator agent  3. When asked about laptop costs, it fetches the price data and delegates the math to the calculator agent </p>"},{"location":"tools_mcp/tools/advanced_usage/#conditional-tool-loading","title":"\ud83d\udd04 Conditional Tool Loading","text":"<p>For more advanced use cases, you might want to load tools dynamically based on runtime conditions or user preferences.</p> <pre><code>import railtracks as rt\nfrom typing import List, Any\n\ndef create_agent_with_conditional_tools(user_permissions: List[str]) -&gt; rt.Node:\n    \"\"\"Create an agent with tools based on user permissions.\"\"\"\n\n    available_tools = []\n\n    # Base tools available to everyone\n    @rt.function_node\n    def get_weather(location: str):\n        return f\"Weather in {location}: Sunny, 22\u00b0C\"\n\n    available_tools.append(get_weather)\n\n    # Admin-only tools\n    if \"admin\" in user_permissions:\n        @rt.function_node\n        def delete_user(user_id: str):\n            return f\"User {user_id} deleted\"\n\n        available_tools.append(delete_user)\n\n    # Premium user tools\n    if \"premium\" in user_permissions:\n        @rt.function_node\n        def advanced_analytics():\n            return \"Advanced analytics data...\"\n\n        available_tools.append(advanced_analytics)\n\n    return rt.agent_node(\n        pretty_name=\"Dynamic Agent\",\n        llm_model=rt.llm.OpenAILLM(\"gpt-4o\"),\n        system_message=\"You are a helpful assistant with access to various tools based on user permissions.\",\n        tool_nodes=available_tools\n    )\n\n# Usage\nadmin_agent = create_agent_with_conditional_tools([\"admin\", \"premium\"])\nbasic_agent = create_agent_with_conditional_tools([])\n</code></pre>"},{"location":"tools_mcp/tools/advanced_usage/#complex-agent-hierarchies","title":"\ud83c\udfd7\ufe0f Complex Agent Hierarchies","text":"<p>We can create sophisticated agent hierarchies where specialized agents handle specific domains.</p> <p><pre><code>import railtracks as rt\nfrom railtracks.nodes.manifest import ToolManifest\nfrom railtracks.llm import Parameter\n\n# Domain-specific agents\nemail_agent_manifest = ToolManifest(\n    description=\"Handles all email-related tasks\",\n    parameters=[\n        Parameter(name=\"action\", description=\"The email action to perform\", type=\"str\"),\n        Parameter(name=\"details\", description=\"Details for the email action\", type=\"str\"),\n    ],\n)\n\ncalendar_agent_manifest = ToolManifest(\n    description=\"Manages calendar and scheduling tasks\",\n    parameters=[\n        Parameter(name=\"action\", description=\"The calendar action to perform\", type=\"str\"),\n        Parameter(name=\"details\", description=\"Details for the calendar action\", type=\"str\"),\n    ],\n)\n</code></pre> Now we can create the specialized agents.  In this case we are assuming we have multiple tools defined:</p> <pre><code># Create specialized agents\nemail_agent = rt.agent_node(\n    pretty_name=\"Email Agent\",\n    llm_model=rt.llm.OpenAILLM(\"gpt-4o\"),\n    system_message=\"You specialize in email management and communication tasks.\",\n    tool_nodes=[send_email, ...],  # Assume send_email is defined\n    manifest=email_agent_manifest\n)\n\ncalendar_agent = rt.agent_node(\n    pretty_name=\"Calendar Agent\",\n    llm_model=rt.llm.OpenAILLM(\"gpt-4o\"),\n    system_message=\"You specialize in calendar and scheduling tasks.\",\n    tool_nodes=[...],  # Add calendar tools here\n    manifest=calendar_agent_manifest\n)\n</code></pre> <p>Now we can create the coordinator agent that combines these multi-domain agents:</p> <pre><code># Master coordinator agent\ncoordinator_agent = rt.agent_node(\n    pretty_name=\"Personal Assistant\",\n    llm_model=rt.llm.OpenAILLM(\"gpt-4o\"),\n    system_message=\"You are a personal assistant that coordinates with specialized agents to help users.\",\n    tool_nodes=[email_agent, calendar_agent]\n)\n</code></pre>"},{"location":"tools_mcp/tools/advanced_usage/#related","title":"\ud83d\udcda Related","text":"<ul> <li> <p>\ud83d\udd27 Agents as Tools    Learn the basics of using agents as tools.</p> </li> <li> <p>\ud83d\udd27 Functions as Tools    Learn how to turn Python functions into tools.</p> </li> <li> <p>\ud83d\udee0\ufe0f What are tools?    Understand the fundamental concepts of tools in Railtracks.</p> </li> <li> <p>\ud83e\udd16 How to build your first agent    Start with the basics of agent creation.</p> </li> </ul>"},{"location":"tools_mcp/tools/agents_as_tools/","title":"\ud83d\udd27 Agents as Tools","text":"<p>In Railtracks, you can use any <code>Agent Node</code> as a tool that other agents can have access to. This allows you to create complex agents that can be composed of smaller, reusable components. </p> <p>What are Nodes?</p> <p>Nodes are the building blocks of Railtracks. They are responsible for executing a single task and returning a result. Read more about Nodes here.</p> <p>How to build an Agent?</p> <p>Read more about how to build an agent here.</p>"},{"location":"tools_mcp/tools/agents_as_tools/#understanding-toolmanifest","title":"\ud83d\udccb Understanding ToolManifest","text":"<p>Before diving into examples, it's important to understand what a <code>ToolManifest</code> is and why it's essential when creating agents that can be used as tools.</p> <p>A <code>ToolManifest</code> is a specification that describes how an agent should be used when called as a tool by other agents. It defines:</p> <ul> <li>Description: What the tool does and how it should be used</li> <li>Parameters: What inputs the tool expects, including their names, types, and descriptions</li> </ul> <pre><code>class ToolManifest:\n    \"\"\"\n    Creates a manifest for a tool, which includes its description and parameters.\n\n    Args:\n        description (str): A description of the tool.\n        parameters (Iterable[Parameter] | None): An iterable of parameters for the tool. If None, there are no parameters.\n    \"\"\"\n</code></pre> <p>The <code>ToolManifest</code> acts as a contract that tells other agents exactly how to interact with your agent when using it as a tool. Without it, other agents wouldn't know what parameters to pass or what to expect from your agent.</p>"},{"location":"tools_mcp/tools/agents_as_tools/#using-agents-as-tools","title":"\u2699\ufe0f Using Agents as Tools","text":""},{"location":"tools_mcp/tools/agents_as_tools/#basic-example-research-assistant","title":"\ud83c\udf5e Basic Example: Research Assistant","text":"<p>Let's create a research assistant that combines a function tool (for fetching articles) with an agent tool (for summarizing them):</p> <pre><code>import railtracks as rt\nfrom railtracks.nodes.manifest import ToolManifest\nfrom railtracks.llm import Parameter\n\n# First, create a function tool that fetches articles\n@rt.function_node\ndef fetch_article(url: str) -&gt; str:\n    \"\"\"Fetch the content of an article from a URL.\"\"\"\n    # In a real implementation, this would fetch and parse the article\n    return \"Mock response for fetch_article\"\n\n# Next, create an agent tool for summarizing content\nsummary_agent_manifest = ToolManifest(\n    description=\"A specialized agent that creates concise summaries of long texts.\",\n    parameters=[\n        Parameter(name=\"text\", description=\"The text content to summarize.\", type=\"str\"),\n        Parameter(name=\"length\", description=\"Desired summary length: 'short', 'medium', or 'long'.\", type=\"str\"),\n    ],\n)\n\nsummary_agent = rt.agent_node(\n    pretty_name=\"Summary Agent\",\n    llm_model=rt.llm.OpenAILLM(\"gpt-4o\"),\n    system_message=\"You are an expert at creating clear, concise summaries. Always preserve the key points while making the content accessible.\",\n    manifest=summary_agent_manifest,    # This makes the agent usable as a tool\n)\n\n# Now create the main research assistant that uses both tools\nresearch_assistant = rt.agent_node(\n    pretty_name=\"Research Assistant\",\n    llm_model=rt.llm.OpenAILLM(\"gpt-4o\"),\n    system_message=\"You are a research assistant. When users ask about articles, fetch them and provide summaries.\",\n    tool_nodes=[fetch_article, summary_agent],  # Both function and agent as tools\n)\n\n# Demonstrate the tools working together\nresponse = rt.call_sync(\n    research_assistant,\n    \"Please fetch the article from 'https://example.com/ai-trends' and give me a short summary of the key points.\"\n)\nprint(response.content)\n</code></pre> <p>In this example:  1. The function tool (<code>fetch_article</code>) retrieves raw article content.  2. The agent tool (<code>summary_agent</code>) intelligently processes that content into a summary.  3. The main research assistant orchestrates both tools to complete the user's request. </p>"},{"location":"tools_mcp/tools/agents_as_tools/#advanced-example-content-manager-with-notion-integration","title":"\ud83e\udd6a Advanced Example: Content Manager with Notion Integration","text":"<p>Let's create a more sophisticated example that combines data processing with Notion document creation using MCP tools:</p> <pre><code>import railtracks as rt\n\n# ================ TOOL 1 - Function as a Tool ===============\n# First, create a function tool for data analysis\n@rt.function_node\ndef analyze_sales_data(month: str, year: int) -&gt; dict:\n    \"\"\"Analyze sales data for a given month and year.\n    Args:\n        month (str): The month to analyze.\n        year (int): The year to analyze.\n    \"\"\"\n    # In a real implementation, this would query a database\n    return {\"sample_data\": 42}\n</code></pre> <p>MCP Integration</p> <p>Find a detailed example of how to set up the Notion MCP in the Notion Integration page. Learn more about MCP architecture in our MCP Guide.</p> <p><pre><code>from railtracks.nodes.manifest import ToolManifest\nfrom railtracks.llm import Parameter\n\n# ================ TOOL 2 - Agent as a Tool ===============\n# Get tools from Notion MCP (see Notion Integration guide for details)\ntools = server.tools    # from the Notion integration Tutorial\n\n# Create a Notion agent that can handle documentation tasks\nnotion_agent_manifest = ToolManifest(\n    description=\"A Notion agent that can create pages, databases, and manage content in Notion workspaces.\",\n    parameters=[\n        Parameter(name=\"task_description\", description=\"Detailed description of the Notion task to perform.\", type=\"str\"),\n        Parameter(name=\"content_data\", description=\"Any data or content to include (as JSON string if structured data).\", type=\"str\"),\n    ],\n)\n\nnotion_agent = rt.agent_node(\n    pretty_name=\"Notion Agent\",\n    tool_nodes=tools,  # MCP tools for Notion integration\n    llm_model=rt.llm.OpenAILLM(\"gpt-4o\"),\n    system_message=\"You are a Notion specialist who can create pages, update databases, manage content, and organize information in Notion workspaces.\",\n    manifest=notion_agent_manifest,\n)\n</code></pre>  Now we can create the main content manager that uses both tools: <pre><code># Create the main content manager that orchestrates both tools\ncontent_manager = rt.agent_node(\n    pretty_name=\"Content Manager\",\n    llm_model=rt.llm.OpenAILLM(\"gpt-4o\"),\n    system_message=\"You are a content manager who creates business reports and documentation. You analyze data and organize it in Notion.\",\n    tool_nodes=[analyze_sales_data, notion_agent],\n)\n\n# Demonstrate the tools working together\nresponse = rt.call_sync(\n    content_manager,\n    \"Please analyze our sales data for March 2024 and create a comprehensive report in Notion. Create a new page called 'Q1 Sales Performance Review' with the analysis results, charts, and key insights organized in a professional format.\"\n)\nprint(response.content)\n</code></pre></p> <p>In this example:  1. The function tool (<code>analyze_sales_data</code>) processes and retrieves business data from various sources  2. The Notion agent tool uses MCP tools to create pages, format content, add databases, insert charts, and organize information in Notion  3. The content manager coordinates both tools to create comprehensive business documentation </p> <p>This pattern showcases the power of combining simple data processing functions with sophisticated MCP-enabled agents that can interact with external platforms like Notion, Slack, GitHub, and more.</p>"},{"location":"tools_mcp/tools/agents_as_tools/#related","title":"\ud83d\udcda Related","text":"<p>Want to go further with tools in Railtracks?</p> <ul> <li> <p>\ud83d\udee0\ufe0f What are tools?    Learn how tools fit into the bigger picture of Railtracks and agent orchestration.</p> </li> <li> <p>\ud83d\udd27 Functions as Tools    Learn how to turn Python functions into tools.</p> </li> <li> <p>\ud83e\udd16 How to build your first agent    Start with the basics of agent creation.</p> </li> <li> <p>\ud83e\udde0 Advanced Tooling    Explore dynamic tool loading, runtime validation, and other advanced patterns including tool calling agents.</p> </li> </ul>"},{"location":"tools_mcp/tools/functions_as_tools/","title":"\ud83d\udd27 Functions as Tools","text":"<p>In Railtracks, you can turn any Python function into a tool that agents can call\u2014no special boilerplate needed. The key is to provide a Google-style docstring, which acts as the tool's description and schema.  </p> <p>Function Nodes</p> <p><code>rt.function_node</code> is a convenience function that wraps a function into a Railtrack node. Read more about this DynamicFunctionNode.</p>"},{"location":"tools_mcp/tools/functions_as_tools/#creating-a-function-tool","title":"\u2699\ufe0f Creating a Function Tool","text":""},{"location":"tools_mcp/tools/functions_as_tools/#1-using-an-rt-function","title":"1. Using an RT Function","text":"<p>Let's start with a simple function that takes two arguments and returns their sum:</p> <pre><code>def add(a: int, b: int) -&gt; int:\n    \"\"\"\n    Adds two numbers together.\n    Args:\n        a (int): The first number.\n        b (int): The second number.\n\n    Returns:\n        int: The sum of the two numbers.\n    \"\"\"\n    return a + b\n</code></pre> <p>To turn this function into a tool, we need to provide a docstring that describes the function's parameters. Then we can pass the function to <code>rt.function_node</code> to create a tool:</p> <pre><code>import railtracks as rt\n\nadd_tool_node = rt.function_node(add)\n</code></pre>"},{"location":"tools_mcp/tools/functions_as_tools/#2-using-a-decorator","title":"2. Using a decorator","text":"<p>Let's make another tool that we can use in our agent, this time using the <code>@rt.function_node</code> decorator:</p> <pre><code>from sympy import solve, sympify\nimport railtracks as rt\n\n@rt.function_node\ndef solve_expression(equation: str, solving_for: str):\n    \"\"\"\n    Solves the given equation (assumed to be equal to 0) for the specified variable.\n\n    Args:\n        equation (str): The equation to solve, written in valid Python syntax.\n        solving_for (str): The variable to solve for.\n    \"\"\"\n    # Convert the string into a symbolic expression\n    eq = sympify(equation, evaluate=True)\n\n    # Solve the equation for the given variable\n    return solve(eq, solving_for)\n</code></pre>"},{"location":"tools_mcp/tools/functions_as_tools/#using-the-tools","title":"\ud83d\udd2e Using the tools","text":"<p>Now that we have our tool, we can use it in our agent:</p> <pre><code>import railtracks as rt\n\n# Create an agent with tool access\nmath_agent = rt.agent_node(\n                pretty_name=\"MathAgent\",\n                tool_nodes=[\n                  solve_expression, \n                  add_tool_node,\n                ],    # the agent has access to these tools\n                llm_model = rt.llm.OpenAILLM(\"gpt-4o\"),\n            )\n\n# run the agent\nresult = rt.call_sync(math_agent, \"What is 3 + 4?\")\n</code></pre>"},{"location":"tools_mcp/tools/functions_as_tools/#related","title":"\ud83d\udcda Related","text":"<p>Want to go further with tools in Railtracks?</p> <ul> <li> <p>\ud83d\udee0\ufe0f What are tools?    Learn how tools fit into the bigger picture of Railtracks and agent orchestration.</p> </li> <li> <p>\ud83e\udd16 How to build your first agent    Follow along with a tutorial to build your first agent.</p> </li> <li> <p>\ud83d\udd27 Agents as Tools    Discover how you can turn entire agents into callable tools inside other agents.</p> </li> <li> <p>\ud83e\udde0 Advanced Tooling    Explore dynamic tool loading, runtime validation, and other advanced patterns.</p> </li> </ul>"},{"location":"tools_mcp/tools/tools/","title":"\ud83d\udee0\ufe0f Tools","text":"<p>In Railtracks, tools are capabilities that agents can call\u2014functions, other agents, or external systems. This makes them composable, flexible, and powerful in multi-agent workflows.</p> <p>Whether you're looking to expose your Python functions to agents, wrap other agents as tools, or explore advanced use cases like dynamic tool routing and function schemas\u2014this section has you covered.</p>"},{"location":"tools_mcp/tools/tools/#contents","title":"\ud83d\udcda Contents","text":"Page Description \ud83d\udd27 Functions as Tools Make Python functions callable by agents. \ud83e\udd16 Agents as Tools Let one agent act as a tool for another, enabling nested orchestration. \ud83e\udde0 Advanced Tooling Explore tool behaviour encapsulation, conditional tool loading and advanced integration patterns."},{"location":"tutorials/byfa/","title":"How to Build Your First Agent","text":"<p>RailTracks allows you to easily create custom agents using the <code>define_agent</code> function by configuring a few simple parameters in any combination!</p> <p>Start by specifying:</p> <ul> <li><code>llm_model</code>: Choose which LLM the agent will use.</li> <li><code>system_message</code>: Define the agent\u2019s behavior. This guides the agent and often improves output quality. (See also: Prompt Engineering)</li> </ul> <p>Then, configure your agent class by selecting which functionalities to enable:</p> <ul> <li><code>tools</code>: If you pass this parameter, the agent gains access to the specified tools. If you don't it will act as a conversational agent instead.</li> <li><code>schema</code>: Given a schema, the agents responses will follow that schema. Otherwise it will return output as it sees fit.</li> </ul> <p>Optionally, you can define attributes for using the agent as a tool itself and debugging:</p> <ul> <li><code>agent_name</code>: The identifier used when referencing the agent while debugging.</li> <li><code>agent_params</code>: Parameters required when the agent is called as a tool.</li> <li><code>agent_doc</code>: A short explanation of what the agent does \u2014 this helps other LLMs decide when and how to use it.</li> </ul> <p>For advanced users you can see context, for further configurability.</p>"},{"location":"tutorials/byfa/#example","title":"Example","text":"<pre><code>weather_agent_class = rt.define_agent(\n    agent_name=\"Weather Agent\",\n    llm_model=\"gpt-4o\",\n    system_message=\"You are a helpful assistant that answers weather-related questions.\",\n    tools={weather_tool},\n    schema=weather_schema,\n    agent_params=weather_param,\n    agent_doc=\"This is an agent that will give you the current weather and answer weather related questions you have\"    \n)\n</code></pre>"},{"location":"tutorials/byfa/#tool-calling-agents","title":"Tool-Calling Agents","text":"<p>Tool-calling agents can invoke one or more tools during a conversation. This allows them to take actions that conventional LLM's cannot.</p> <p>When making a Tool-Calling Agent you can also specify <code>max_tool_calls</code> to have a safety net for your agents calls. If you don't specify <code>max_tool_calls</code>, your agent will be able to make as many tool calls as it sees fit.</p>"},{"location":"tutorials/byfa/#example_1","title":"Example","text":"<pre><code>weather_agent_class = rt.define_agent(\n    agent_name=\"Weather Agent\",\n    llm_model=\"gpt-4o\",\n    system_message=\"You are a helpful assistant that answers weather-related questions.\",\n    tools=weather_tool_set,\n    maximum_tool_calls=10\n)\n</code></pre> <p>Additionally, we have an MCP agent if you would like integrate API functionalities as tools your agent can use directly. See Using MCP for more details.</p>"},{"location":"tutorials/byfa/#example_2","title":"Example","text":"<pre><code>notion_agent_class = rt.define_agent(\n    agent_name=\"Notion Agent\",\n    mcp_command: notion_command,\n    mcp_args: notion_args,\n    mcp_env: notion_env,\n    llm_model=\"gpt-4o\",\n    system_message=\"You are a helpful assistant that help edit users Notion pages\",\n\n)\n</code></pre>"},{"location":"tutorials/byfa/#structured-agents","title":"Structured Agents","text":"<p>Structured agents are built to return output that conforms to a consistent schema (Currently we only support Pydantic models). This is especially useful for:</p> <ul> <li>Parsing responses programmatically</li> <li>Integrating with downstream processes</li> <li>Enforcing predictable structure for evaluation or validation</li> </ul> <p>Define the schema or expected structure when initializing the agent so the model can reliably adhere to it.</p> <p> Create Your Own Agent Using Context </p>"},{"location":"tutorials/node_builder/","title":"How to Customize Class Building","text":"<p>RailTracks <code>define_agent</code> allows user to configure agent classes with set parameters you can choose from. These parameters suffice for most cases but what if you want to make an agent with just slightly more functionality? RailTracks has you covered again! Using our NodeBuilder class you can create your own agent class with many of the same functionalities provided in <code>define_agent</code> but the option to add class methods and attributes of your choice. </p> <p>NodeBuilder is sort of the subway of agents, you get to choose what you do and don't want your nodes to have. This will be important to keep in mind as every line will be a choice changing the node factory that you're building.</p>"},{"location":"tutorials/node_builder/#initializing-nodebuilder","title":"Initializing NodeBuilder","text":"<p>To initialize your NodeBuilder you will first choose which node class you would like to inherit from. Currently we have Terminal, Structured, ToolCall, and StructuredToolCall as the most common classes to inherit from. If you need more complexity you will need to inherit from ancestors to the more common classes. This is not advised unless you are an expert in the framework, in which case would you like a job at Railtown.ai? Once you have chosen the class to inherit from you will choose a name (helpful for debugging), a class name, and advanced parameters for if you would like certain results directly returned to context.</p>"},{"location":"tutorials/node_builder/#creating-an-llm-node","title":"Creating an LLM Node","text":"<p>Although most of the time you are going to be creating nodes that use an LLM, this is not required and you can use NodeBuilder to build a totally different node. This is not advised though as we have function nodes which should be used instead. If you know about function nodes but choose to use NodeBuilder without calling <code>llm_base</code>, please consider applying at railtown.ai.</p> <p>So the first method you will be calling is <code>llm_base</code> where you will specify the system message and the llm you would like the node to use.</p>"},{"location":"tutorials/node_builder/#choosing-functionality","title":"Choosing Functionality","text":"<p>As is the case with our agent_node function, you will be able to choose the tools, max tool calls allowed, the structure of the output, and tool details. As usual, all of these are not required, unless you inherit from a class that requires certain attributes.</p>"},{"location":"tutorials/node_builder/#building","title":"Building!","text":"<p>Finally, you can build your node using the build method and you have your first custom made node!</p>"},{"location":"tutorials/node_builder/#example-of-building-a-structured-tool-call-llm","title":"Example of building a Structured Tool Call LLM","text":""},{"location":"tutorials/node_builder/#builder-nodebuilderstructuredtoolcallllm_toutput-structuredtoolcallllm-namename-class_nameeasystructuredtoolcallllm-return_intoreturn_into-format_for_returnformat_for_return-format_for_contextformat_for_context-builderllm_basellm_model-system_message-buildertool_calling_llmsettool_nodes-max_tool_calls-buildertool_callable_llmtool_details-tool_params-builderstructuredoutput_schema-node-builderbuild","title":"<pre><code>builder = NodeBuilder[StructuredToolCallLLM[_TOutput]](\n        StructuredToolCallLLM,\n        name=name,\n        class_name=\"EasyStructuredToolCallLLM\",\n        return_into=return_into,\n        format_for_return=format_for_return,\n        format_for_context=format_for_context,\n    )\n\nbuilder.llm_base(llm_model, system_message)\nbuilder.tool_calling_llm(set(tool_nodes), max_tool_calls)\nbuilder.tool_callable_llm(tool_details, tool_params)\nbuilder.structured(output_schema)\n\nnode = builder.build()\n</code></pre>","text":""},{"location":"tutorials/node_builder/#advanced-usage","title":"Advanced Usage","text":"<p>If you are an advanced user and would like to add further functionality to your node we have you covered!(Please apply to railtown.ai) By using our <code>add_attribute</code> method, you can add class attributes and methods that are not currently covered by RailTracks.</p> <pre><code>\n</code></pre> <pre><code>#This is the add_attribute method we would have in NodeBuilder\ndef add_attribute(self, **kwargs):\n\n        for key, val in kwargs.items():\n            if callable(val):\n                self._with_override(key, classmethod(val))\n            else:\n                self._with_override(key, val)\n</code></pre> <pre><code>#What our easywrapper classes would look like now\ndef anything_LLM_Base_Wrapper(\n    name,\n    ...\n    return_onto_into_and_possibly_nearby_context,\n    **kwargs\n):\n    builder = NodeBuilder(...)\n    ...\n    add_attribute(**kwargs) #We add this one line to all classes. Maybe put in build() to help DRY\n    builder.build()\n</code></pre> <pre><code>#What using this would look like with one_wrapper\ndef chat_ui(\n        self,\n        chat_ui: ChatUI,\n    ):\n\n        chat_ui.start_server_async()\n        self._with_override(\"chat_ui\", chat_ui)\n\n\nasync def new_invoke(self):  # noqa: C901\n        # If there's no last user message, we need to wait for user input\n        if self.message_hist[-1].role != Role.user:\n            msg = await self.chat_ui.wait_for_user_input()\n            if msg == \"EXIT\":\n                return self.return_output()\n            self.message_hist.append(\n                UserMessage(\n                    msg,\n                )\n            )\n\n        ...\n\n            else:\n                # the message is malformed from the model\n                raise LLMError(\n                    reason=\"ModelLLM returned an unexpected message type.\",\n                    message_history=self.message_hist,\n                )\n\n        return self.return_output()\n\ndef new_return_output(self):\n    \"\"\"Returns the message history\"\"\"\n    return self.message_hist\n\n\nagent_chat = agent_node(\n    name=\"cool_agent\"\n    ...\n    chat_ui=chat_ui,\n    return_ouput=new_return_output,\n    invoke=new_invoke\n)\n</code></pre>"},{"location":"tutorials/ryfa/","title":"How to Run Your First Agent","text":"<p>Once you have defined your agent class you can then run your work flow and see results!</p> <p>To begin you just have to use <code>call</code> for asynchronous flows or <code>call_sync</code> if it's s sequential flow. You simply pass your agent node as a parameter as well as the prompt as <code>user_input</code>:</p>"},{"location":"tutorials/ryfa/#example","title":"Example","text":"<pre><code>response = rt.call(\n    weather_agent_class,\n    user_input=\"Would you please be able to tell me the forecast for the next week?\"\n)\n</code></pre> <p>Just like that you have ran your first agent!</p>"},{"location":"tutorials/ryfa/#customization-and-configurability","title":"Customization and Configurability","text":"<p>Although it really is that simple to run your agent, you can do more of course. If you have a dynamic work flow you can delay parameters like <code>llm_model</code> and you can add a <code>SystemMessage</code> along with your prompt directly to <code>user_input</code> as a <code>MessageHistory</code> object.</p>"},{"location":"tutorials/ryfa/#example_1","title":"Example","text":"<pre><code>weather_agent_class = rt.agent_node(\n    tool_nodes={weather_tool},\n    output_schema=weather_schema, \n)\n\nsystem_message = rt.message.SystemMessage(\"You are a helpful assistant that answers weather-related questions.\")\nuser_message = rt.message.UserMessage(\"Would you please be able to tell me the forecast for the next week?\")\n\nresponse = rt.call(\n    weather_agent_class,\n    user_input=MessageHistory([system_message, user_message]),\n    llm_model='claude-3-5-sonnet-20240620',\n)\n</code></pre> <p>Should you pass <code>llm_model</code> to <code>agent_node</code> and then a different llm model to either call function, RailTracks will use the parameter passed in the call. If you pass <code>system_message</code> to <code>agent_node</code> and then another <code>system_message</code> to a call function, the system messages will be stacked.</p>"},{"location":"tutorials/ryfa/#example_2","title":"Example","text":"<p><pre><code>default_model = \"gpt-40\"\ndefault_system_message = \"You are a helpful assistant that answers weather-related questions.\"\n\nweather_agent_class = rt.agent_node(\n    tool_nodes={weather_tool},\n    system_message=default_system_message,\n    llm_model=default_model,\n)\n\nsystem_message = rt.message.SystemMessage(\"If not specified, the user is talking about Vancouver.\")\nuser_message = rt.message.UserMessage(\"Would you please be able to tell me the forecast for the next week?\")\n\nresponse = rt.call(\n    weather_agent_class,\n    user_input=MessageHistory([system_message, user_message]),\n    llm_model='claude-3-5-sonnet-20240620',\n)\n</code></pre> In this example RailTracks will use claude rather than chatgpt and the system message will become \"You are a helpful assistant that answers weather-related questions. If not specified, the user is talking about Vancouver.\"</p>"},{"location":"tutorials/ryfa/#retrieving-the-results-of-a-run","title":"Retrieving The Results of a Run","text":"<p>All agents return a response object which you can use to get the last message or the entire message history if you would prefer.</p>"},{"location":"tutorials/ryfa/#unstructured-response-example","title":"Unstructured Response Example","text":"<pre><code>coding_agent_node = rt.agent_node()\n\nsystem_message = rt.message.SystemMessage(\"You are an assistant that helps users write code and learn about coding.\")\nuser_message = rt.message.UserMessage(\"Would you be able to help me figure out a good solution to running agentic flows?\")\n\nresponse = rt.call(\n    coding_agent_node,\n    user_input=MessageHistory([system_message, user_message]),\n    llm_model='claude-3-5-sonnet-20240620',\n)\n\nanswer_string = response.text()\nmessage_history_object = response.message_history\n</code></pre>"},{"location":"tutorials/ryfa/#structured-response-example","title":"Structured Response Example","text":"<pre><code>from pydantic import BaseModel\n\nclass User(BaseModel):\n    user_number : int\n    age: int\n    name: str\n\nagent_node = rt.agent_node(\n    output_schema=User,\n    system_message=agent_message,\n    llm_model=\"gpt-4o\"\n)\n\nresponse = rt.call(\n    agent_class,\n    user_input=input_str\n)\n\nuser_number = response.structured().user_number\nmessage_history_object = response.message_history\n</code></pre>"},{"location":"tutorials/ryfa/#advanced-usage-examples","title":"Advanced Usage Examples","text":""},{"location":"tutorials/ryfa/#deciding-precise-run-time-calls","title":"Deciding Precise Run Time Calls","text":"<pre><code>class stocks(BaseModel):\n    action : str \n\ndef find_stocks(...):\n    ...\n\ndef own_stocks(...):\n    ...\n\ndef analyze_stock(...):\n    ...\n\ndef buy_stocks(...):\n    ...\n\ndef sell_stocks(...):\n    ...\n\nscoutNode = agent_node(\n    name=\"Scout Node\",\n    system_message=\"\"\"You are a trading agent that helps find the best stocks to buy and sell. To find stocks you can use the find_stocks tool,\n    to see which stocks you own you can use own_stocks tool, and to analyze\"\"\" \n)\nbuyingNode = function_node(buy_stocks)\nsellingNode = function_node(sell_stocks)\n\nwhile stillHaveMoney:\n    stocks = rt.call(scoutNode)\n\n    if stocks.structure[action] = \"buy\" \n</code></pre>"},{"location":"tutorials/ryfa/#the-power-of-railtracks","title":"The power of RailTracks","text":"<pre><code>class ShortAndLongSummaries(BaseModel):\n    OneLineSummarry: str = Field(description=\"One line summary of the commit\")\n    DetailedSummary: str = Field(description=\"Detailed summary of the commit\")\n\nsystem_prompt = \"\"\"You are an expert software developer and manager. Your job is to look at developers' code updates, and summarize what was done in a brief but understable way. \n                        Always provide only the summary - no acknowledgements or greetings. You are to generate a brief one-line summary as well as a longer more detailed one. \n                        Be sure to focus on the function or purpose of the changes made rather than the minutiae of the code changes. If the diff is short, your detailed summary can be brief.\"\"\"\n\n\nCommitSummarizerNode = agent_node(\n    output_schema=ShortAndLongSummaries,\n    system_message=system_prompt)\n\n\nasync def top_level_summarizer(Summarize, commit_diffs, prompt_data) -&gt; list[dict]: \n    \"\"\"\n    Top level node for the commit summarizer\n    \"\"\"\n    contracts = [rc.call(Summarize, commit_diff, prompt_data) for commit_diff in commit_diffs]\n\n    results = await asyncio.gather(*contracts)\n    return results\n\nasync def summarize(commit: types.CommitData, prompt_data: dict) -&gt; dict:\n    \"\"\"\n    Summarize a commit using the provided prompt data\n    \"\"\"\n    full_diff = combine_file_diffs(commit.FileDiffs)\n    user_prompt = prompt_data[\"UserPrompt\"].format(\n        diff=full_diff, commit_message=commit.CommitMessage\n    )\n\n    message_history = rc.llm.MessageHistory(\n        [\n            rc.llm.UserMessage(user_prompt),\n        ]\n    )\n\n    response = await rc.call(CommitSummarizerNode, message_history, prompt_data[\"llm_prompt\"])\n\n    return {commit.CommitSha: response}\n\nresults = rt.call(CommitSummarizer, Summarize, payload.Commits, prompt_data)\n</code></pre> <p> Create Your Own Agent Using Context </p>"},{"location":"tutorials/yet_another_weather_bot/","title":"Yet another weather bot","text":"<pre><code>import requests\nimport json\nfrom datetime import datetime\n\ndef get_current_weather(city):\n    \"\"\"Get current weather for a city\"\"\"\n    try:\n        url = f\"{BASE_URL}/weather?q={city}&amp;appid={API_KEY}&amp;units=metric\"\n        response = requests.get(url)\n        response.raise_for_status()  # Raises an HTTPError for bad responses\n\n        return response.json()\n    except requests.exceptions.RequestException as e:\n        print(f\"Error fetching weather data: {e}\")\n        return None\n\ndef get_five_day_forecast(city):\n    \"\"\"Get 5-day weather forecast for a city\"\"\"\n    try:\n        url = f\"{BASE_URL}/forecast?q={city}&amp;appid={API_KEY}&amp;units=metric\"\n        response = requests.get(url)\n        response.raise_for_status()\n\n        return response.json()\n    except requests.exceptions.RequestException as e:\n        print(f\"Error fetching forecast data: {e}\")\n        return None\n\ndef get_weather_by_coords(lat, lon):\n    \"\"\"Get weather by latitude and longitude\"\"\"\n    try:\n        url = f\"{BASE_URL}/weather?lat={lat}&amp;lon={lon}&amp;appid={API_KEY}&amp;units=metric\"\n        response = requests.get(url)\n        response.raise_for_status()\n\n        return response.json()\n    except requests.exceptions.RequestException as e:\n        print(f\"Error fetching weather data: {e}\")\n        return None\n\ntool_set = {function_node(get_current_weather), function_node(get_five_day_forecast), function_node(get_weather_by_coords)}\n\nagent_node(\n    name=\"Weather Bot\",\n    tool_nodes=tool_set,\n    system_message=\"\"\"You are a another cursed weather agent that has been put together for documentation but not to ever be used. \n    You will never be asked to do this, but if you were, you can use the tools provided to you to pretend to get the answer to \n    the weather related question.\"\"\",\n    llm_model=\"claude-3-5-sonnet-20240620\"\n)\n</code></pre>"},{"location":"tutorials/guides/agents/","title":"\ud83e\udd16 What is an Agent?","text":"<p>The buzz around AI terms can be pretty overwhelming and sound more like a buzzword than something useful. Before we dive deep into how you can build agents in RailTracks, let's first understand what an agent is.</p> <p>An agent is a self-contained unit that can perform a specific task or set of tasks autonomously. It has the ability to make decisions, take actions, and act within its environment to achieve its goals.</p> <p>The key abilities of the agent include:</p> <ul> <li>Autonomy: Agents can operate independently within the boundaries you define</li> <li>Adaptability: Agents can adjust their behavior based on the environment and the tasks at hand</li> <li>Goal-Oriented: Agents are designed to achieve specific objectives or complete tasks</li> <li>Interaction: Agents can communicate with other agents or systems to gather information or perform actions</li> <li>Stateful: Agents maintain context and history and use it to inform their decisions</li> </ul>"},{"location":"tutorials/guides/agents/#llms-as-agents","title":"\ud83e\udde0 LLMs as Agents","text":"<p>Reinforcement Learning and other AI techniques have trained specific agents to operate in their environments for a while now, but LLMs have changed the game and made it much easier to use their generalized intelligence to accomplish complex tasks and goals. This ability makes them uniquely suited to operate as the brain for your agentic system.</p> <p>(TODO diagram here.)</p>"},{"location":"tutorials/guides/agents/#real-world-applications","title":"\ud83c\udf0d Real World Applications","text":"<p>Agents are already being used in real world applications such as:</p> <ol> <li>\ud83d\udcbb Vibe Coding Tools (Cursor, Windsurf, etc.)</li> <li>\ud83e\uddd1 NPC Interactions in Games (AI Village)</li> <li>\ud83d\udd8a\ufe0f Technical Documentation Writing (ParagraphAI)</li> <li>\ud83d\udef0\ufe0f Deep Research Tools (GPT Deep Research)</li> </ol>"},{"location":"tutorials/guides/agents/#related-topics","title":"\ud83d\udcda Related Topics","text":"<ul> <li>Tools</li> </ul>"},{"location":"tutorials/guides/agents/#build-your-own","title":"Build Your Own","text":"<p>You don't need to be a rocket scientist to build your own agent. With just a simple prompt and a bit of Python, you\u2019re already well on your way to building your first agent. applications.</p> Start Building with RailTracks"},{"location":"tutorials/guides/tools/","title":"\ud83d\udee0\ufe0f Tools","text":"<p>One of the most important parts of any agentic system is the set of tools that the agent can use.</p> <p>While an LLM on its own can understand and generate language, it's the ability to use tools that transforms it from a chatbot into a true agent capable of taking action, solving problems, and achieving real-world goals.</p>"},{"location":"tutorials/guides/tools/#what-is-a-tool","title":"\ud83e\udd16 What Is a Tool?","text":"<p>A tool is an action the agent can take in the external world.</p> <p>At a technical level, tools are often defined as functions that the agent can call. Large Language Models (LLMs) like OpenAI\u2019s or Anthropic\u2019s support something called <code>tool_calls</code> or <code>function_calls</code>. These allow the LLM to output structured <code>JSON</code>.</p> <p>Common Tools</p> <ul> <li>\ud83d\udd0d Search the web</li> <li>\ud83d\udda5\ufe0f Execute code in a sandbox</li> <li>\ud83d\udce8 Send a message to Slack</li> <li>\ud83d\udd17 Call a REST API</li> <li>\ud83d\udcc2 Read or write to a file system</li> </ul> <p>Think of tools as the hands and eyes of your agent: they allow it to act beyond just thinking and talking.</p>"},{"location":"tutorials/guides/tools/#example-connecting-tools-to-an-agent","title":"\u2699\ufe0f Example: Connecting Tools to an Agent","text":"<p>Here\u2019s a conceptual diagram of an LLM agent using tools to interact with different systems:</p> <pre><code>graph TD\n    A[LLM Agent] --&gt;|Tool Call| B[SlackTool]\n    A --&gt;|Tool Call| C[GitHubTool]\n    A --&gt;|Tool Call| D[CodeExecutionTool]\n    B --&gt; E[Send Message]\n    C --&gt; F[Create Comment]\n    C --&gt; G[List Issues]\n    D --&gt; H[Run Python Code]\n</code></pre> <p>Each tool defines:</p> <ul> <li>Name: the identifier used by the agent (e.g. <code>post_to_slack</code>)</li> <li>Description: a brief explanation of what the tool does (e.g. \"Posts a message to a Slack channel\")</li> <li>Parameters: the inputs the agent must provide (e.g. message, channel)</li> <li>Function: the logic the tool actually executes (e.g. send HTTP request)</li> </ul>"},{"location":"tutorials/guides/tools/#why-tools-matter","title":"\ud83d\udca1 Why Tools Matter","text":"<p>Without tools, LLMs are limited to just generating language.</p> <p>With tools, agents can:</p> <ul> <li>Automatically debug code</li> <li>File support tickets</li> <li>Summarize incoming Slack messages</li> <li>Generate and send reports</li> <li>Chain actions across different services</li> </ul>"},{"location":"tutorials/guides/tools/#related-topics","title":"\ud83d\udcda Related Topics","text":"<ul> <li>Agents</li> </ul> Check out RailTracks Tools"}]}