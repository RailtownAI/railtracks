{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Railtracks Agentic Framework <p>Railtracks is a streamlined agentic workflow creation tool that allows users to quickly prototype, test, and  deploy agentic workflows. The foundational principles of Railtracks were designed to make the process of creating agents an exercise in writing code, not writing a configuration file.</p> <p> Quick Start API Reference Community Discussions Contributing </p>"},{"location":"#why-railtracks","title":"Why Railtracks?","text":"<p>The space of agentic AI frameworks is vast and diverse, with many frameworks offering many features it can be hard to decide which one to use.  Railtracks offers a unique approach that focuses on simplicity and developer experience.</p> Agent Building <p>Write your agentic workflows in Python code, no need to learn an API. Basic Python skills is all it takes to get started.</p> Visualizable <p>Easy to create visualizations that are perfect to show off the internals of the agent you created.</p> Tool Ecosystem <p>Railtracks is modular to its core. Your tools and components should be reusable for any system.</p> Multi Agent Flows <p>For complex applications, Railtracks provides a simple interface for coordinating multiple agents with built-in logging.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions of all kinds! Check out our contributing guide to get started.</p>"},{"location":"advanced_usage/config/","title":"Configuration","text":"<p>Railtracks provides flexible configuration options to customize the behavior of your agent executions. You can control timeouts, logging, error handling, and more through a simple configuration system.</p>"},{"location":"advanced_usage/config/#configuration-methods","title":"Configuration Methods","text":"<p>Configuration parameters follow a specific precedence order, allowing you to override settings at different levels:</p> <ol> <li>Session Constructor Parameters - Highest priority</li> <li>Global Configuration (<code>rt.set_config()</code>) - Medium priority  </li> <li>Default Values - Lowest priority</li> </ol>"},{"location":"advanced_usage/config/#available-configuration-parameters","title":"Available Configuration Parameters","text":""},{"location":"advanced_usage/config/#core-execution-settings","title":"Core Execution Settings","text":"<ul> <li><code>timeout</code> (<code>float</code>): Maximum seconds to wait for a response to your top-level request</li> <li><code>end_on_error</code> (<code>bool</code>): Stop execution when an exception is encountered</li> </ul>"},{"location":"advanced_usage/config/#logging-configuration","title":"Logging Configuration","text":"<ul> <li><code>logging_setting</code> (AllowableLogLevels | None): Level of logging detail.  Here are the <code>AllowableLogLevels</code> options:<ul> <li><code>VERBOSE</code></li> <li><code>REGULAR</code></li> <li><code>QUIET</code></li> <li><code>NONE</code></li> </ul> </li> <li><code>log_file</code> (<code>str | os.PathLike | None</code>): File path for log output (None = no file logging)</li> </ul>"},{"location":"advanced_usage/config/#advanced-settings","title":"Advanced Settings","text":"<ul> <li><code>context</code> (<code>Dict[str, Any]</code>): Global context variables for execution</li> <li><code>broadcast_callback</code> (<code>Callable</code>): Callback function for broadcast messages</li> <li><code>prompt_injection</code> (<code>bool</code>): Automatically inject prompts from context variables</li> <li><code>save_state</code> (<code>bool</code>): Save execution state to <code>.railtracks</code> directory</li> </ul>"},{"location":"advanced_usage/config/#default-values","title":"Default Values","text":"<pre><code># Default configuration values\ntimeout = 150.0                   # seconds\nend_on_error = False              # continue on errors\nlogging_setting = \"REGULAR\"       # standard logging level\nlog_file = None                   # no file logging\nbroadcast_callback = None         # no broadcast callback\nprompt_injection = True           # enable prompt injection\nsave_state = True                 # save execution state\n</code></pre>"},{"location":"advanced_usage/config/#method-1-session-constructor","title":"Method 1: Session Constructor","text":"<p>Configure settings when creating a session for your agent execution:</p> <pre><code>import railtracks as rt\n\n# Configure for a partiular session execution\nwith rt.session(\n    timeout=300.0,\n    end_on_error=True,\n    logging_setting=\"DEBUG\",\n    log_file=\"execution.log\",\n    prompt_injection=False,\n    save_state=False,\n    context={\"user_name\": \"Alice\", \"environment\": \"production\"}\n    ):\n    response = await rt.call(\n        my_agent,\n        \"Hello world!\",\n    )\n</code></pre>"},{"location":"advanced_usage/config/#method-2-global-configuration","title":"Method 2: Global Configuration","text":"<p>Set configuration globally using <code>rt.set_config()</code>. This must be called before any <code>rt.call()</code>:</p> <pre><code>import railtracks as rt\n\n# Set global configuration\nrt.set_config(\n    timeout=200.0,\n    logging_setting=\"DEBUG\",\n    log_file=\"app_logs.log\",\n    end_on_error=True,\n    context={\"app_version\": \"1.2.3\"}\n)\n\n# Now all subsequent calls will use these settings\nresponse1 = await rt.call(agent1, \"First request\")\nresponse2 = await rt.call(agent2, \"Second request\")\n</code></pre>"},{"location":"advanced_usage/config/#configuration-precedence","title":"Configuration Precedence","text":"<p>When the same parameter is set in multiple places, Railtracks uses this priority order:</p> <pre><code>import railtracks as rt\n\n# 1. Set global config (medium priority)\nrt.set_config(timeout=100.0, logging_setting=\"REGULAR\")\n\n# 2. Session overrides global config (highest priority)\nwith rt.session(\n    timeout=300.0,        # This overrides the global timeout=100.0\n    end_on_error=True     # This uses session-level setting\n    # logging_setting not specified, so uses global \"REGULAR\"\n):\n    response = await rt.call(\n        my_agent,\n        \"Hello!\",\n    )\n\n# Final effective configuration:\n# - timeout: 300.0 (from session constructor)\n# - end_on_error: True (from session constructor)  \n# - logging_setting: \"REGULAR\" (from global config)\n# - All other parameters use default values\n</code></pre>"},{"location":"advanced_usage/config/#best-practices","title":"Best Practices","text":""},{"location":"advanced_usage/config/#development-vs-production","title":"Development vs Production","text":"<pre><code>import railtracks as rt\nimport os\n\n# Configure based on environment\nif os.getenv(\"ENVIRONMENT\") == \"production\":\n    rt.set_config(\n        timeout=300.0,\n        logging_setting=\"REGULAR\",\n        log_file=\"production.log\",\n        end_on_error=False,\n        save_state=True\n    )\nelse:\n    rt.set_config(\n        timeout=60.0,\n        logging_setting=\"DEBUG\", \n        log_file=\"debug.log\",\n        end_on_error=True,\n        save_state=False\n    )\n</code></pre>"},{"location":"advanced_usage/config/#debugging-configuration","title":"Debugging Configuration","text":"<pre><code>import railtracks as rt\n\n# Enhanced debugging setup\nrt.set_config(\n    logging_setting=\"DEBUG\",\n    log_file=\"debug_session.log\",\n    end_on_error=True,           # Stop on first error\n    save_state=True,             # Save state for inspection\n)\n\ndef debug_callback(message: str):\n    print(f\"Broadcast: {message}\")\n\nwith rt.session(\n    broadcast_callback=debug_callback,\n):\n    response = await rt.call(\n        my_agent,\n        \"Debug this workflow\",\n    )\n</code></pre>"},{"location":"advanced_usage/config/#important-notes","title":"Important Notes","text":"<ul> <li><code>rt.set_config()</code> must be called before any agent execution</li> <li>Session constructor parameters always take highest precedence</li> <li>Configuration is global and affects all subsequent agent calls</li> <li>Default values are used for any unspecified parameters</li> </ul>"},{"location":"advanced_usage/context/","title":"Global Context","text":"<p>Railtracks includes a concept of global context, letting you store and retrieve shared information across the lifecycle of a run. This makes it easy to coordinate data like config settings, environment flags, or shared resources.</p>"},{"location":"advanced_usage/context/#what-is-global-context","title":"What is Global Context?","text":"<p>The context system gives you a simple and clear API for interacting with shared values. It's scoped to the duration of a run, so everything is neatly contained within that execution lifecycle. One of the key features of the context system is that it can be accessed from within any node in your workflow, making it ideal for sharing data between different parts of your application.</p>"},{"location":"advanced_usage/context/#core-functions","title":"Core Functions","text":"<p>You can use the context with the following main functions:</p> <ul> <li><code>rt.context.get(key, default=None)</code> - Retrieves a value from the context</li> <li><code>rt.context.put(key, value)</code> - Stores a value in the context</li> <li><code>rt.context.update(dict)</code> - Updates multiple values in the context at once</li> <li><code>rt.context.delete(key)</code> - Removes a value from the context</li> </ul>"},{"location":"advanced_usage/context/#quick-start","title":"Quick Start","text":"<p>Here\u2019s how you can use context during a run:</p> <pre><code>import railtracks as rt\n\n# Set up some context data\ndata = {\"var_1\": \"value_1\"}\n\nwith rt.Session(context=data):\n    rt.context.get(\"var_1\")  # Outputs: value_1\n    rt.context.get(\"var_2\", \"default_value\")  # Outputs: default_value\n\n    rt.context.put(\"var_2\", \"value_2\")  # Sets var_2 to value_2\n    rt.context.put(\"var_1\", \"new_value_1\")  # Replaces var_1 with new_value_1\n</code></pre> <p>Context in a Node</p> <p>The context can be accessed from within any node in your Railtracks workflow, regardless of where the node is defined or how it's called:</p> <pre><code>import railtracks as rt\n\n@rt.function_node\ndef some_node():\n    return rt.context.get(\"var_1\")\n\n@rt.session(context={\"var_1\": \"value_1\"})\nasync def main():\n    await rt.call(some_node)\n</code></pre> <p>Warning</p> <p>The context only exists while the run is active. After that, it's gone.</p>"},{"location":"advanced_usage/context/#real-world-examples","title":"Real-World Examples","text":""},{"location":"advanced_usage/context/#prevent-hallucinations-in-agentic-systems","title":"Prevent Hallucinations in Agentic Systems","text":"<p>In agentic systems, you can use context to store important facts or constraints that agents will need to use. This helps reduce hallucinations by providing a reliable source of truth.</p> <p>Example</p> <pre><code>import railtracks as rt\n\n# Define a tool that uses context\n@rt.function_node\ndef find_issue(input: str) -&gt; str:\n    issue = get_issue_from_input(input)  # Assume this function finds an issue number from the inpu\n    rt.context.put(\"issue_number\", issue)\n    return issue\n\n@rt.function_node\ndef comment_on_issue(comment: str):\n    try:\n        issue = rt.context.get(\"issue_number\")\n    except KeyError:\n        return \"No relevant issue is available.\"\n\n    comment_on_github_issue(issue, comment)  # Assume this function comments on the issue\n\n# Define the agent with the tool\nGitHubAgent = rt.agent_node(\n    tool_nodes=[find_issue, comment_on_issue],\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n    system_message=\"You are an agent that provides information based on important facts.\",\n)\n\n# Run the agent\n@rt.session()\nasync def gh_agent():\n    response = await rt.call(\n        GitHubAgent,\n        \"What is the last issue created? Please write a comment on it.\"\n    )\n</code></pre>"},{"location":"advanced_usage/context/#prompt-injection","title":"Prompt Injection","text":"<p>One of the most powerful features built on top of the context system is \"prompt injection\" (also called \"context injection\"). This allows dynamically inserting values from the global context into prompts for LLMs:</p> <p>Tip</p> <p>For more details on prompt injection, see the Prompts and Context Injection documentation.</p>"},{"location":"advanced_usage/context/#benefits-of-using-context","title":"Benefits of Using Context","text":"<p>Why use the context system?</p> <p>The context system provides several advantages over alternatives like global variables</p> <ol> <li>Safer and clearer way to manage shared values</li> <li>Makes runs more predictable</li> <li>Makes data easier to reason about</li> <li>Reduces repetitive code</li> <li>Keeps sensitive information out of LLM inputs</li> <li>Provides clean scoping tied to execution lifecycle</li> </ol>"},{"location":"advanced_usage/node_builder/","title":"Custom Class Building with NodeBuilder","text":"<p>Railtracks' <code>define_agent</code> provides a robust foundation for configuring agent classes with predefined parameters that handle most use cases. However, when you need agents or nodes with specialized functionality beyond these standard configurations, the NodeBuilder class offers the flexibility to create custom implementations while maintaining the core Railtracks functionality.</p>"},{"location":"advanced_usage/node_builder/#overview","title":"Overview","text":"<p>NodeBuilder enables you to: - Inherit from existing Railtracks node classes - Add custom class methods and attributes - Maintain compatibility with the Railtracks ecosystem - Create specialized agents tailored to your specific requirements</p>"},{"location":"advanced_usage/node_builder/#getting-started","title":"Getting Started","text":""},{"location":"advanced_usage/node_builder/#1-initialize-nodebuilder","title":"1. Initialize NodeBuilder","text":"<p>Begin by selecting the appropriate base node class for inheritance. </p> <p>Common Classes</p> <ul> <li>Terminal - For endpoint nodes</li> <li>Structured - For nodes with structured outputs</li> <li>ToolCall - For nodes that can call tools</li> <li>StructuredToolCall - For nodes combining structured outputs with tool calling</li> </ul> <p>When initializing NodeBuilder, you'll specify both a <code>name</code> and <code>class_name</code>: - <code>name</code>: User-friendly identifier for differentiation in multi-node scenarios - <code>class_name</code>: Internal identifier used by Railtracks and Python</p> <pre><code>builder = NodeBuilder[StructuredLLM[_TOutput]](\n    StructuredLLM,\n    name='yourAgentName',\n    class_name='yourClassName',\n)\n</code></pre>"},{"location":"advanced_usage/node_builder/#2-configure-llm-base-optional","title":"2. Configure LLM Base (Optional)","text":"<p>For nodes that utilize Large Language Models, use the <code>llm_base</code> method to specify the system message and model:</p> <pre><code>builder.llm_base(your_llm_model, your_system_message)\n</code></pre> <p>Warning</p> <p>While LLM configuration is technically optional, it's highly recommended. If you are making a non-LLM node, consider using function nodes instead.</p>"},{"location":"advanced_usage/node_builder/#adding-functionality","title":"Adding Functionality","text":"<p>NodeBuilder provides several methods to decide your node's capabilities, mirroring the functionality available in <code>agent_node</code> :</p>"},{"location":"advanced_usage/node_builder/#core-functionality-options","title":"Core Functionality Options","text":"Method Purpose Required For <code>structured(output_schema)</code> Define structured output format Structured classes <code>tool_callable_llm(tool_details, tool_params)</code> Enable tool invocation capabilities ToolCall classes <code>tool_calling_llm(tool_nodes, max_tool_calls)</code> Configure tool calling behavior ToolCall classes <pre><code># Structured output\nbuilder.structured(output_schema)\n\n# Tool capabilities\nbuilder.tool_callable_llm(tool_details, tool_params)\nbuilder.tool_calling_llm(tool_nodes, max_tool_calls)\n</code></pre>"},{"location":"advanced_usage/node_builder/#custom-attributes","title":"Custom Attributes","text":"<p>Extend your node with custom methods or class variables using the <code>add_attribute</code> method:</p> <pre><code>builder.add_attribute(\n    attribute_name,\n    is_function,\n    args,\n    kwargs\n)\n</code></pre>"},{"location":"advanced_usage/node_builder/#building-your-node","title":"Building Your Node","text":"<p>Complete the build process by calling the <code>build()</code> method:</p> <pre><code>node = builder.build()\n</code></pre>"},{"location":"advanced_usage/node_builder/#complete-example","title":"Complete Example","text":"<pre><code># Initialize the builder\nbuilder = NodeBuilder[StructuredToolCallLLM[_TOutput]](\n    StructuredToolCallLLM,\n    name=name,\n    class_name=\"EasyStructuredToolCallLLM\",\n    return_into=return_into,\n    format_for_return=format_for_return,\n    format_for_context=format_for_context,\n)\n\n# Configure LLM base\nbuilder.llm_base(llm_model, system_message)\n\n# Add tool functionality\nbuilder.tool_calling_llm(set(tool_nodes), max_tool_calls)\nbuilder.tool_callable_llm(tool_details, tool_params)\n\n# Configure structured output\nbuilder.structured(output_schema)\n\n# Build the final node\nnode = builder.build()\n</code></pre>"},{"location":"advanced_usage/session/","title":"Session Management","text":"<p>Sessions in Railtracks manage the execution environment for your flows. The recommended approach is using the <code>@rt.session</code> decorator for clean, automatic session management.</p>"},{"location":"advanced_usage/session/#the-rtsession-decorator","title":"The <code>@rt.session</code> Decorator","text":"<p>The decorator automatically wraps your top level async functions with a Railtracks session:</p> <pre><code>@rt.function_node\nasync def greet(name: str) -&gt; str:\n    return f\"Hello, {name}!\"\n\n\n@rt.session()\nasync def empty_workflow():\n    result = await rt.call(greet, name=\"Alice\")\n    return result\n\n\n# Run your workflow\nresult, session = asyncio.run(empty_workflow())\nprint(result)  # \"Hello, Alice!\"\nprint(f\"Session ID: {session._identifier}\")\n</code></pre>"},{"location":"advanced_usage/session/#configuring-your-session","title":"Configuring Your Session","text":"<p>The decorator supports all session configuration options:</p> <pre><code>@rt.session(\n    timeout=30,  # 30 second timeout\n    context={\"user_id\": \"123\"},  # Global context variables\n    logging_setting=\"QUIET\",  # Enable debug logging\n    save_state=True,  # Save execution state to file\n    name=\"my-unique-run\",  # Custom session name\n)\nasync def configured_workflow():\n    result1 = await rt.call(greet, name=\"Bob\")\n    result2 = await rt.call(greet, name=\"Charlie\")\n    return [result1, result2]\n\n\nresult, session = asyncio.run(configured_workflow())\nprint(result)  # ['Hello, Bob!', 'Hello, Charlie!']\nprint(f\"Session ID: {session._identifier}\")\n</code></pre>"},{"location":"advanced_usage/session/#multiple-workflows","title":"Multiple Workflows","text":"<p>Each decorated function gets its own isolated session:</p> <pre><code>@rt.function_node\nasync def farewell(name: str) -&gt; str:\n    return f\"Bye, {name}!\"\n\n\n@rt.session(context={\"action\": \"greet\", \"name\": \"Diana\"}, logging_setting=\"QUIET\")\nasync def first_workflow():\n    if rt.context.get(\"action\") == \"greet\":\n        return await rt.call(greet, rt.context.get(\"name\"))\n\n\n@rt.session(context={\"action\": \"farewell\", \"name\": \"Robert\"}, logging_setting=\"QUIET\")\nasync def second_workflow():\n    if rt.context.get(\"action\") == \"farewell\":\n        return await rt.call(farewell, rt.context.get(\"name\"))\n\n\n# Run independently\nresult1, session1 = asyncio.run(first_workflow())\nresult2, _ = asyncio.run(second_workflow()) # if we don't want to work with Session Object\nprint(result1)  # \"Hello, Diana!\"\nprint(result2)  # \"Bye, Robert!\"\n</code></pre> <p>Important Notes</p> <ul> <li>Async Only: The <code>@rt.session</code> decorator only works with async functions. Using it on sync functions raises a <code>TypeError</code></li> <li>Automatic Cleanup: Sessions automatically clean up resources when functions complete</li> <li>Unique Identifiers: Each session gets a unique identifier for tracking and debugging. If you do use the same identifier in different flows, their unique saved states will overwrite with the warning:     <code>RT.Session  : WARNING  - File .railtracks/my-unique-run.json already exists, overwriting...</code>.</li> </ul> Session Context Manager More Examples <p>Error Handling</p> <pre><code>@rt.session(end_on_error=True)\nasync def safe_workflow():\n    try:\n        return await rt.call(sample_node)\n    except Exception as e:\n        print(f\"Workflow failed: {e}\")\n        return None\n</code></pre> <p>API Workflows</p> <pre><code>@rt.session(context={\"api_key\": \"secret\", \"region\": \"us-west\"})\nasync def api_workflow():\n    # Context variables are available to all nodes\n    result = await rt.call(sample_node)\n    return result\n</code></pre> <p>Tracked Execution</p> <pre><code>@rt.session(save_state=True, name=\"daily-report-v1\")\nasync def daily_report():\n    # Execution state saved to .railtracks/daily-report-v1.json\n    return await rt.call(sample_node)\n</code></pre>"},{"location":"advanced_usage/session/#when-to-use-context-managers","title":"When to Use Context Managers","text":"<p>For more complex scenarios or when you need fine-grained control, use the context manager approach:</p> <pre><code>async def context_workflow():\n    with rt.Session(\n        timeout=30,  # 30 second timeout\n        context={\"user_id\": \"123\"},  # Global context variables\n        logging_setting=\"QUIET\",  # Enable debug logging\n        save_state=True,  # Save execution state to file\n        name=\"my-unique-run\",  # Custom session name\n    ):\n        result1 = await rt.call(greet, name=\"Bob\")\n        result2 = await rt.call(greet, name=\"Charlie\")\n        return [result1, result2]\n\n\nresult = asyncio.run(context_workflow())\nprint(result)  # ['Hello, Bob!', 'Hello, Charlie!']\n</code></pre>"},{"location":"background/RAG/","title":"What is RAG?","text":"<p>Similar to how LLMs can become more powerful when equipped with tools they can use to take actions, we can also boost their performance by providing relevant information alongside the prompts we give them.</p> <p>Ever wished your AI agent could know about other files in your coding project, company policies, or the latest updates in your knowledge base? Thats exactly what RAG does!</p> <p>RAG stands for Retrieval-Augmented Generation. At it's core, is a method that combines a large language model\u2019s ability to generate text with an external source of knowledge. Instead of relying only on what the model learned during training, RAG allows it to \u201clook things up\u201d in a database, knowledge base, or document collection. It can then use that information to produce more accurate, relevant, and up-to-date answers.</p>"},{"location":"background/RAG/#why-should-you-care-about-rag","title":"Why Should You Care About RAG?","text":"<p>LLMs are smart, but they have some pretty big limitations:</p> <ul> <li>No access to your private data (company docs, internal policies, recent updates)</li> <li>Knowledge cutoff dates: They miss recent information</li> <li>Hallucinations when they confidently make up facts</li> </ul>"},{"location":"background/RAG/#how-rag-works","title":"How RAG Works?","text":"<p>When people talk about using RAG (Retrieval-Augmented Generation), they mean a system that can pull in relevant information from a knowledge base and inject it into an LLM\u2019s prompt, so the model can generate responses that are more accurate and context-aware.</p> <p>The process often looks like this:</p> <ol> <li> <p>Chunk    Split each document into manageable text chunks.</p> </li> <li> <p>Embed    Convert each chunk to a vector using an embedding model.</p> </li> <li> <p>Store    Write vectors and associated text/metadata to a vector store.</p> </li> <li> <p>Search    At query time, embed the question and perform a similarity search to retrieve relevant chunks of information.</p> </li> <li> <p>Compose Prompt    Join retrieved snippets into a context string and pass it to an LLM for a more informed answer.</p> </li> </ol>"},{"location":"background/RAG/#what-it-looks-like","title":"What It Looks Like?","text":""},{"location":"background/RAG/#creating-a-rag-knowledge-base","title":"Creating a RAG Knowledge Base","text":"<pre><code>sequenceDiagram\n    participant U as Knowledge Base\n    participant A as  Chunk\n    participant B as  Embed\n    participant C as  Store\n\n    U-&gt;&gt;A: Provide documents\n    A-&gt;&gt;B: Send chunks\n    B-&gt;&gt;C: Store vectors\n</code></pre>"},{"location":"background/RAG/#using-your-rag","title":"Using Your RAG","text":"<pre><code>sequenceDiagram\n    participant U as User Prompt\n    participant C as  Store\n    participant E as  Prompt Injection\n    participant L as  LLM\n\n    U-&gt;&gt;C: Search Store for similarity \n    C-&gt;&gt;E: Return most relevent results\n    E-&gt;&gt;L: Provide new prompt with added context\n</code></pre> <p>When RAG Shines</p> <p>RAG is perfect when you need:</p> <ul> <li>Grounded answers from internal docs, policies, FAQs, or knowledge bases</li> <li>Real-time accuracy with frequently changing information</li> <li>Source traceability to show exactly where answers come from</li> <li>Smart retrieval when your knowledge base is too large for prompts</li> </ul> <p>When RAG Might Be Overkill</p> <p>Skip RAG if:</p> <ul> <li>You only need general world knowledge your model already has</li> <li>Your entire knowledge base easily fits in a single prompt</li> <li>You're doing creative writing rather than fact-based responses</li> </ul>"},{"location":"background/RAG/#conclusion","title":"Conclusion","text":"<p>On their own, LLMs are powerful but limited\u2014they can \u201challucinate\u201d or produce outdated answers. By pairing them with retrieval, you effectively give your model a live knowledge companion. The LLM becomes the brain, while the retrieval system acts as the memory it can query when it needs help.</p>"},{"location":"background/agents/","title":"What is an Agent?","text":"<p>The news around AI terms can be pretty overwhelming and sound more like a buzzword than something useful. Before we dive deep into how you can build agents in Railtracks, let's first understand what an agent is.</p> <p>An agent is a self-contained unit that can perform a specific task or set of tasks autonomously. It has the ability to make decisions, take actions, and act within its environment to achieve its goals.</p> <p>The key abilities of the agent include:</p> <ul> <li>Autonomy: Agents can operate independently within the boundaries you define</li> <li>Adaptability: Agents can adjust their behavior based on the environment and the tasks at hand</li> <li>Goal-Oriented: Agents are designed to achieve specific objectives or complete tasks</li> <li>Interaction: Agents can communicate with other agents or systems to gather information or perform actions</li> <li>Stateful: Agents maintain context and history and use it to inform their decisions</li> </ul>"},{"location":"background/agents/#llms-as-agents","title":"LLMs as Agents","text":"<p>Reinforcement Learning and other AI techniques have trained specific agents to operate in their environments for a while now, but LLMs have changed the game and made it much easier to use their generalized intelligence to accomplish complex tasks and goals. This ability makes them uniquely suited to operate as the brain for your agentic system.</p> <pre><code>graph LR\n    User[User] --&gt; LLM[LLM Agent]\n    LLM --&gt; Tools[Tools]\n    Tools --&gt; Environment[Environment]\n    Environment --&gt; Tools\n    Tools --&gt; LLM\n    LLM --&gt; User\n\n    style LLM fill:#e1f5fe\n    style User fill:#f3e5f5\n    style Tools fill:#fff3e0\n    style Environment fill:#e8f5e8\n</code></pre>"},{"location":"background/agents/#real-world-applications","title":"Real World Applications","text":"<p>Agents are already being used in real world applications such as:</p> <ol> <li>Vibe Coding Tools (Cursor, Windsurf, etc.)</li> <li>NPC Interactions in Games (AI Village)</li> <li>Technical Documentation Writing (ParagraphAI)</li> <li>Deep Research Tools (GPT Deep Research)</li> </ol>"},{"location":"background/agents/#related-topics","title":"Related Topics","text":"<ul> <li>Tools</li> </ul>"},{"location":"background/agents/#build-your-own","title":"Build Your Own","text":"<p>We have build Railtracks with developers in mind; with just a simple prompt and a bit of Python, you\u2019re already well on your way to building your first agent. Get started Building with Railtracks</p>"},{"location":"background/async_await/","title":"Async/Await in Python","text":"<p>We're going to switch the context here a bit and talk about <code>async</code> and <code>await</code> in Python. As a Python developer, the <code>async</code> and <code>await</code> keywords can sometimes stir up confusion. This guide will give you all the basics you need to get started with RT's usage of <code>async</code> and <code>await</code> since they are core to how RT operates.</p>"},{"location":"background/async_await/#what-is-asyncawait","title":"What is <code>async/await</code>?","text":"<p><code>async</code> and <code>await</code> are keywords in Python that unlock the ability to write asynchronous code. This asynchronous code allows your program to behave like a multithreaded one \u2014 but in a single thread \u2014 by efficiently managing I/O-bound tasks.</p> <p>Let's clarify what each keyword does:</p> <ul> <li><code>async</code>: A keyword that you place before a function to indicate it is an async function.</li> </ul> <pre><code>async def my_async_function():\n    pass\n</code></pre> <ul> <li><code>await</code>: A keyword that you use to call an async function. It tells Python to pause the execution of the current function until the awaited function completes.</li> </ul> <pre><code>async def my_async_function():\n    await another_async_function()\n</code></pre> <p>Warning</p> <p><code>await</code> can only be used inside an <code>async</code> function.</p> <p>Tip</p> <p>I encourage you to visit the official documentation.</p>"},{"location":"background/async_await/#when-is-it-useful","title":"When is it useful?","text":"<p>Asynchronous code shines when the tasks you are performing involve I/O operations, which is perfect for LLMs because they are often waiting for a response from the LLM APIs.</p>"},{"location":"background/async_await/#some-advanced-features","title":"Some Advanced Features","text":""},{"location":"background/async_await/#tasks","title":"Tasks","text":"<p>The <code>asyncio</code> library provides a way to run background tasks just like you would do with <code>concurrent.futures.ThreadPoolExecutor.submit(...)</code>.</p> <pre><code>import asyncio\n\nasync def my_async_function():\n    await asyncio.sleep(2)  # Simulate a long-running task\n    return \"Task completed\"\n\nasync def main():\n    task = asyncio.create_task(my_async_function())  # Create a background task to be completed soon\n    result = await task # await the task just as you would with a normal async function\n</code></pre>"},{"location":"background/async_await/#gather","title":"Gather","text":"<p>You can use <code>asyncio.gather(...)</code> to run multiple async functions concurrently and gather their results. This allows for high level parallelism.</p> <pre><code>import asyncio\n\nasync def task1():\n    await asyncio.sleep(1)\n    return \"Task 1 completed\"\n\nasync def task2():\n    await asyncio.sleep(2)\n    return \"Task 2 completed\"\n\nasync def main():\n    results = await asyncio.gather(task1(), task2())\n    print(results)  # Output: ['Task 1 completed', 'Task 2 completed']\n</code></pre>"},{"location":"background/async_await/#parallelism-vs-sequential-execution","title":"Parallelism vs. Sequential Execution","text":"<p>Parallelism</p> <p>Each of the coroutines can run concurrently, only finishing when all of them are done. <pre><code>    await asyncio.gather(*coroutines)\n</code></pre></p> <p>Sequential Execution</p> <p>Each coroutine will run one after the other, waiting for each to finish before starting the next <pre><code>    await coroutine_1()\n    await coroutine_2()\n</code></pre></p>"},{"location":"background/async_await/#how-rt-uses-asyncawait","title":"How RT uses <code>async/await</code>","text":"<p>If you are writing a tool in RT and need to call another tool, use <code>rt.call(...)</code>. This ensures the RT backend tracks the tool invocation, enabling logging and visualization.</p> <p>Note</p> <p>Because <code>rt.call(...)</code> returns a coroutine, your function must be declared as async (e.g., <code>async def my_function(...)</code>) so you can <code>await rt.call(...)</code>.</p> <pre><code>import railtracks as rt\nimport asyncio\n\n@rt.function_node\ndef split_text(text: str) -&gt; list[str]:\n    return text.split()\n\n# since the alternate_capitalization function is a simple operation, it can be a regular function\n@rt.function_node\ndef alternate_capitalization(text: str) -&gt; str:\n    return text.swapcase()\n\n# Since the modify_text function calls other nodes, it must be an async function\n# You can use the asyncio library to run the nodes however you want.\n@rt.function_node\nasync def modify_text(text: str) -&gt; str:\n    # Call the split_text node sequentially.\n    words = await rt.call(split_text, text)\n\n    # Process each word parallelly using asyncio.gather\n    modified_words = await asyncio.gather(*(rt.call(alternate_capitalization, word) for word in words))\n\n    # Join the modified words back into a single string\n    return ' '.join(modified_words)\n</code></pre>"},{"location":"background/tools/","title":"Tools","text":"<p>One of the most important parts of any agentic system is the set of tools that the agent can use. While an LLM on its own can understand and generate language, it's the ability to use tools that transforms it from simple conversational bot into a true agent capable of taking action, solving problems, and achieving real-world goals.</p>"},{"location":"background/tools/#what-is-a-tool","title":"What Is a Tool?","text":"<p>A tool provides the agent with the ability to interact with external systems, perform actions, or access information that is not contained within the LLM's training data. Tools can be anything from APIs, databases, or even simple functions that perform specific tasks.</p> <p>At a technical level, tools are often defined as functions that the agent can call. Large Language Models (LLMs) like OpenAI\u2019s or Anthropic\u2019s support something called <code>tool_calls</code> or <code>function_calls</code>. These allow the LLM to output structured <code>JSON</code>.</p> <p>Common Tools</p> <ul> <li>Search the web</li> <li>Execute code in a sandbox</li> <li>Send a message to Slack</li> <li>Call a REST API</li> <li>Read or write to a file system</li> </ul> <p>Think of tools as the hands and eyes of your agent: they allow it to act beyond just thinking and talking.</p>"},{"location":"background/tools/#example-connecting-tools-to-an-agent","title":"Example: Connecting Tools to an Agent","text":"<p>Here\u2019s a conceptual diagram of an LLM agent using tools to interact with different systems:</p> <pre><code>graph TD\n    A[LLM Agent] --&gt;|Tool Call| B[SlackTool]\n    A --&gt;|Tool Call| C[GitHubTool]\n    A --&gt;|Tool Call| D[CodeExecutionTool]\n    B --&gt; E[Send Message]\n    C --&gt; F[Create Comment]\n    C --&gt; G[List Issues]\n    D --&gt; H[Run Python Code]\n</code></pre> <p>Each tool defines:</p> <ul> <li>Name: the identifier used by the agent (e.g. <code>post_to_slack</code>)</li> <li>Description: a brief explanation of what the tool does (e.g. \"Posts a message to a Slack channel\")</li> <li>Parameters: the inputs the agent must provide (e.g. message, channel)</li> <li>Function: the logic the tool actually executes (e.g. send HTTP request)</li> </ul>"},{"location":"background/tools/#why-tools-matter","title":"Why Tools Matter","text":"<p>Without tools, LLMs are limited to just generating language.</p> <p>With tools, agents can:</p> <ul> <li>Automatically debug code</li> <li>File support tickets</li> <li>Summarize incoming Slack messages</li> <li>Generate and send reports</li> <li>Chain actions across different services</li> </ul>"},{"location":"background/tools/#related-topics","title":"Related Topics","text":"<ul> <li>Agents</li> </ul>"},{"location":"deployments/docker/","title":"Deployment Guide","text":"<p>This guide covers deploying Railtracks AI agents as dockerized containers. Agents can be deployed for various use cases including web APIs, batch processing, scheduled tasks, and one-time runs.</p>"},{"location":"deployments/docker/#overview","title":"Overview","text":"<p>Railtracks agents are Python applications that can be containerized and deployed in multiple patterns:</p> <ul> <li>API Services: Expose agents as REST APIs for real-time interactions</li> <li>Batch Processing: Run agents on datasets or queues</li> <li>Scheduled Tasks: Execute agents on cron-like schedules</li> <li>One-time Runs: Execute agents for specific tasks and exit</li> </ul>"},{"location":"deployments/docker/#docker-basics","title":"Docker Basics","text":""},{"location":"deployments/docker/#base-dockerfile-template","title":"Base Dockerfile Template","text":"<pre><code>FROM python:3.10-slim\n\nWORKDIR /app\n\n# Install system dependencies if needed\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    git \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Copy requirements and install Python dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Set environment variables\nENV PYTHONPATH=/app\nENV PYTHONUNBUFFERED=1\n\n# Default command (override for specific use cases)\nCMD [\"python\", \"agent.py\"]\n</code></pre>"},{"location":"deployments/docker/#example-requirementstxt","title":"Example <code>requirements.txt</code>","text":"<pre><code>railtracks\nfastapi&gt;=0.104.0\nuvicorn&gt;=0.24.0\n</code></pre>"},{"location":"deployments/docker/#deployment-patterns","title":"Deployment Patterns","text":""},{"location":"deployments/docker/#1-api-service-deployment","title":"1. API Service Deployment","text":"<p>For agents exposed as REST APIs using FastAPI:</p>"},{"location":"deployments/docker/#fastapi-wrapper-example","title":"FastAPI Wrapper Example","text":"<pre><code># api_agent.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport railtracks as rt\nfrom agent_file_name import MyAgent  # Your agent definition\n\napp = FastAPI(title=\"Railtracks Agent API\")\n\nclass AgentRequest(BaseModel):\n    prompt: str\n    context: dict = {}\n\nclass AgentResponse(BaseModel):\n    result: str\n    metadata: dict = {}\n\n@app.post(\"/chat\", response_model=AgentResponse)\nasync def chat_with_agent(request: AgentRequest):\n    try:\n\n        # Run the agent\n        with rt.Session(\n          rt.ExecutorConfig(timeout=600),\n          context=request.context\n        ) as session:\n            response = await session.call(\n                Agent,\n                input_kwargs={\"prompt\": request.prompt}\n            )\n\n        return AgentResponse(\n            result=response.get(\"result\", \"\"),\n            metadata=response.get(\"metadata\", {})\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\"}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre>"},{"location":"deployments/docker/#api-dockerfile","title":"API Dockerfile","text":"<pre><code>FROM python:3.10-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nENV PYTHONPATH=/app\nENV PYTHONUNBUFFERED=1\n\nEXPOSE 8000\n\nCMD [\"uvicorn\", \"api_agent:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"deployments/docker/#2-batch-processing-deployment","title":"2. Batch Processing Deployment","text":"<p>For agents that process data in batches:</p> <pre><code># batch_agent.py\nimport os\nimport sys\nimport railtracks as rt\nfrom your_agent import create_agent\n\ndef main():\n    # Get batch parameters from environment\n    input_path = os.getenv(\"INPUT_PATH\", \"/data/input\")\n    output_path = os.getenv(\"OUTPUT_PATH\", \"/data/output\")\n    batch_size = int(os.getenv(\"BATCH_SIZE\", \"10\"))\n\n    agent = create_agent()\n\n    # Process batch\n    with rt.Runner(rt.ExecutorConfig(timeout=3600)) as runner:\n        result = runner.run_sync(\n            agent,\n            input_data={\n                \"input_path\": input_path,\n                \"output_path\": output_path,\n                \"batch_size\": batch_size\n            }\n        )\n\n    print(f\"Batch processing completed: {result}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"deployments/docker/#batch-dockerfile","title":"Batch Dockerfile","text":"<pre><code>FROM python:3.10-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nENV PYTHONPATH=/app\nENV PYTHONUNBUFFERED=1\n\n# Create data directories\nRUN mkdir -p /data/input /data/output\n\nCMD [\"python\", \"batch_agent.py\"]\n</code></pre>"},{"location":"deployments/docker/#3-scheduled-tasks","title":"3. Scheduled Tasks","text":"<p>For cron-like scheduled execution:</p> <pre><code># scheduled_agent.py\nimport schedule\nimport time\nimport railtracks as rt\nfrom your_agent import create_agent\n\ndef run_agent_task():\n    agent = create_agent()\n\n    with rt.Runner(rt.ExecutorConfig(timeout=1800)) as runner:\n        result = runner.run_sync(agent, input_data={})\n\n    print(f\"Scheduled task completed at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n    return result\n\ndef main():\n    # Schedule the agent to run every hour\n    schedule.every().hour.do(run_agent_task)\n\n    print(\"Scheduler started. Waiting for tasks...\")\n    while True:\n        schedule.run_pending()\n        time.sleep(60)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"deployments/docker/#4-one-time-run","title":"4. One-time Run","text":"<p>For single execution tasks:</p> <pre><code># oneshot_agent.py\nimport sys\nimport railtracks as rt\nfrom your_agent import create_agent\n\ndef main():\n    if len(sys.argv) &lt; 2:\n        print(\"Usage: python oneshot_agent.py '&lt;task_description&gt;'\")\n        sys.exit(1)\n\n    task = sys.argv[1]\n    agent = create_agent()\n\n    with rt.Runner(rt.ExecutorConfig(timeout=600)) as runner:\n        result = runner.run_sync(\n            agent,\n            input_data={\"task\": task}\n        )\n\n    print(f\"Task completed: {result}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"deployments/docker/#kubernetes-deployment","title":"Kubernetes Deployment","text":""},{"location":"deployments/docker/#api-service-deployment","title":"API Service Deployment","text":"<pre><code># api-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: railtracks-agent-api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: railtracks-agent-api\n  template:\n    metadata:\n      labels:\n        app: railtracks-agent-api\n    spec:\n      containers:\n      - name: agent-api\n        image: your-registry/railtracks-agent-api:latest\n        ports:\n        - containerPort: 8000\n        env:\n        - name: OPENAI_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: agent-secrets\n              key: openai-api-key\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: railtracks-agent-api-service\nspec:\n  selector:\n    app: railtracks-agent-api\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8000\n  type: LoadBalancer\n</code></pre>"},{"location":"deployments/docker/#batch-job","title":"Batch Job","text":"<pre><code># batch-job.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: railtracks-batch-job\nspec:\n  template:\n    spec:\n      containers:\n      - name: batch-agent\n        image: your-registry/railtracks-batch-agent:latest\n        env:\n        - name: INPUT_PATH\n          value: \"/data/input\"\n        - name: OUTPUT_PATH\n          value: \"/data/output\"\n        - name: BATCH_SIZE\n          value: \"50\"\n        volumeMounts:\n        - name: data-volume\n          mountPath: /data\n        resources:\n          requests:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"2Gi\"\n            cpu: \"1\"\n      volumes:\n      - name: data-volume\n        persistentVolumeClaim:\n          claimName: agent-data-pvc\n      restartPolicy: Never\n  backoffLimit: 3\n</code></pre>"},{"location":"deployments/docker/#scheduled-cronjob","title":"Scheduled CronJob","text":"<pre><code># scheduled-cronjob.yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: railtracks-scheduled-agent\nspec:\n  schedule: \"0 */6 * * *\"  # Every 6 hours\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: scheduled-agent\n            image: your-registry/railtracks-scheduled-agent:latest\n            env:\n            - name: TASK_TYPE\n              value: \"periodic_analysis\"\n            resources:\n              requests:\n                memory: \"512Mi\"\n                cpu: \"250m\"\n              limits:\n                memory: \"1Gi\"\n                cpu: \"500m\"\n          restartPolicy: OnFailure\n</code></pre>"},{"location":"deployments/docker/#secrets-management","title":"Secrets Management","text":"<pre><code># secrets.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: agent-secrets\ntype: Opaque\ndata:\n  openai-api-key: &lt;base64-encoded-api-key&gt;\n  database-url: &lt;base64-encoded-db-url&gt;\n</code></pre>"},{"location":"deployments/docker/#configuration-best-practices","title":"Configuration Best Practices","text":""},{"location":"deployments/docker/#environment-variables","title":"Environment Variables","text":"<p>Use environment variables for configuration:</p> <pre><code>import os\n\n# Agent configuration\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nDATABASE_URL = os.getenv(\"DATABASE_URL\")\nLOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"INFO\")\nTIMEOUT = int(os.getenv(\"AGENT_TIMEOUT\", \"600\"))\n\n# Validate required environment variables\nrequired_vars = [\"OPENAI_API_KEY\"]\nfor var in required_vars:\n    if not os.getenv(var):\n        raise ValueError(f\"Required environment variable {var} is not set\")\n</code></pre>"},{"location":"deployments/docker/#multi-stage-dockerfile-for-production","title":"Multi-stage Dockerfile for Production","text":"<pre><code># Multi-stage build for smaller production images\nFROM python:3.10-slim as builder\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --user --no-cache-dir -r requirements.txt\n\nFROM python:3.10-slim\n\n# Copy installed packages from builder stage\nCOPY --from=builder /root/.local /root/.local\n\n# Make sure scripts in .local are usable\nENV PATH=/root/.local/bin:$PATH\n\nWORKDIR /app\nCOPY . .\n\nENV PYTHONPATH=/app\nENV PYTHONUNBUFFERED=1\n\nCMD [\"python\", \"agent.py\"]\n</code></pre>"},{"location":"deployments/docker/#deployment-checklist","title":"Deployment Checklist","text":"<ul> <li>[ ] Security: Store API keys and secrets in Kubernetes secrets</li> <li>[ ] Resources: Set appropriate CPU/memory requests and limits</li> <li>[ ] Health Checks: Implement health endpoints for API services</li> <li>[ ] Logging: Configure structured logging for observability</li> <li>[ ] Monitoring: Set up metrics collection (TODO: detailed guide coming)</li> <li>[ ] Scaling: Configure horizontal pod autoscaling if needed</li> <li>[ ] Persistence: Use persistent volumes for stateful agents</li> <li>[ ] Networking: Configure ingress controllers for external access</li> </ul>"},{"location":"deployments/docker/#example-deploying-the-weather-agent","title":"Example: Deploying the Weather Agent","text":"<p>Here's a complete example using one of the demo agents:</p> <pre><code># 1. Build the Docker image\ndocker build -t railtracks-weather-agent .\n\n# 2. Tag for your registry\ndocker tag railtracks-weather-agent your-registry/railtracks-weather-agent:v1.0.0\n\n# 3. Push to registry\ndocker push your-registry/railtracks-weather-agent:v1.0.0\n\n# 4. Deploy to Kubernetes\nkubectl apply -f weather-agent-deployment.yaml\n\n# 5. Verify deployment\nkubectl get pods -l app=railtracks-weather-agent\nkubectl logs deployment/railtracks-weather-agent\n</code></pre>"},{"location":"deployments/docker/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployments/docker/#common-issues","title":"Common Issues","text":"<ol> <li>Module Import Errors: Ensure <code>PYTHONPATH</code> is set correctly</li> <li>Timeout Issues: Adjust <code>timeout</code> values in <code>ExecutorConfig</code></li> <li>Memory Issues: Increase memory limits in Kubernetes deployments</li> <li>API Key Errors: Verify secrets are properly mounted and accessible</li> </ol>"},{"location":"deployments/docker/#debugging-commands","title":"Debugging Commands","text":"<pre><code># Check pod logs\nkubectl logs &lt;pod-name&gt;\n\n# Execute into running container\nkubectl exec -it &lt;pod-name&gt; -- /bin/bash\n\n# Check resource usage\nkubectl top pods\n\n# Describe pod for events\nkubectl describe pod &lt;pod-name&gt;\n</code></pre> <p>Next Steps: </p> <ul> <li>[ ] Add monitoring and observability guide</li> <li>[ ] Include specific examples for popular cloud providers</li> <li>[ ] Add CI/CD pipeline templates</li> </ul>"},{"location":"human_in_the_loop/hil/","title":"Extending the HIL Abstract Class","text":"<p>The Human-in-the-Loop (HIL) interface allows you to create custom communication channels between your Railtracks agents and users. This guide shows you how to implement your own HIL interface by extending the <code>HIL</code> abstract class.</p>"},{"location":"human_in_the_loop/hil/#overview","title":"Overview","text":"<p>The <code>HIL</code> abstract class defines a contract for bidirectional communication with users. Any implementation must provide four key methods:</p> <ul> <li><code>connect()</code> - Initialize the communication channel</li> <li><code>disconnect()</code> - Clean up resources</li> <li><code>send_message()</code> - Send messages to the user</li> <li><code>receive_message()</code> - Receive input from the user</li> </ul> The HIL Interface: human_in_the_loop.py <pre><code>from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Any, Dict\n\n\n@dataclass\nclass HILMessage:\n    content: str\n    metadata: Dict[str, Any] | None = None\n\n\nclass HIL(ABC):\n    @abstractmethod\n    async def connect(self) -&gt; None:\n        \"\"\"\n        Creates or initializes the user interface component.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def disconnect(self) -&gt; None:\n        \"\"\"\n        Disconnects the user interface component.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def send_message(\n        self, content: HILMessage, timeout: float | None = None\n    ) -&gt; bool:\n        \"\"\"\n        HIL uses this function to send a message to the user through the interface.\n\n        Args:\n            content: The message content to send.\n            timeout: The maximum time in seconds to wait for the message to be sent.\n\n        Returns:\n            True if the message was sent successfully, False otherwise.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def receive_message(self, timeout: float | None = None) -&gt; HILMessage | None:\n        \"\"\"\n        HIL uses this function to wait for the user to provide input.\n\n        This method should block until input is received or the timeout is reached.\n\n        Args:\n            timeout: The maximum time in seconds to wait for input.\n\n        Returns:\n            The user input if received within the timeout period, None otherwise.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"human_in_the_loop/hil/#implementation-guide","title":"Implementation Guide","text":""},{"location":"human_in_the_loop/hil/#basic-structure","title":"Basic Structure","text":"<p>Create a class that inherits from <code>HIL</code> and initialize the necessary state for your communication channel. At minimum, you'll need to track connection status and set up mechanisms for bidirectional communication (such as queues or event handlers).</p> <p>Below are some of our suggestions for such implementation, however, your way of defining it is completely up to you and your system design choices.</p> Suggested Steps"},{"location":"human_in_the_loop/hil/#1-implement-connect","title":"1. Implement connect()","text":"<p>The <code>connect()</code> method initializes all resources needed for communication with users. This is where you:</p> <ul> <li>Start any servers or services (web servers, WebSocket connections, messaging clients)</li> <li>Initialize communication channels</li> <li>Set up authentication or session management if needed</li> <li>Update the connection state to indicate the channel is ready</li> </ul> <p>Key considerations:</p> <ul> <li>Track connection state explicitly</li> <li>Raise appropriate exceptions if initialization fails</li> <li>Make the method safe to call multiple times if possible</li> </ul>"},{"location":"human_in_the_loop/hil/#2-implement-disconnect","title":"2. Implement disconnect()","text":"<p>The <code>disconnect()</code> method performs cleanup of all resources. This should:</p> <ul> <li>Update connection state immediately</li> <li>Close servers, connections, or file handles</li> <li>Cancel any running background tasks</li> <li>Clean up gracefully even if resources weren't fully initialized</li> </ul> <p>Key considerations:</p> <ul> <li>Set connection state to <code>False</code> at the start</li> <li>Don't raise exceptions during cleanup - log errors instead</li> <li>Make it safe to call multiple times</li> </ul>"},{"location":"human_in_the_loop/hil/#3-implement-send_message","title":"3. Implement send_message()","text":"<p>This method sends messages from your agent to the user through your communication channel. It should:</p> <ul> <li>Verify the connection is active before attempting to send</li> <li>Format the message appropriately for your channel</li> <li>Transmit the message through your communication mechanism</li> <li>Return <code>True</code> on success, <code>False</code> on any failure</li> <li>Respect the timeout parameter if provided</li> </ul> <p>Key considerations:</p> <ul> <li>Always check connection state first</li> <li>Return <code>False</code> rather than raising exceptions on failure</li> <li>Handle timeouts and queue full conditions gracefully</li> <li>Log warnings or errors for debugging</li> </ul>"},{"location":"human_in_the_loop/hil/#4-implement-receive_message","title":"4. Implement receive_message()","text":"<p>This method waits for and receives input from the user. It should:</p> <ul> <li>Verify the connection is active</li> <li>Wait for user input through your communication channel</li> <li>Handle shutdown events to allow clean termination</li> <li>Return a <code>HILMessage</code> when input is received</li> <li>Return <code>None</code> on timeout, disconnection, or shutdown</li> </ul> <p>Key considerations:</p> <ul> <li>Return <code>None</code> for timeout, disconnection, or shutdown scenarios</li> <li>Handle multiple concurrent events (input arrival and shutdown signals)</li> <li>Always cancel pending tasks to prevent resource leaks</li> <li>Respect timeout parameters and handle timeout exceptions</li> </ul>"},{"location":"human_in_the_loop/hil/#reference-implementation","title":"Reference Implementation","text":"<p>For a complete example, see the <code>ChatUI</code> class in local_chat_ui.py</p> <p>The <code>ChatUI</code> implementation demonstrates:</p> <ul> <li>FastAPI server with SSE for real-time updates</li> <li>Proper queue management with size limits</li> <li>Clean shutdown handling</li> <li>Static file serving for the UI</li> <li>Tool invocation updates (additional feature)</li> <li>Port availability checking</li> <li>Browser auto-opening</li> </ul> Common Pitfalls"},{"location":"human_in_the_loop/hil/#1-blocking-operations","title":"1. Blocking Operations","text":"<p>Don't block the event loop: <pre><code>async def receive_message(self, timeout=None):\n    return input(\"Enter message: \")  # WRONG: Blocks event loop\n</code></pre></p> <p>Use asyncio.to_thread for blocking I/O: <pre><code>async def receive_message(self, timeout=None):\n    return await asyncio.to_thread(input, \"Enter message: \")\n</code></pre></p>"},{"location":"human_in_the_loop/hil/#2-not-handling-disconnection","title":"2. Not Handling Disconnection","text":"<p>Don't forget to check connection state: <pre><code>async def send_message(self, content, timeout=None):\n    await self.queue.put(content)  # May fail if disconnected\n    return True\n</code></pre></p> <p>Always check first: <pre><code>async def send_message(self, content, timeout=None):\n    if not self.is_connected:\n        return False\n    await self.queue.put(content)\n    return True\n</code></pre></p>"},{"location":"human_in_the_loop/hil/#3-not-canceling-tasks","title":"3. Not Canceling Tasks","text":"<p>Don't leave tasks running: <pre><code>async def receive_message(self, timeout=None):\n    task1 = asyncio.create_task(self.queue.get())\n    task2 = asyncio.create_task(self.event.wait())\n    done, pending = await asyncio.wait([task1, task2], ...)\n    return done.pop().result()  # Pending tasks still running!\n</code></pre></p> <p>Always cancel pending tasks: <pre><code>async def receive_message(self, timeout=None):\n    task1 = asyncio.create_task(self.queue.get())\n    task2 = asyncio.create_task(self.event.wait())\n    done, pending = await asyncio.wait([task1, task2], ...)\n\n    for task in pending:\n        task.cancel()  # Clean up!\n\n    return done.pop().result()\n</code></pre></p>"},{"location":"human_in_the_loop/hil/#last-step-updating-the-interactive-method","title":"Last step: Updating the <code>interactive</code> method","text":"<p>Currently, the <code>interactive</code> method only supports <code>ChatUI</code> implementation, but you could easily modify it, append to it, or completely write new logic to work with your specific child class of <code>HIL</code>.</p> <pre><code>    if not issubclass(node, LLMBase):\n        raise ValueError(\n            \"Interactive sessions only support nodes that are children of LLMBase.\"\n        )\n    response = None\n    try:\n        logger.info(\"Connecting with Local Chat Session\")\n\n        chat_ui = ChatUI(**chat_ui_kwargs)\n\n        await chat_ui.connect()\n\n        response = await _chat_ui_interactive(\n            chat_ui,\n            node,\n            initial_message_to_user,\n            initial_message_to_agent,\n            turns,\n            *args,\n            **kwargs,\n        )\n\n    except Exception as e:\n        logger.error(f\"Error during interactive session: {e}\")\n    finally:\n        return response  # type: ignore\n</code></pre>"},{"location":"human_in_the_loop/hil/#share-your-work","title":"Share your work","text":"<p>We're excited to see what implemenations you come up with and welcome incorporating your suggested changes or new implementations to the framework!</p>"},{"location":"human_in_the_loop/hil/#next-steps","title":"Next Steps","text":"<ul> <li>See Local Chat UI for documentation on using the built-in ChatUI</li> <li>Check the Human-in-the-Loop Overview for integration patterns</li> </ul>"},{"location":"human_in_the_loop/local_chat_ui/","title":"Local Chat UI","text":"<p>In this tutorial, we'll quickly cover how you can interact with any previous made agents in a multi-turn style chat.</p>"},{"location":"human_in_the_loop/local_chat_ui/#1-local_chat-from-interactive-module","title":"1. <code>local_chat</code> from <code>interactive</code> module","text":"<p>Simply pass any of the agents you've made so far to the <code>local_chat</code> method as follows: <pre><code>@rt.function_node\ndef programming_language_info(language: str) -&gt; str:\n    \"\"\"\n    Returns the version of the specified programming language\n\n    Args:\n        language (str): The programming language to get the version for. Supported values are \"python\".\n    \"\"\"\n    if language == \"python\":\n        return f\"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\"\n    return \"Unknown language\"\n\n\nChatAgent = rt.agent_node(\n    name=\"ChatAgent\",\n    system_message=\"You are a helpful assistant\",\n    llm=rt.llm.OpenAILLM(\"gpt-5\"),\n    tool_nodes=[programming_language_info],\n)\n\n\nasync def main():\n    response = await rt.interactive.local_chat(ChatAgent)\n    print(response.content)\n</code></pre> and you will see automatically a window pop open in your browser with the following style:  Additionally, you can also view the tool invocations that happened throughout your conversation:  After you are finished with talking interactive with the model, you can press <code>END SESSION</code> and the rest of your workflow will continue to execute.</p> <p>The possibilities from here on are endless. For instance you can replace the <code>llm</code> parameter with a locally running <code>Ollama</code> model and have your own locally run agent that you can chat with at your leisure! Give it tools to empower it even more.</p> Streaming <p>Currently for the local chat streaming LLM responses is not supported.</p>"},{"location":"human_in_the_loop/local_chat_ui/#2-customizing-the-ui","title":"2. Customizing the UI","text":"<p>If you would like to customize the appearance of the UI, we have provided the relevant css file for you to modify at chat.css. The full html and javascript are also available within the same folder if you'd like to take things further.</p>"},{"location":"human_in_the_loop/local_chat_ui/#3-integrating-human-in-the-loop-in-your-flow","title":"3. Integrating Human In the Loop in your flow","text":"<p>You can expand the section below for a more involved example regarding a conversation analyzer agent and a customer service agent.</p> Human in the Loop Flow <p>In this example we'll have an agent analysis the conversation of another agent. You can imagine similar scenarios happening in customer support cases where you'd like to assess the helpfulness and/or issue resolution progress.</p> <p>Of course, this is meant as a simple local example and if you'd like to customize or deploy such behaviour you can take extend the <code>HIL</code> (Human In the Loop) class.</p> <p>Once the person interacting presses <code>END SESSION</code> in the UI, the rest of the flow will proceed. <pre><code>AnalysisAgent = rt.agent_node(\n    name=\"AnalysisAgent\",\n    system_message=\"You are a helpful assistant that analyzes customer interactions with agents\",\n    llm=rt.llm.OpenAILLM(\"gpt-5\"),\n)\n\n\nasync def analysis():\n    response = await rt.interactive.local_chat(ChatAgent)\n\n    analysis_response = await rt.call(\n        AnalysisAgent,\n        f\"Analyze the following conversation and provide a summary in less than 10 words:\\n\\n{response.message_history}\",\n    )\n    print(analysis_response.content)\n</code></pre> Here's the conversation:  And here's the output of the <code>AnalysisAgent</code>: <pre><code>Confirmed Python 3.10.18; not latest; guidance for C/C++ versions.\n</code></pre></p>"},{"location":"human_in_the_loop/overview/","title":"Overview","text":"<p>Quite often an agentic workflow needs further input from a human user (or another agent) in a muli-turn style conversation or verification.</p> <p>Revisiting the figure from the background on agents, we can take a new view:</p> <pre><code>graph LR\n    User[User] &lt;-- \"Input/Output\" --&gt; LLM[LLM Agent]\n    LLM &lt;--&gt; Tools[Tools]\n    LLM &lt;-- \"Iterative Input/Output\" --&gt; Environment[User, Other Users, Other Agents]\n    User &lt;-- \"Iterative Input/Output\" --&gt; Environment\n\n    style LLM fill:#e1f5fe\n    style User fill:#f3e5f5\n    style Tools fill:#fff3e0\n    style Environment fill:#e8f5e8\n</code></pre> <p>To allow this for the users of the framework, we have designed an extendible abstract class called <code>HIL</code> (Human In the Loop). Additionally we have implemented a local chat server for quick development and prototyping of such behaviours. Please refer to the following sections for futher information:</p> <ul> <li>Local ChatUI</li> <li>Custom HIL Behaviour</li> </ul>"},{"location":"llm_support/prompts/","title":"Prompts and Context Injection","text":"<p>Prompts are a fundamental part of working with LLMs in the Railtracks framework. This guide explains how to create dynamic prompts that use our context injection feature to make your prompts more flexible and powerful.</p>"},{"location":"llm_support/prompts/#understanding-prompts-in-railtracks","title":"Understanding Prompts in Railtracks","text":"<p>In Railtracks, prompts are provided as system messages or user messages when interacting with LLMs. These messages guide the LLM's behavior and responses.</p>"},{"location":"llm_support/prompts/#context-injection","title":"Context Injection","text":"<p>Railtracks provides a powerful feature called \"context injection\" (also referred to as \"prompt injection\") that allows you to dynamically insert values from the global context into your prompts. This makes your prompts more flexible and reusable across different scenarios.</p>"},{"location":"llm_support/prompts/#what-is-context-injection","title":"What is Context Injection?","text":"<p>Context injection refers to the practice of dynamically inserting values into a prompt template. This is especially useful when your prompt needs information that isn't known until runtime.</p> <p>Passing prompt details up the chain can be expensive in both tokens and latency. In many cases, it's more efficient to inject values directly into a prompt using our context system.</p>"},{"location":"llm_support/prompts/#how-context-injection-works","title":"How Context Injection Works","text":"<ol> <li>Define placeholders in your prompts using curly braces: <code>{variable_name}</code></li> <li>Set values in the Railtracks context (see Context Management for details)</li> <li>When the prompt is processed, the placeholders are replaced with the corresponding values from the context</li> </ol>"},{"location":"llm_support/prompts/#basic-example","title":"Basic Example","text":"<pre><code>import railtracks as rt\n\n# Define a prompt with placeholders\nsystem_message = \"You are a {role} assistant specialized in {domain}.\"\n\n# Create an LLM node with this prompt\nassistant = rt.agent_node(\n    name=\"Assistant\",\n    system_message=system_message,\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n)\n\n# Run with context values\nwith rt.Session(context={\"role\": \"technical\", \"domain\": \"Python programming\"}):\n    response = await rt.call(assistant, user_input=\"Help me understand decorators.\")\n</code></pre> <p>In this example, the system message will be expanded to: \"You are a technical assistant specialized in Python programming.\"</p>"},{"location":"llm_support/prompts/#enabling-and-disabling-context-injection","title":"Enabling and Disabling Context Injection","text":"<p>Context injection is enabled by default but can be disabled if needed:</p> <pre><code># Disable context injection for a specific run\nwith rt.Session(\n    prompt_injection=False\n): ...\n\n# or globally via \nrt.set_config(prompt_injection=False)\n</code></pre> <p>This may be useful when formatting prompts that should not change based on the context.</p> <p>Message-Level Control</p> <p>Context injection can be controlled at the message level using the <code>inject_prompt</code> parameter:</p> <pre><code># This message will have context injection applied\nsystem_msg = rt.llm.Message(role=\"system\", content=\"You are a {role}.\", inject_prompt=True)\n\n# This message will not have context injection applied\nuser_msg = rt.llm.Message(role=\"user\", content=\"Tell me about {topic}.\", inject_prompt=False)\n</code></pre> <p>This can be useful when you want to control which messages should have context injected and which should not. </p> <p>As an example, in a Math Assistant, you might want to inject context into the system message, but not the user message that may contain LaTeX that has <code>{}</code> characters. To prevent formatting issues, you can set <code>inject_prompt=False</code> for the user message.</p>"},{"location":"llm_support/prompts/#escaping-placeholders","title":"Escaping Placeholders","text":"<p>If you need to include literal curly braces in your prompt without triggering context injection, you can escape them by doubling the braces:</p> <pre><code># This will not be replaced with a context value\n\"Use the {{variable}} placeholder in your code.\"\n</code></pre>"},{"location":"llm_support/prompts/#debugging-prompts","title":"Debugging Prompts","text":"<p>If your prompts aren't producing the expected results:</p> <ol> <li>Check context values: Ensure the context contains the expected values for your placeholders</li> <li>Verify prompt injection is enabled: Check that <code>prompt_injection=True</code> in your session configuration</li> <li>Look for syntax errors: Ensure your placeholders use the correct format <code>{variable_name}</code></li> </ol>"},{"location":"llm_support/prompts/#example-reusable-prompt-templates","title":"Example (Reusable Prompt Templates)","text":"<p>You can create reusable prompt templates that adapt to different scenarios:</p> <pre><code>import railtracks as rt\nfrom railtracks.llm import OpenAILLM\n\n# Define a template with multiple placeholders\ntemplate = \"\"\"You are a {assistant_type} assistant.\nYour task is to help the user with {task_type} tasks.\nUse a {tone} tone in your responses.\nThe user's name is {user_name}.\"\"\"\n\n# Create an LLM node with this template\nassistant = rt.agent_node(\n    name=\"Dynamic Assistant\",\n    system_message=template,\n    llm=OpenAILLM(\"gpt-4o\"),\n)\n\n# Different context for different scenarios\ncustomer_support_context = {\n    \"assistant_type\": \"customer support\",\n    \"task_type\": \"troubleshooting\",\n    \"tone\": \"friendly and helpful\",\n    \"user_name\": \"Alex\"\n}\n\ntechnical_expert_context = {\n    \"assistant_type\": \"technical expert\",\n    \"task_type\": \"programming\",\n    \"tone\": \"professional\",\n    \"user_name\": \"Taylor\"\n}\n\n# Run with different contexts for different scenarios\nwith rt.Session(context=customer_support_context):\n    response1 = await rt.call(assistant, user_input=\"My product isn't working.\")\n\nwith rt.Session(context=technical_expert_context):\n    response2 = await rt.call(assistant, user_input=\"How do I implement a binary tree?\")\n</code></pre>"},{"location":"llm_support/prompts/#benefits-of-context-injection","title":"Benefits of Context Injection","text":"<p>Using context injection provides several advantages:</p> <ol> <li>Reduced token usage: Avoid passing the same context information repeatedly</li> <li>Improved maintainability: Update prompts in one place</li> <li>Dynamic adaptation: Adjust prompts based on runtime conditions</li> <li>Separation of concerns: Keep prompt templates separate from variable data</li> <li>Reusability: Use the same prompt template with different contexts</li> </ol>"},{"location":"llm_support/providers/","title":"Supported Providers","text":"<p>We currently support connecting to different available LLMs through the following providers:</p> <ul> <li>OpenAI - GPT models</li> <li>Anthropic - Claude models</li> <li>Cohere - Cohere models</li> <li>Gemini - Google's Gemini models</li> <li>Azure AI Foundry - Azure-hosted models</li> <li>Ollama - Local and self-hosted models</li> <li>HuggingFace - HuggingFace Serverless Inference models</li> </ul> <p>This allows you to use the same codebase to interact with different LLMs, making it easy to switch providers or use multiple providers in parallel, completely abstracting the underlying API differences.</p> <p>Take a look at the examples below to see how using different providers look for achieving the same task.</p>"},{"location":"llm_support/providers/#quick-start-examples","title":"Quick Start Examples","text":"OpenAIAnthropicCohereGeminiAzure AI FoundryOllamaHuggingFace <p>Environment Variables Configuration</p> <p>Make sure you set the appropriate environment variable keys for your specific provider. By default, Railtracks uses the <code>dotenv</code> framework to load environment variables from a <code>.env</code> file. Variable name for the API key: <code>OPENAI_API_KEY</code></p> <pre><code>import railtracks as rt\nfrom dotenv import load_dotenv\nload_dotenv()  # Load environment variables from .env file\n\nmodel = rt.llm.OpenAILLM(\"gpt-4o\")\n</code></pre> <p>Environment Variables Configuration</p> <p>Make sure you set the appropriate environment variable keys for your specific provider. By default, Railtracks uses the <code>dotenv</code> framework to load environment variables from a <code>.env</code> file. Variable name for the API key: <code>ANTHROPIC_API_KEY</code></p> <pre><code>import railtracks as rt\nfrom dotenv import load_dotenv\nload_dotenv()  # Load environment variables from .env file\n\nmodel = rt.llm.AnthropicLLM(\"claude-sonnet-4\")\n</code></pre> <p>Environment Variables Configuration</p> <p>Make sure you set the appropriate environment variable keys for your specific provider. By default, RailTracks uses the <code>dotenv</code> framework to load environment variables from a <code>.env</code> file. Variable name for the API key: <code>COHERE_API_KEY</code></p> <pre><code>import railtracks as rt\nfrom dotenv import load_dotenv\nload_dotenv()  # Load environment variables from .env file\n\nmodel = rt.llm.CohereLLM(\"command-a-03-2025\")\n</code></pre> <p>Environment Variables Configuration</p> <p>Make sure you set the appropriate environment variable keys for your specific provider. By default, Railtracks uses the <code>dotenv</code> framework to load environment variables from a <code>.env</code> file. Variable name for the API key: <code>GEMINI_API_KEY</code></p> <pre><code>import railtracks as rt\nfrom dotenv import load_dotenv\nload_dotenv()  # Load environment variables from .env file\n\nmodel = rt.llm.GeminiLLM(\"gemini-2.5-flash\")\n</code></pre> <pre><code>import railtracks as rt\n# make sure to configure your environment variables for Azure AI\n\nmodel = rt.llm.AzureAILLM(\"azure_ai/deepseek-r1\")\n</code></pre> <pre><code>import railtracks as rt\n# make sure to configure your environment variables for Ollama\n\nmodel = rt.llm.OllamaLLM(\"deepseek-r1:8b\")\n</code></pre> <p>Environment Variables Configuration</p> <p>Make sure you set the appropriate environment variable keys for your specific provider. By default, Railtracks uses the <code>dotenv</code> framework to load environment variables from a <code>.env</code> file. Variable name for the API key: <code>HF_TOKEN</code></p> <p>Tool Calling Support</p> <p>For HuggingFace serverless inference models, you need to make sure that the model you are using supports tool calling. We DO NOT  check for tool calling support in HuggingFace models. If you are using a model that does not support tool calling, it will default to regular chat, even if the <code>tool_nodes</code> parameter is provided.</p> <p>In case of HuggingFace, <code>model_name</code> must be of the format:</p> <ul> <li><code>huggingface/&lt;provider&gt;/&lt;hf_org_or_user&gt;/&lt;hf_model&gt;</code></li> <li><code>&lt;provider&gt;/&lt;hf_org_or_user&gt;/&lt;hf_model&gt;</code>\"</li> </ul> <p>Here are a few example models that you can use:</p> <pre><code>rt.llm.HuggingFaceLLM(\"together_ai/meta-llama/Llama-3.3-70B-Instruct\") \nrt.llm.HuggingFaceLLM(\"sambanova/meta-llama/Llama-3.3-70B-Instruct\")\n\n# does not support tool calling\nrt.llm.HuggingFaceLLM(\"featherless-ai/mistralai/Mistral-7B-Instruct-v0.2\")\n</code></pre> <pre><code>import railtracks as rt\n# make sure to configure your environment variables for HuggingFace\n\nmodel = rt.llm.HuggingFaceLLM(\"together/deepseek-ai/DeepSeek-R1\")\n</code></pre> <pre><code># Insert the model you want to use in your agent.\nGeneralAgent = rt.agent_node(\n    llm=model,\n    system_message=\"You are a helpful AI assistant.\",\n)\n</code></pre> <p>Tool Calling Capabilities</p> <p>If you want to use tool calling capabilities by passing the <code>tool_nodes</code> parameter to the <code>agent_node</code>, you can do so with any of the above providers. However, you need to ensure that the provider and the specific LLM model you are using support tool calling.</p>"},{"location":"llm_support/providers/#writing-custom-llm-providers","title":"Writing Custom LLM Providers","text":"<p>We hope to cover most of the common and widely used LLM providers, but if you need to use a provider that is not currently supported, you can implement your own LLM provider by subclassing <code>LLMProvider</code> and implementing the required methods. </p> <p>For our implementation, we have benefited from the amazing LiteLLM framework, which provides excellent multi-provider support.</p> <p>Custom Provider Documentation</p> <p>Please check out the <code>llm</code> module for more details on how to build a integration.</p>"},{"location":"llm_support/streaming/","title":"Streaming","text":""},{"location":"llm_support/streaming/#what-is-streaming","title":"What Is Streaming?","text":"<p>Streaming is a way to make your agent feel more responsive. Instead of waiting for the complete response, you can stream intermediate results as they arrive.</p>"},{"location":"llm_support/streaming/#streaming-support","title":"Streaming Support","text":"<p>Railtracks supports streaming responses from your agent. To interact with a stream, just set the appropriate flag when creating your LLM.</p> <pre><code>import railtracks.llm as llm\n\nmodel = llm.OpenAILLM(model_name=\"gpt-4o\", stream=True)\n</code></pre> <p>When you call the LLM, it will return a generator that you can iterate through:</p> <pre><code>model = llm.OpenAILLM(model_name=\"gpt-4o\", stream=True)\n\nresponse = model.chat(llm.MessageHistory([\n    llm.UserMessage(\"Tell me who you are are\"),\n]))\n\n# The response object can act as an iterator returning string chunks terminating with the complete message.\nfor chunk in response:\n    print(chunk)\n</code></pre>"},{"location":"llm_support/streaming/#agent-support","title":"Agent Support","text":"<p>Agents in Railtracks also support streamed responses. When creating your agent, you provide an LLM with streaming enabled:</p> <pre><code>import railtracks as rt\n\nagent = rt.agent_node(\n    llm=rt.llm.OpenAILLM(model_name=\"gpt-4o\", stream=True),\n)\n</code></pre> <p>The output of the agent will be a generator containing a sequence of strings, followed by the complete message.</p> <p>Usage</p> <p><pre><code>agent = rt.agent_node(\n    llm=rt.llm.OpenAILLM(model_name=\"gpt-4o\", stream=True),\n)\n\n@rt.session\nasync def main():\n    result = await rt.call(agent, rt.llm.MessageHistory([\n        rt.llm.UserMessage(\"Tell me who you are are\"),\n        ]))\n\n    # The response object can act as an iterator returning string chunks terminating with the complete message.\n\n    for chunk in result:\n        print(chunk)\n</code></pre> `</p> <p>Warning</p> <p>When using streaming, you should fully exhaust the returned object within the session. If you do this outside of the session, the visualizer suite will not work as expected.</p> <p>Warning</p> <p>Streaming is not currently supported for tool-calling agents. See issue #756.</p>"},{"location":"observability/broadcasting/","title":"Broadcasting","text":"<p>Broadcasting lets you monitor your agents' progress in real time by sending live updates during execution. This can be useful for:</p> <ul> <li>Displaying progress in a UI or dashboard</li> <li>Logging intermediate steps for debugging</li> <li>Triggering alerts based on runtime events</li> </ul> <p>Railtracks supports basic data broadcasting, enabling you to receive these updates via a callback function.</p>"},{"location":"observability/broadcasting/#usage","title":"Usage","text":"<p>To enable broadcasting, provide a callback function to the <code>broadcast_callback</code> parameter in <code>set_config</code>. This function will receive broadcasting updates:</p> <pre><code>def example_broadcasting_handler(data):\n    print(f\"Received data: {data}\")\n\nrt.set_config(broadcast_callback=example_broadcasting_handler)\n</code></pre> <p>With broadcasting enabled, call <code>rt.broadcast(...)</code> inside any function run in RT to invoke the handler.</p> <pre><code>@rt.function_node\nasync def example_node(data: list[str]):\n    await rt.broadcast(f\"Handling {len(data)} items\")\n</code></pre> <p>Warning</p> <p>Currently, only string messages can be broadcasted.</p>"},{"location":"observability/chat/","title":"Chat","text":""},{"location":"observability/chat/#overview","title":"Overview","text":"<p>Conversational interactions with agents is currently one of the most common methods. This guide will go over how to quickly visualize and come up with a proof of concept assessing how the potential user experience would look using <code>Railtracks</code>.</p> <pre><code>import random\nimport datetime\n\nimport railtracks as rt\n\ndef get_todays_date(tell_time: bool, tell_location: bool):\n    \"\"\"\n    Returns the correct date once called. Time can also be provided.\n\n    Args:\n        tell_time (bool): if set to True will also return time\n    \"\"\"\n\n    if tell_location:\n        raise ValueError(\"Location is not supported in this example.\")\n\n    if tell_time:\n        if random.random() &lt; 0.8:\n            raise RuntimeError(\"Random exception occurred!\")\n        return str(datetime.datetime.now())\n    else:\n        return str(datetime.date.today())\n\nINSTRUCTION =\"\"\"\nYou are a helpful agent that can analyze images and answer questions about them.\n\"\"\"\n\nChatBot = rt.chatui_node(\n    port=5000,\n    auto_open=True,\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n    pretty_name=\"ChatBot\",\n    system_message=rt.llm.SystemMessage(\n        INSTRUCTION\n    ),\n    tool_nodes={\n        get_todays_date,\n    },\n)\n\nwith rt.Session(timeout=600) as session:\n    resp = await rt.call(\n        ChatBot,\n        rt.llm.MessageHistory(),\n    )\n</code></pre>"},{"location":"observability/chat/#walkthrough","title":"Walkthrough","text":"<p>The above code defines a simple tool that provides the agent with either date or datetime together and randomly throws a bug. This is done on purpose to demonstrate the tool view and illustration of different invocation outcomes.</p> <p>By using the INSERT NAME here, you can easily do this. Upon running the above code, the following window will open: </p> <p>Here you can see the conversation history with the agent markdown formatted. Additionally, you can also use the <code>Invoked Tools</code> tab to visualize the input/output of the tools the agent has used. </p> <p>While this is meant to be used locally, the styling of the UI can be adjusted by changing the files <code>chat.css</code> and <code>chat.html</code>.</p>"},{"location":"observability/error_handling/","title":"Error Handling","text":"<p>Railtracks (RT) provides a comprehensive error handling system designed to give developers clear, actionable feedback when things go wrong. The framework uses a hierarchy of specialized exceptions that help you understand exactly what went wrong and where.</p>"},{"location":"observability/error_handling/#error-hierarchy","title":"Error Hierarchy","text":"<p>All Railtracks errors inherit from the base <code>RTError</code> class, which provides colored console output and structured error reporting.</p> <pre><code>RTError (base)\n\u251c\u2500\u2500 NodeCreationError\n\u251c\u2500\u2500 NodeInvocationError\n\u251c\u2500\u2500 LLMError\n\u251c\u2500\u2500 GlobalTimeOutError\n\u251c\u2500\u2500 ContextError\n\u2514\u2500\u2500 FatalError\n</code></pre>"},{"location":"observability/error_handling/#error-types","title":"Error Types","text":""},{"location":"observability/error_handling/#internally-raised-errors","title":"Internally Raised Errors","text":"<p>These errors are automatically raised by Railtracks when issues occur during execution. All inherit from <code>RTError</code> and provide colored terminal output with debugging information.</p> <ul> <li><code>NodeCreationError</code> - Raised during node setup and validation</li> <li><code>NodeInvocationError</code> - Raised during node execution (has <code>fatal</code> flag)</li> <li><code>LLMError</code> - Raised during LLM operations (includes <code>message_history</code>)</li> <li><code>GlobalTimeOutError</code> - Raised when execution exceeds timeout</li> <li><code>ContextError</code> - Raised for context related issues</li> </ul> <p>All internal errors include helpful debugging notes and formatted error messages to guide troubleshooting.</p>"},{"location":"observability/error_handling/#user-raised-errors","title":"User-Raised Errors","text":"<p><code>FatalError</code> - The only error type designed for developers to raise manually when encountering unrecoverable situations. When raised within a run it will stop it.</p> <p>Usage</p> <pre><code>def critical_function():\n    from railtracks.exceptions import FatalError\n    raise FatalError(\"A critical error occurred.\")\n</code></pre>"},{"location":"observability/error_handling/#error-handling-patterns","title":"Error Handling Patterns","text":"Basic Error Handling <pre><code>from railtracks.exceptions import NodeInvocationError, LLMError\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    result = await rt.call(func, \"Tell me about machine learning\")\nexcept NodeInvocationError as e:\n    if e.fatal:\n        # Fatal errors should stop execution\n        logger.error(f\"Fatal node error: {e}\")\n        raise\n    else:\n        # Non-fatal errors can be handled gracefully\n        logger.warning(f\"Node error (recoverable): {e}\")\n        # Implement retry logic or fallback\n\nexcept LLMError as e:\n    logger.error(f\"LLM operation failed: {e.reason}\")\n    # Maybe retry with different parameters\n    # Or fallback to a simpler approach\n</code></pre> Comprehensive Error Handling <pre><code>from railtracks.exceptions import (\n    NodeCreationError, NodeInvocationError, \n    LLMError, GlobalTimeOutError, ContextError, FatalError\n)\n\ntry:\n    # Setup phase\n    node = rt.agent_node(\n        llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n        system_message=\"You are a helpful assistant\",\n    )\n\n    # Configure timeout\n    rt.set_config(timeout=60.0)\n\n    # Execution phase\n    result = await rt.call(node, user_input=\"Explain quantum computing\")\n\nexcept NodeCreationError as e:\n    # Configuration or setup issue\n    logger.error(\"Node setup failed - check your configuration\")\n    print(e)  # Shows debugging tips\n\nexcept NodeInvocationError as e:\n    # Runtime execution issue\n    if e.fatal:\n        logger.error(\"Fatal execution error - stopping\")\n        raise\n    else:\n        logger.warning(\"Recoverable execution error\")\n        # Implement recovery strategy\n\nexcept LLMError as e:\n    # LLM-specific issue\n    logger.error(f\"LLM error: {e.reason}\")\n    if e.message_history:\n        # Analyze conversation for debugging\n        pass\n\nexcept GlobalTimeOutError as e:\n    # Execution took too long\n    logger.error(f\"Execution timed out after {e.timeout}s\")\n    # Maybe increase timeout or optimize graph\n\nexcept ContextError as e:\n    # Context management issue\n    logger.error(\"Context error - check your context setup\")\n    print(e)  # Shows debugging tips\n\nexcept FatalError as e:\n    # User-defined critical error\n    logger.critical(f\"Fatal error: {e}\")\n    # Implement emergency shutdown procedures\n\nexcept Exception as e:\n    # Non-RT errors\n    logger.error(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"observability/error_handling/#error-recovery-strategies","title":"Error Recovery Strategies","text":"Retry with Exponetial Backoff <pre><code>import asyncio\nimport railtracks as rt\nfrom railtracks.exceptions import NodeInvocationError, NodeCreationError\n\nasync def call_with_retry(node, user_input, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return await rt.call(node, user_input=user_input)\n        except (NodeInvocationError, LLMError) as e:\n            if attempt == max_retries - 1:\n                raise  # Last attempt, re-raise\n\n            wait_time = 2 ** attempt  # Exponential backoff\n            logger.warning(f\"Attempt {attempt + 1} failed, retrying in {wait_time}s\")\n            await asyncio.sleep(wait_time)\n</code></pre> Graceful Fallback <pre><code>from railtracks.exceptions import NodeInvocationError\n\nasync def call_with_fallback(primary_node, fallback_node, user_input):\n    try:\n        return await rt.call(primary_node, user_input=user_input)\n    except NodeInvocationError as e:\n        if not e.fatal:\n            logger.info(\"Primary execution failed, trying fallback\")\n            return await rt.call(fallback_node, user_input=user_input)\n        raise\n</code></pre>"},{"location":"observability/error_handling/#best-practices","title":"Best Practices","text":""},{"location":"observability/error_handling/#1-handle-errors-at-the-right-level","title":"1. Handle Errors at the Right Level","text":"<ul> <li>Handle <code>NodeCreationError</code> during setup/configuration</li> <li>Handle <code>NodeInvocationError</code> during execution with appropriate recovery</li> <li>Handle <code>LLMError</code> with retry logic and fallbacks</li> <li>Let <code>FatalError</code> bubble up to stop execution</li> </ul>"},{"location":"observability/error_handling/#2-use-error-information","title":"2. Use Error Information","text":"<ul> <li>Check the <code>fatal</code> flag on <code>NodeInvocationError</code></li> <li>Examine <code>message_history</code> in <code>LLMError</code> for debugging</li> <li>Read the <code>notes</code> property for debugging tips</li> </ul>"},{"location":"observability/error_handling/#3-implement-appropriate-recovery","title":"3. Implement Appropriate Recovery","text":"<ul> <li>Retry transient errors (network issues, rate limits)</li> <li>Fallback for recoverable errors</li> <li>Fail fast for configuration errors</li> <li>Log appropriately for debugging</li> </ul>"},{"location":"observability/error_handling/#4-monitor-and-alert","title":"4. Monitor and Alert","text":"<p>For detailed logging and monitoring strategies, see Logging.</p>"},{"location":"observability/error_handling/#debugging-tips","title":"Debugging Tips","text":"<ol> <li>Enable Debug Logging: Railtracks errors include colored output and debugging notes</li> <li>Check Error Properties: Many errors include additional context (notes, message_history, etc.)</li> <li>Use Message History: LLMError includes conversation context for debugging</li> <li>Examine Stack Traces: RT errors preserve the full stack trace for debugging</li> <li>Test Error Scenarios: Write tests that verify your error handling works correctly</li> </ol> <p>The Railtracks error system is designed to fail fast when appropriate, provide clear feedback, and enable robust error recovery strategies.</p>"},{"location":"observability/logging/","title":"Logging","text":"<p>Railtracks provides built-in logging to help track the execution of your flows. Logs are automatically generated and can be viewed in the terminal or saved to a file.</p> Example Logs <pre><code>[+3.525  s] RT          : INFO     - START CREATED Github Agent\n[+8.041  s] RT          : INFO     - Github Agent CREATED create_issue\n[+8.685  s] RT          : INFO     - create_issue DONE\n[+14.333 s] RT          : INFO     - Github Agent CREATED assign_copilot_to_issue\n[+14.760 s] RT          : INFO     - assign_copilot_to_issue DONE\n[+17.540 s] RT          : INFO     - Github Agent CREATED assign_copilot_to_issue\n[+18.961 s] RT          : INFO     - assign_copilot_to_issue DONE\n[+23.401 s] RT          : INFO     - Github Agent DONE\n</code></pre> <p>Critical</p> <p>Every log sent by Railtracks will contain a parameter in <code>extras</code> for <code>session_id</code> which will be uuid tied to the session the error was thrown in.</p>"},{"location":"observability/logging/#configuring-logging","title":"Configuring Logging","text":""},{"location":"observability/logging/#logging-levels","title":"Logging Levels","text":"<p>Railtracks supports four logging levels:</p> <ol> <li><code>VERBOSE</code>: Includes all logs, including <code>DEBUG</code>.</li> <li><code>REGULAR</code>: (Default) Includes <code>INFO</code> and above. Ideal for local development.</li> <li><code>QUIET</code>: Includes <code>WARNING</code> and above. Recommended for production.</li> <li><code>NONE</code>: Disables all logging.</li> </ol> <pre><code>rt.set_config(logging_setting=\"VERBOSE\")\nrt.set_config(logging_setting=\"REGULAR\")\nrt.set_config(logging_setting=\"QUIET\")\nrt.set_config(logging_setting=\"NONE\")\n</code></pre>"},{"location":"observability/logging/#logging-handlers","title":"Logging Handlers","text":"<p>Console Handler</p> <p>By default, logs are printed to <code>stdout</code> and <code>stderr</code>.</p> <p>File Handler</p> <p>To save logs to a file, pass a <code>log_file</code> parameter to the config:</p> <pre><code>rt.set_config(log_file=\"my_logs.log\")\n</code></pre> <p>Custom Handlers</p> <p>Railtracks uses the standard Python <code>logging</code> module with the <code>RT</code> prefix. You can attach custom handlers:</p> <pre><code>import logging\n\n\nclass CustomHandler(logging.Handler):\n    def emit(self, record):\n        # Custom logging logic\n        pass\n\n\nlogger = logging.getLogger()\nlogger.addHandler(CustomHandler())\n</code></pre>"},{"location":"observability/logging/#example-usage","title":"Example Usage","text":"<p>You can configure logging globally or per-run.</p> <p>Global Configuration</p> <pre><code>rt.set_config(logging_setting=\"VERBOSE\", log_file=\"my_logs.log\")\n</code></pre> <p>This will apply to all flows.</p> <p>Scoped Configuration</p> <p><pre><code>with rt.Session(logging_setting=\"VERBOSE\", log_file=\"my_logs.log\") as runner:\n    pass\n</code></pre> Applies only within the context of the <code>Session</code>.</p>"},{"location":"observability/logging/#forwarding-logs-to-external-services","title":"Forwarding Logs to External Services","text":"<p>You can forward logs to services like Loggly, Sentry, or Conductr by attaching custom handlers. Refer to each provider's documentation for integration details.</p> ConductrLogglySentry <pre><code>import railtownai\n\nRAILTOWN_API_KEY = \"YOUR_RAILTOWN_API_KEY\"\nrailtownai.init(RAILTOWN_API_KEY)\n</code></pre> <pre><code>import logging\nfrom loggly.handlers import HTTPSHandler\n\nLOGGLY_TOKEN = \"YOUR_LOGGLY_TOKEN\"\nhttps_handler = HTTPSHandler(\n    url=f\"https://logs-01.loggly.com/inputs/{LOGGLY_TOKEN}/tag/python\"\n)\nlogger = logging.getLogger()\nlogger.addHandler(https_handler)\n</code></pre> <pre><code>import sentry_sdk\n\nsentry_sdk.init(\n    dsn=\"https://examplePublicKey@o0.ingest.sentry.io/0\",\n    send_default_pii=True,  # Collects additional metadata\n)\n</code></pre>"},{"location":"observability/logging/#log-message-examples","title":"Log Message Examples","text":"DEBUG Messages Type Example Runner Created <code>RT.Runner   : DEBUG    - Runner &lt;RUNNER_ID&gt; is initialized</code> Node Created <code>RT.Publisher: DEBUG    - RequestCreation(current_node_id=&lt;PARENT_NODE_ID&gt;, new_request_id=&lt;REQUEST_ID&gt;, running_mode=async, new_node_type=&lt;NODE_NAME&gt;, args=&lt;INPUT_ARGS&gt;, kwargs=&lt;INPUT_KWARGS&gt;)</code> Node Completed <code>RT.Publisher: DEBUG    - &lt;NODE_NAME&gt; DONE with result &lt;RESULT&gt;</code> INFO Messages Type Example Initial Request <code>RT          : INFO     - START CREATED &lt;NODE_NAME&gt;</code> Invoking Nodes <code>RT          : INFO     - &lt;PARENT_NODE_NAME&gt; CREATED &lt;CHILD_NODE_NAME&gt;</code> Node Completed <code>RT          : INFO     - &lt;NODE_NAME&gt; DONE</code> Run Data Saved <code>RT.Runner   : INFO     - Saving execution info to .railtracks\\&lt;RUNNER_ID&gt;.json</code> WARNING Messages Type Example Overwriting File <code>RT.Runner   : WARNING  - File .railtracks\\&lt;RUNNER_ID&gt;.json already exists, overwriting...</code> ERROR Messages Type Example Node Failed <code>RT          : ERROR    - &lt;NODE_NAME&gt; FAILED</code>"},{"location":"observability/visualization/","title":"Visualization","text":"<p>One of the number one complaints when working with LLMs is that they can be a black box. Agentic applications exacerbate this problem by adding even more complexity. Railtracks aims to make it easier than ever to visualize your runs. </p> <p>We support:</p> <ul> <li>Local Visualization (no sign up required) </li> <li>Remote Visualization (Ideal for deployed agents)</li> </ul>"},{"location":"observability/visualization/#local-development-visualization","title":"Local Development Visualization","text":"<p>Railtracks comes with a built-in visualization tool that runs locally with no sign up required.</p>"},{"location":"observability/visualization/#usage","title":"Usage","text":"Install CLI tTool<pre><code>pip install railtracks-cli\n</code></pre> Initialize UI and Start<pre><code>railtracks init\nrailtracks viz\n</code></pre> <p>This will create a <code>.railtracks</code> directory in your current working directory setting up the web app in your web browser</p> <p> </p> <p>Saving State</p> <p>By default, all of your runs will be saved to the <code>.railtracks</code> directory so you can view them locally. If you don't want that, set the flag to <code>False</code>:</p> <pre><code>import railtracks as rt\n\n# set the configuration globally\nrt.set_config(save_state=True)\n\n# or by session \n@rt.session(save_state=False)\nasync def main(): ...\n\nwith rt.Session(save_state=True): ...\n</code></pre>"},{"location":"observability/visualization/#remote-visualization","title":"Remote Visualization","text":"<p>Note</p> <p>Would you be interested in observability for your agents in an all in one platform?</p> <p>Please fill out the following form</p>"},{"location":"quickstart/quickstart/","title":"Quickstart","text":"<p>Large Language Models (LLMs) are powerful, but they\u2019re not enough on their own. Railtracks gives them structure, tools, and visibility so you can build agents that actually get things done.  </p> <p>In this quickstart, you\u2019ll install Railtracks, run your first agent, and visualize its execution \u2014 all in a few minutes.</p>"},{"location":"quickstart/quickstart/#1-installation","title":"1. Installation","text":"Install Library<pre><code>pip install railtracks\npip install railtracks-cli\n</code></pre> <p>Note</p> <p><code>railtracks-cli</code> is optional, but required for the visualization step. </p>"},{"location":"quickstart/quickstart/#2-running-your-agent","title":"2. Running your Agent","text":"<p>Define an agent with a model and system message, then call it with a prompt:</p> <pre><code>import asyncio\nimport railtracks as rt\n\n# To create your agent, you just need a model and a system message. \nAgent = rt.agent_node(\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n    system_message=\"You are a helpful AI assistant.\"\n)\n\n# Now to call the Agent, we just need to use the `rt.call` function\nasync def main():\n    result = await rt.call(\n        Agent,\n        \"Hello, what can you do?\"\n    )\n    return result\n\nresult = asyncio.run(main())\n</code></pre> <p>Example Output</p> <p>Your exact output will vary depending on the model. Example Response<pre><code>Hello! I can help you out with a wide range of tasks...\n</code></pre></p> No API key set? <p>Make sure you are calling a model you have an API key set in your <code>.env</code> file. </p> .env<pre><code>OPENAI_API_KEY=\"...\"\nANTHROPIC_API_KEY=\"...\"\n</code></pre> <p>Railtracks supports many of the most popular model providers. See the full list</p> Jupyter Notebooks <p>If you\u2019re running this in a Jupyter notebook, remember that notebooks already run inside an event loop. In that case, call <code>await rt.call(...)</code> directly:</p>"},{"location":"quickstart/quickstart/#3-visualize-the-run","title":"3. Visualize the Run","text":"<p>Railtracks has a built-in visualizer to inspect and review your agent runs. </p> Initialize Visualizer (Run Once)<pre><code>railtracks init\n</code></pre> Run Visualizer<pre><code>railtracks viz\n</code></pre> <p> </p> <p>This will open a web interface showing the execution flow, node interactions, and performance metrics of your agentic system.</p> <p>Next Steps</p> <p>You\u2019ve got your first agent running! Here\u2019s where to go next:</p> <p>Learn the Basics</p> <p>Build Something</p> <ul> <li>Building your First Agent</li> <li> <p>Running your First Agent</p> </li> <li> <p>What is an Agent?</p> </li> <li>What is a Tool?</li> </ul>"},{"location":"system_internals/concepts/","title":"Core Concepts","text":""},{"location":"system_internals/concepts/#node-based-execution-model","title":"Node-Based Execution Model","text":"<ul> <li>Nodes: Atomic units of work (functions, AI agents, tools)</li> <li>Execution Graph: Dynamic tree built during runtime</li> <li>Call Semantics: How nodes invoke other nodes</li> </ul>"},{"location":"system_internals/concepts/#immutable-state-architecture","title":"Immutable State Architecture","text":"<ul> <li>Forest Data Structures: Why trees, not graphs</li> <li>Temporal Tracking: Time-travel debugging capabilities</li> <li>State Snapshots: Point-in-time execution views</li> </ul>"},{"location":"system_internals/concepts/#event-driven-coordination","title":"Event-Driven Coordination","text":"<ul> <li>Request Lifecycle: Creation \u2192 Execution \u2192 Completion</li> <li>Message Types: Success, Failure, Streaming, Fatal</li> <li>Loose Coupling: Components communicate via events only</li> </ul>"},{"location":"system_internals/concepts/#session-isolation","title":"Session Isolation","text":"<ul> <li>Context Boundaries: Each session is independent</li> <li>Resource Management: Automatic cleanup and lifecycle</li> <li>Configuration Scoping: Settings apply per-session</li> </ul>"},{"location":"system_internals/coordinator/","title":"Coordinator","text":""},{"location":"system_internals/coordinator/#overview","title":"Overview","text":"<p>The <code>Coordinator</code> is the central component responsible for invoking and managing the execution of tasks within the Railtracks system. It acts as the concrete invoker, receiving tasks and delegating them to the appropriate execution strategies. It ensures that every task is tracked from submission to completion, maintaining a comprehensive state of all ongoing and completed jobs.</p>"},{"location":"system_internals/coordinator/#key-components","title":"Key Components","text":""},{"location":"system_internals/coordinator/#coordinator_1","title":"<code>Coordinator</code>","text":"<p>This class orchestrates task execution. It maintains the system's state via <code>CoordinatorState</code>, uses different <code>AsyncioExecutionStrategy</code> implementations to run tasks, and listens for task completion events through the pub/sub system to keep the state up-to-date.</p>"},{"location":"system_internals/coordinator/#coordinatorstate","title":"<code>CoordinatorState</code>","text":"<p>A state container that holds a list of all <code>Job</code> objects. It tracks every task that is currently running or has been completed, providing a complete history of work handled by the <code>Coordinator</code>.</p>"},{"location":"system_internals/coordinator/#job","title":"<code>Job</code>","text":"<p>Represents a single unit of work. A <code>Job</code> is created when a task is submitted, and its lifecycle is tracked from an <code>opened</code> to a <code>closed</code> state. It records the task's identifiers, status, result, and timing information, offering a detailed view of each task's execution.</p>"},{"location":"system_internals/coordinator/#asyncioexecutionstrategy","title":"<code>AsyncioExecutionStrategy</code>","text":"<p>An execution strategy that uses asyncio for task execution. This strategy provides async-await style execution for tasks, allowing for efficient concurrent processing without the need for threads or processes. It handles task invocation, result processing, and error handling while publishing completion messages through the pub/sub system.</p>"},{"location":"system_internals/coordinator/#execution-flow","title":"Execution Flow","text":"<p>The execution of a task follows a well-defined sequence of events, ensuring reliable processing and state management:</p> <ol> <li>Submission: A task is submitted to the system via a call to <code>Coordinator.submit(task, mode)</code> where <code>mode</code> is the key for which <code>TaskExecutionStrategy</code> to be used.</li> <li>Job Creation: The <code>Coordinator</code> uses its member <code>CoordinatorState</code> object's <code>add_job</code> method which creates a <code>Job</code> instance for the submitted <code>Task</code> initialized with a status of <code>opened</code> and a start time.</li> <li>Delegation: The <code>Coordinator</code> determines the correct <code>TaskExecutionStrategy</code> based on the task's configuration and delegates the execution to it.</li> <li>Asynchronous Execution: The execution strategy runs the task asynchronously, allowing the <code>Coordinator</code> to manage other tasks concurrently.</li> <li>Completion Notification: Upon completion, the <code>TaskExecutionStrategy</code> publishes a <code>RequestCompletionMessage</code> to the pub/sub system.</li> <li>Handling Completion: The <code>Coordinator</code>, being a subscriber to these messages, receives the notification in its <code>handle_item</code> method.</li> <li>Finalizing the Job: The <code>Coordinator</code> finds the corresponding <code>Job</code> in its <code>CoordinatorState</code> using the <code>request_id</code> from the message and updates its status to <code>closed</code>, recording the final result and end time.</li> </ol>"},{"location":"system_internals/coordinator/#diagrams","title":"Diagrams","text":"<p>This diagram shows the sequence of interactions when a task is submitted and processed.</p> <p><pre><code>sequenceDiagram\n    participant A as Actor\n    participant C as Coordinator\n    participant CS as CoordinatorState\n    participant J as Job\n    participant TES as TaskExecutionStrategy\n    participant RT as RTPublisher\n\n    A-&gt;&gt;C: start(publisher)\n    A-&gt;&gt;C: submit(task)\n    C-&gt;&gt;RT: subscribe(callback)\n    C-&gt;&gt;CS: add_job(task)\n    CS-&gt;&gt;J: create_new(task)\n    J-&gt;&gt;CS: Job\n    C-&gt;&gt;TES: execute(task)\n    TES-&gt;&gt;C: RequestSuccess/Failure\n    TES-&gt;&gt;RT: publish(respone)\n\n    Note over RT: Coordinator is subscribed to RTPublisher and gets notified of the response\n</code></pre> Note: The Coordinator is subscribed to RTPublisher and gets notified of the response automatically through the pub/sub system.</p>"},{"location":"system_internals/node/","title":"Node","text":""},{"location":"system_internals/node/#overview","title":"Overview","text":""},{"location":"system_internals/node/#overview_1","title":"Overview","text":"<p>Nodes are the main components of Railtracks core of our abstractions. Looking at the abstract class, we can see that each Node needs the following methods implemented: Nodes are the main components of Railtracks core of our abstractions. Looking at the abstract class, we can see that each Node needs the following methods implemented:</p>"},{"location":"system_internals/node/#execution-flow","title":"Execution Flow","text":"<p>After the creation of a Node, execution primarily happens through the <code>call</code> method, which is responsible for invoking the node's logic and handling its execution flow. The <code>call</code> method can be called synchronously or asynchronously, depending on the node's configuration and the execution strategy in use.</p> <p>The scenario for the execution of a node is one of the following cases:</p> <p>I. Standalone Execution</p> <p>II. Within a Session Context:</p> <pre><code>A. **Top Level Node Execution**: In this case, the node is executed as the main entry point of the session, and it has full access to the session context and its resources.\n\nB. **Node Invoking Another Node**: Here, the node acts as a caller to another node, passing the necessary context and parameters for the invoked node to execute.\n</code></pre>"},{"location":"system_internals/node/#i-standalone-execution","title":"I. Standalone Execution","text":"<p>This is the simplest case where a node is executed independently, without being part of a larger workflow or session. This is mainly when we want to quickly test a node's functionality or when the node is designed to be used in isolation and we do not care about tracking its execution history or state.</p> <pre><code>graph TD\n    call[rt.call] --&gt; session[Temporary Session Context]\n    session--&gt; start[rt.interaction._start]\n    start --&gt; |await|startPublisher[rt.context.central.activate_publisher]\n    start --&gt; execute[rt.interaction._execute]\n    startPublisher --&gt; getPublisher[rt.context.get_publisher]\n    execute --&gt; getPublisher[rt.context.get_publisher]\n    getPublisher --&gt; publisher[publisher]\n    publisher --&gt; |await| awaitPublish[publisher.publish]\n    publisher --&gt; |await| listen[publisher.listener]\n</code></pre> <p>Here is a breakdown of the flow:</p> <ol> <li>Call Invocation: The process begins with the invocation of the <code>call</code> method on a node, which is the entry point for executing the node's logic.</li> <li>Session Context Wrapper: Inside call, if we identify if no session context exists, we create a temporary session context wrapper to manage the execution environment.</li> <li>Start Interaction: The <code>rt.interaction._start</code> function is called to firstly initalize the publisher and then execute the node's logic.</li> <li>Publisher Activation: The publisher is activated to handle message publishing and subscribing.</li> <li>Node Execution: The node's logic is executed within the <code>_execute</code> function, which handles the actual processing of the node.</li> <li>Node Execution: The node's logic is executed within the <code>_execute</code> function, which publishes a <code>RequestCreation</code> message to signal the start of the node's execution and then \"listens\" for completion messages and returns the final result.</li> <li>Cleanup: After execution, the publisher is shutdown and we exit the temporary session context.</li> </ol>"},{"location":"system_internals/node/#ii-within-a-session-context","title":"II. Within a Session Context","text":"<p>The recommended and intended way to execute agents (which are often comprised of interactive nodes) is within a session context. This allows for better management of state, history, and interactions between nodes, also allowing for monitoring, deployment, visualization, and debugging of the entire workflow.</p> <p>The workflow for executing a node within a session context is quite similar to the standalone execution, but with the removal of creating the \"temporary session\" since upon entering the session context, we already have a session context available. However, we still need to use the <code>_start</code> method to initialize the publisher and execute the node's logic, which corresponds point II.A from the list above.</p> <p>II.B is a sub-category of the context having been activated already as well and we therefore simply use the go directly to the <code>_execute</code> method to execute the node's logic.</p>"},{"location":"system_internals/overview/","title":"Railtracks Internal Architecture Overview","text":"<p>Welcome to the internal architecture overview of Railtracks, our framework for building agentic workflows. This document provides a high-level understanding of how the system is structured and how its different components interact with each other.</p>"},{"location":"system_internals/overview/#execution-flow","title":"Execution Flow","text":""},{"location":"system_internals/overview/#railtracks-internal-architecture-overview_1","title":"Railtracks Internal Architecture Overview","text":"<p>Welcome to the internal architecture overview of Railtracks, our framework for building agentic workflows. This document provides a high-level understanding of how the system is structured and how its different components interact with each other.</p>"},{"location":"system_internals/overview/#execution-flow_1","title":"Execution Flow","text":"<ol> <li>Agent Creation: Users create an agent using <code>rt.agent_node(...)</code> customizing the various different available parameters such as <code>llm</code>, <code>system_message</code>, and <code>tool_nodes</code>. </li> <li>Session Initialization: The <code>Session</code> is initialized as a context manager, which sets up the necessary components like <code>RTPublisher</code>, <code>Coordinator</code>, and <code>RTState</code>.</li> <li>Agent Execution: Users run the agent by calling <code>call</code> method inside the context with the appropriate parameters given the setup of their agent.</li> </ol> <pre><code>graph TD\n    A[Agent Creation] --&gt; |User Creates Agent| B[Session Initialization]\n    B --&gt; |Sets up Components and Context| C[Agent Execution]\n    C --&gt; |Runs Agent and Returns Results| D[Results]\n    B --&gt; |Saves the temporal State| E[RTState]\n    D --&gt; |Saves output| E[RTState]\n</code></pre>"},{"location":"system_internals/overview/#core-architectural-principles","title":"Core Architectural Principles","text":"<ul> <li>Immutable State Trees: All execution history preserved</li> <li>Event-Driven Communication: Pub/sub messaging between components  </li> <li>Graceful Error Recovery: Continue execution when possible and desirable</li> <li>Session-Scoped Isolation: Clean boundaries between workflow runs</li> </ul>"},{"location":"system_internals/overview/#developer-journey","title":"Developer Journey","text":"<ol> <li>Start Here: Core Concepts - Understand the mental model behind Railtracks</li> <li>Core Components<ul> <li>Session</li> <li>PubSub System</li> <li>Coordinator</li> </ul> </li> </ol>"},{"location":"system_internals/pubsub/","title":"PubSub (Publisher-Subscriber) Documentation","text":""},{"location":"system_internals/pubsub/#overview","title":"Overview","text":"<p>The PubSub (Publisher-Subscriber) system is a messaging pattern that allows different parts of the Railtracks system to communicate asynchronously. Think of it like a radio station: publishers broadcast messages (like radio shows), and subscribers listen for messages they're interested in (like tuning into specific stations).</p>"},{"location":"system_internals/pubsub/#what-is-pubsub","title":"What is PubSub?","text":"<p>PubSub is a communication pattern where:</p> <ul> <li>Publishers send messages without knowing who will receive them</li> <li>Subscribers listen for messages they care about without knowing who sent them</li> <li>A message broker (in our case, the Publisher class) handles the routing</li> </ul> <p>This creates loose coupling between components - they don't need to know about each other directly.</p>"},{"location":"system_internals/pubsub/#what-are-callbacks","title":"What are Callbacks?","text":"<p>A callback is a function that gets called automatically when something happens. In the context of PubSub:</p> <ul> <li>You give the system a function (the callback)</li> <li>The system calls your function when a relevant message arrives</li> <li>Your function receives the message as a parameter</li> </ul> <p>Example: <pre><code>def my_callback(message):\n    print(f\"Received: {message}\")\n\n# This callback will be called whenever a message is published\n</code></pre></p>"},{"location":"system_internals/pubsub/#core-components","title":"Core Components","text":""},{"location":"system_internals/pubsub/#1-messages-messagespy","title":"1. Messages (<code>messages.py</code>)","text":"<p>Messages are the data structures that flow through the PubSub system. All messages inherit from <code>RequestCompletionMessage</code>:</p>"},{"location":"system_internals/pubsub/#base-message-class","title":"Base Message Class","text":"<ul> <li><code>RequestCompletionMessage</code>: The foundation for all messages in the system</li> <li>Has a <code>log_message()</code> method for debugging</li> </ul>"},{"location":"system_internals/pubsub/#request-lifecycle-messages","title":"Request Lifecycle Messages","text":"<ul> <li><code>RequestCreation</code>: Sent when a new request is created<ul> <li>Contains: current node ID, new request ID, execution mode, node type, and arguments</li> </ul> </li> <li><code>RequestSuccess</code>: Sent when a request completes successfully<ul> <li>Contains: request ID, node state, and the result</li> </ul> </li> <li><code>RequestFailure</code>: Sent when a request fails during execution<ul> <li>Contains: request ID, node state, and the error</li> </ul> </li> <li><code>RequestCreationFailure</code>: Sent when request creation itself fails<ul> <li>Contains: request ID and the error</li> </ul> </li> </ul>"},{"location":"system_internals/pubsub/#special-messages","title":"Special Messages","text":"<ul> <li><code>FatalFailure</code>: Indicates an irrecoverable system failure</li> <li><code>Streaming</code>: Used for streaming data during execution<ul> <li>Contains: the streamed object and node ID</li> </ul> </li> </ul>"},{"location":"system_internals/pubsub/#2-publisher-publisherpy","title":"2. Publisher (<code>publisher.py</code>)","text":"<p>The Publisher is the central message broker that manages message distribution.</p>"},{"location":"system_internals/pubsub/#basic-publisher-features","title":"Basic Publisher Features","text":"<ul> <li>Asynchronous: Uses <code>asyncio</code> for non-blocking operations</li> <li>Ordered: Messages are processed in the order they arrive</li> <li>Thread-safe: Multiple components can publish simultaneously</li> </ul>"},{"location":"system_internals/pubsub/#key-methods","title":"Key Methods","text":"<p>Starting and Stopping: <pre><code>publisher = Publisher()\nawait publisher.start()  # Start the message processing loop\nawait publisher.shutdown()  # Stop and cleanup\n</code></pre></p> <p>Publishing Messages: <pre><code>await publisher.publish(message)  # Send a message to all subscribers\n</code></pre></p> <p>Subscribing: <pre><code>def my_callback(message):\n    print(f\"Got message: {message}\")\n\nsubscriber_id = publisher.subscribe(my_callback, name=\"my_subscriber\")\n</code></pre></p> <p>Unsubscribing: <pre><code>publisher.unsubscribe(subscriber_id)  # Remove a specific broadcast_callback\n</code></pre></p> <p>Workflow Example: <pre><code>import asyncio\nfrom railtracks.pubsub.publisher import Publisher\n\ndef callback(message: str):\n    \"\"\"\n    A simple callback function that processes incoming messages.\n    \"\"\"\n    print(f\"Received message: {message}\")\n\npublisher = Publisher()\nawait publisher.start()  # Start the publisher\nsubscriber_id = publisher.subscribe(callback, name=\"example_subscriber\")\nawait publisher.publish(\"Hello, World!\")  # Publish a message\n\nawait asyncio.sleep(1)  # Wait for the message to be processed\nawait publisher.shutdown()  # Stop the publisher\n</code></pre></p>"},{"location":"system_internals/pubsub/#advanced-features","title":"Advanced Features","text":"<p>Listeners: Listeners wait for a specific message that matches criteria: <pre><code># Wait for the first message that matches the filter\nresult = await publisher.listener(\n    message_filter=lambda msg: isinstance(msg, RequestSuccess),\n    result_mapping=lambda msg: msg.result,\n    listener_name=\"success_listener\"\n)\n</code></pre></p> <p>Context Manager Support: <pre><code>async with Publisher() as pub:\n    await pub.publish(message)\n# Publisher automatically shuts down when exiting the context\n</code></pre></p>"},{"location":"system_internals/pubsub/#3-subscriber-subscriberpy","title":"3. Subscriber (<code>subscriber.py</code>)","text":"<p>Contains utilities for creating specialized subscribers.</p>"},{"location":"system_internals/pubsub/#stream-subscriber","title":"Stream Subscriber","text":"<p>Converts streaming callbacks into proper message subscribers: <pre><code>def handle_stream_data(data):\n    print(f\"Streaming: {data}\")\n\nsubscriber = stream_subscriber(handle_stream_data)\npublisher.subscribe(subscriber)\n</code></pre></p>"},{"location":"system_internals/pubsub/#4-utilities-utilspy","title":"4. Utilities (<code>utils.py</code>)","text":"<p>Helper functions for working with messages.</p>"},{"location":"system_internals/pubsub/#output-mapping","title":"Output Mapping","text":"<p>Converts message results into their final outputs or raises errors: <pre><code>try:\n    result = output_mapping(message)\n    print(f\"Success: {result}\")\nexcept Exception as error:\n    print(f\"Failed: {error}\")\n</code></pre></p>"},{"location":"system_internals/pubsub/#railtracks-publisher-rtpublisher","title":"Railtracks Publisher (RTPublisher)","text":"<p><code>RTPublisher</code> is a specialized publisher for the Railtracks system that:</p> <ul> <li>Automatically logs all messages for debugging</li> <li>Handles Railtracks specific message types</li> <li>Provides built-in error logging with stack traces</li> </ul> <pre><code>publisher = RTPublisher()\nawait publisher.start()\n</code></pre>"},{"location":"system_internals/pubsub/#common-usage-patterns","title":"Common Usage Patterns","text":""},{"location":"system_internals/pubsub/#1-basic-message-publishing","title":"1. Basic Message Publishing","text":"<pre><code># Create and start publisher\nasync with RTPublisher() as publisher:\n    # Create a message\n    message = RequestSuccess(\n        request_id=\"123\",\n        node_state=some_node_state,\n        result=\"Hello World\"\n    )\n\n    # Publish it\n    await publisher.publish(message)\n</code></pre>"},{"location":"system_internals/pubsub/#2-subscribing-to-specific-message-types","title":"2. Subscribing to Specific Message Types","text":"<pre><code>def handle_success(message):\n    if isinstance(message, RequestSuccess):\n        print(f\"Request {message.request_id} succeeded with: {message.result}\")\n\ndef handle_failure(message):\n    if isinstance(message, RequestFailure):\n        print(f\"Request {message.request_id} failed: {message.error}\")\n\npublisher.subscribe(handle_success, \"success_handler\")\npublisher.subscribe(handle_failure, \"failure_handler\")\n</code></pre>"},{"location":"system_internals/pubsub/#3-waiting-for-specific-results","title":"3. Waiting for Specific Results","text":"<pre><code># Wait for a specific request to complete\nresult = await publisher.listener(\n    message_filter=lambda msg: (\n        isinstance(msg, (RequestSuccess, RequestFailure)) and \n        msg.request_id == \"my_request_123\"\n    ),\n    result_mapping=output_mapping,  # Convert to final result or raise error\n    listener_name=\"request_waiter\"\n)\n</code></pre>"},{"location":"system_internals/pubsub/#4-streaming-data","title":"4. Streaming Data","text":"<pre><code>def process_stream(data):\n    # Process each piece of streaming data\n    print(f\"Processing: {data}\")\n\n# Subscribe to streaming messages\nstream_handler = stream_subscriber(process_stream)\npublisher.subscribe(stream_handler, \"stream_processor\")\n</code></pre>"},{"location":"system_internals/pubsub/#error-handling","title":"Error Handling","text":"<p>The PubSub system handles errors gracefully:</p> <ol> <li>Subscriber Errors: If a subscriber's callback fails, it's logged but doesn't affect other subscribers</li> <li>Publisher Errors: Fatal errors are communicated through <code>FatalFailure</code> messages</li> <li>Request Errors: Both creation and execution failures have specific message types</li> </ol>"},{"location":"system_internals/pubsub/#best-practices","title":"Best Practices","text":"<ol> <li>Always use descriptive names for subscribers to aid debugging</li> <li>Handle errors in your callbacks - don't let them crash the system</li> <li>Use context managers when possible for automatic cleanup</li> <li>Filter messages efficiently - check message types early in your callbacks</li> <li>Unsubscribe when done to prevent memory leaks</li> <li>Use listeners for one-time responses instead of persistent subscribers</li> </ol>"},{"location":"system_internals/pubsub/#thread-safety-and-async-considerations","title":"Thread Safety and Async Considerations","text":"<ul> <li>The Publisher uses <code>asyncio.Queue</code> for thread-safe message handling</li> <li>All operations are asynchronous - always use <code>await</code></li> <li>Messages are processed sequentially to maintain order</li> <li>The system gracefully handles shutdown during active operations</li> </ul>"},{"location":"system_internals/pubsub/#debugging-tips","title":"Debugging Tips","text":"<ol> <li>Enable debug logging to see all message flows</li> <li>Use meaningful subscriber names for easier log interpretation</li> <li>Check the <code>log_message()</code> output for standardized message descriptions</li> <li>Monitor the publisher's running state with <code>is_running()</code></li> <li>Use the built-in logging subscriber in RCPublisher for automatic message logging</li> </ol> <p>This PubSub system provides a robust foundation for decoupled communication throughout the Request Completion framework, allowing components to interact without tight coupling while maintaining reliability and observability.</p>"},{"location":"system_internals/session/","title":"Session","text":""},{"location":"system_internals/session/#overview","title":"Overview","text":"<p>The <code>Session</code> is the primary entry point and orchestrator for the entire Railtracks system. It serves as the main execution context that initializes, configures, and manages all core components required for running node-based workflows. The <code>Session</code> acts as a facade that brings together the <code>Coordinator</code>, <code>RTState</code>, <code>Pub/Sub</code> messaging, logging, and global context management into a unified, easy-to-use interface.</p>"},{"location":"system_internals/session/#overall-flow","title":"Overall Flow","text":"<ol> <li>Session Creation: A new <code>Session</code> is created, initializing all necessary components.</li> <li>Context Management: The <code>Session</code> manages the execution context, including variable scoping and state management.</li> <li>Workflow Execution: Users define and execute workflows within the <code>Session</code>, leveraging its orchestration capabilities.</li> <li>Result Handling: The <code>Session</code> collects and processes results from the workflow execution, providing a unified interface for accessing outcomes.</li> <li>Cleanup: Upon completion, the <code>Session</code> handles cleanup tasks, ensuring all resources are released properly.</li> </ol> <pre><code>graph TD\n    A[Session Initialization] --&gt; |Creates and Starts| B[RTPublisher]\n    A[Session Initialization] --&gt; |Creates| C[ExecutorConfig]\n    A[Session Initialization] --&gt; |Creates and Starts| D[Coordinator]\n    A[Session Initialization] --&gt; |Creates| E[RTState]\n    D[Coordinator] --&gt; |Subscribes to| B[RTPublisher]\n</code></pre>"},{"location":"system_internals/session/#publishersubscriber-integration","title":"Publisher/Subscriber Integration","text":"<p>The <code>Session</code> establishes a pub/sub messaging system where: - <code>Coordinator</code> subscribes to handle task completion messages - <code>RTState</code> subscribes to manage state updates - Optional user subscribers can be attached for streaming</p>"},{"location":"system_internals/session/#global-context-management","title":"Global Context Management","text":"<p>The <code>Session</code> manages global context through: - Registration of session ID, publisher, and configuration - Context variable scoping for nested executions - Cleanup to prevent context leakage between runs</p>"},{"location":"system_internals/session/#key-components","title":"Key Components","text":""},{"location":"system_internals/session/#session_1","title":"<code>Session</code>","text":"<p>The main orchestrator class responsible for system initialization, lifecycle management, and providing both synchronous and asynchronous execution interfaces. It encapsulates all system components and manages their interactions through a well-defined lifecycle.</p> <pre><code>classDiagram\n    class Session {\n        +executor_config: ExecutorConfig\n        +publisher: RTPublisher[RequestCompletionMessage]\n        +coordinator: Coordinator\n        +rc_state: RTState\n        +__init__(executor_config: ExecutorConfig, context: Dict[str, Any])\n        +__enter__() Session\n        +__exit__(exc_type, exc_val, exc_tb)\n        +run_sync(start_node, *args, **kwargs) ExecutionInfo\n        +run(start_node, *args, **kwargs) ExecutionInfo\n        +setup_subscriber()\n        +info() ExecutionInfo\n        +shutdown()\n    }\n</code></pre>"},{"location":"system_internals/session/#executorconfig","title":"<code>ExecutorConfig</code>","text":"<p>A configuration object that defines how the <code>Session</code> operates, including timeout settings, error handling behavior, logging configuration, and execution options. It provides comprehensive customization of the execution environment.</p> <pre><code>classDiagram\n    class ExecutorConfig {\n        +timeout: float\n        +end_on_error: bool\n        +logging_setting: str\n        +log_file: str | None\n        +subscriber: Callable | None\n        +run_identifier: str\n        +prompt_injection: bool\n        +save_state: bool\n        +__init__(timeout, end_on_error, logging_setting, log_file, subscriber, run_identifier, prompt_injection, save_state)\n    }\n</code></pre>"},{"location":"system_internals/session/#rtstate","title":"<code>RTState</code>","text":"<p>The central state management component that tracks execution progress, manages node lifecycles, handles exceptions, and coordinates between different system components. It maintains the complete execution history and current system state.</p> <pre><code>classDiagram\n    class RTState {\n        +execution_info: ExecutionInfo\n        +executor_config: ExecutorConfig\n        +coordinator: Coordinator\n        +publisher: RTPublisher\n        +__init__(execution_info, executor_config, coordinator, publisher)\n        +handle(item: RequestCompletionMessage)\n        +call_nodes(parent_node_id, request_id, node, args, kwargs)\n        +cancel(node_id: str)\n        +shutdown()\n        +info() ExecutionInfo\n        +get_info(ids: List[str]) ExecutionInfo\n    }\n</code></pre>"},{"location":"system_internals/session/#executioninfo","title":"<code>ExecutionInfo</code>","text":"<p>A comprehensive data structure that captures the complete state of a run, including all nodes, requests, execution paths, timing information, and results. It provides both runtime access and post-execution analysis capabilities.</p> <pre><code>classDiagram\n    class ExecutionInfo {\n        +request_heap: RequestForest\n        +node_heap: NodeForest\n        +stamper: StampManager\n        +__init__(request_heap, node_heap, stamper)\n        +create_new() ExecutionInfo\n        +to_graph() Tuple[List[Vertex], List[Edge]]\n        +graph_serialization() str\n        +get_info(ids: List[str]) ExecutionInfo\n        +answer() Any\n        +all_stamps() List[Stamp]\n    }\n</code></pre>"},{"location":"system_internals/session/#session-lifecycle","title":"Session Lifecycle","text":"<p>The <code>Session</code> follows a well-defined lifecycle that ensures proper initialization, execution, and cleanup:</p>"},{"location":"system_internals/session/#1-initialization-phase","title":"1. Initialization Phase","text":"<p>During initialization, the <code>Session</code>:</p> <ul> <li>Creates or uses provided <code>ExecutorConfig</code></li> <li>Initializes logging system based on configuration</li> <li>Creates <code>RTPublisher</code> for pub/sub messaging</li> <li>Instantiates <code>ExecutionInfo</code> for state tracking</li> <li>Creates <code>Coordinator</code> with execution strategies</li> <li>Initializes <code>RTState</code> to manage execution</li> <li>Registers global context variables</li> <li>Sets up subscriber connections</li> </ul>"},{"location":"system_internals/session/#2-execution-phase","title":"2. Execution Phase","text":"<p>During execution, the <code>Session</code>:</p> <ul> <li>Delegates node execution to the global <code>call</code> function</li> <li>Maintains execution state through <code>RTState</code></li> <li>Coordinates task execution via <code>Coordinator</code></li> <li>Publishes and handles completion messages via <code>RTPublisher</code></li> <li>Tracks all execution details in <code>ExecutionInfo</code></li> </ul>"},{"location":"system_internals/session/#3-cleanup-phase","title":"3. Cleanup Phase","text":"<p>During cleanup, the <code>Session</code>:</p> <ul> <li>Optionally saves execution state to disk (if <code>save_state=True</code>)</li> <li>Shuts down all execution strategies</li> <li>Detaches logging handlers</li> <li>Cleans up global context variables</li> <li>Releases system resources</li> </ul>"},{"location":"system_internals/session/#configuration-and-customization","title":"Configuration and Customization","text":"<p>The <code>Session</code> supports extensive customization through <code>ExecutorConfig</code>:</p> <ul> <li>Timeout Control: Set maximum execution time limits</li> <li>Error Handling: Configure whether to stop on first error or continue</li> <li>Logging: Control log levels and output destinations  </li> <li>State Persistence: Enable/disable saving execution state to disk</li> <li>Streaming: Attach custom subscribers for real-time monitoring</li> <li>Context Injection: Control global context variable behavior</li> </ul>"},{"location":"system_internals/session/#error-handling-and-recovery","title":"Error Handling and Recovery","text":"<p>The <code>Session</code> implements robust error handling:</p> <ul> <li>Graceful Degradation: Continues execution when possible</li> <li>Error Propagation: Properly bubbles up fatal errors</li> <li>State Preservation: Maintains execution state even during failures</li> <li>Cleanup Guarantees: Ensures proper resource cleanup in all scenarios</li> </ul>"},{"location":"tools_mcp/RAG/","title":"Retrieval-Augmented Generation (RAG)","text":"<p>Need your AI agents to access your company's knowledge base, docs, or any private data? RAG is your answer! It enables grounded answering by retrieving relevant snippets from your documents and composing them into LLM prompts.</p>"},{"location":"tools_mcp/RAG/#two-ways-to-get-started","title":"Two Ways to Get Started","text":"<p>We offer two approaches to integrate RAG into your application:</p> <p>Choose Your Adventure</p> <ol> <li>Quick &amp; Easy: Use the prebuilt <code>rag_node</code> for instant setup </li> <li>Full Control: Build a custom RAG node using the <code>RAG</code> class for maximum flexibility</li> </ol>"},{"location":"tools_mcp/RAG/#option-1-prebuilt-rag-node-recommended","title":"Option 1: Prebuilt RAG Node (Recommended)","text":"<p>Perfect for getting started quickly! Wrap your RAG index into a callable node so other nodes and LLMs can retrieve relevant context and compose prompts.</p> <pre><code>import railtracks as rt\nfrom railtracks.prebuilt import rag_node\nfrom railtracks.llm import OpenAILLM\n\n# 1) Build the retrieval node\nretriever = rag_node([\n    \"Our company policy requires all employees to work from home on Fridays\",\n    \"Data security guidelines mandate encryption of all sensitive customer information\",\n    \"Employee handbook states vacation requests need 2 weeks advance notice\"\n])\n\n# 2) Create Agent\nagent = rt.agent_node(\n    llm=OpenAILLM(\"gpt-4o\"),\n)\n\n# 3) Run the agent.\n@rt.session()\nasync def main():\n    question = \"What is the work from home policy?\"\n    search_result = await rt.call(retriever, question, top_k=2)\n    context = \"\\n\\n\".join(search_result.to_list_of_texts())\n\n    response = await rt.call(\n        agent,\n        user_input=(\n            \"Based on the following context, please answer the question.\\n\"\n            \"Context:\\n\"\n            f\"{context}\\n\"\n            \"Question:\\n\"\n            f\"{question}\\n\"\n            \"Answer based only on the context provided.\"\n            \"If the answer is not in the context, say \\\"I don't know\\\".\"\n        )\n    )\n</code></pre> <p>File Loading Made Easy</p> <p>The <code>rag_node</code> function accepts raw text content only. For file loading, read the files first:</p> <pre><code>from railtracks.rag.utils import read_file\n\n# Read file contents manually\ntry:\n    doc1_content = read_file(\"./docs/faq.txt\")\n    doc2_content = read_file(\"./docs/policies.txt\")\nexcept FileNotFoundError:\n    doc1_content = \"FAQ file not found. Please ensure docs/faq.txt exists.\"\n    doc2_content = \"Policies file not found. Please ensure docs/policies.txt exists.\"\n\n# Build retriever with file contents\nretriever = rag_node([\n    doc1_content,\n    doc2_content\n])\n</code></pre>"},{"location":"tools_mcp/RAG/#option-2-custom-rag-node-advanced","title":"Option 2: Custom RAG Node (Advanced)","text":"<p>For maximum control and customization, build your own RAG node.</p> <pre><code>import railtracks as rt\nfrom railtracks.rag.rag_core import RAG, RAGConfig, SearchResult\n\nrag_core = RAG(\n        docs=[\"&lt;Your text here&gt;\", \"...\"],\n        config=RAGConfig(\n            embedding={\"model\": \"text-embedding-3-small\"},\n            store={},\n            chunking={\n                \"chunk_size\": 1000,\n                \"chunk_overlap\": 200,\n                \"model\": \"gpt-4o\",\n            },\n        )\n    )\nrag_core.embed_all()\n\n@rt.function_node\nasync def custom_rag_node(query: str) -&gt; SearchResult:\n    \"\"\"A custom RAG function node that retrieves documents based on a query.\"\"\"\n    return rag_core.search(query, top_k=5)\n</code></pre> <p>Pro Tips</p> <ul> <li>The callable node accepts <code>query</code> and optional <code>top_k</code> to control number of retrieved chunks. </li> <li><code>SearchResult</code> can be converted to plain text using <code>.to_list_of_texts()</code> </li> <li>You can inspect the object for similarity scores and metadata</li> </ul>"},{"location":"tools_mcp/RAG/#chunking-strategy","title":"Chunking Strategy","text":"<p>Best Practices:</p> <ul> <li><code>chunk_size</code>: Number of tokens per chunk (approximate, based on <code>token_count_model</code>)</li> <li><code>chunk_overlap</code>: Number of tokens to overlap between adjacent chunks</li> <li>Sweet spot: Start with 600-1200 tokens with 10-20% overlap</li> </ul>"},{"location":"tools_mcp/RAG/#embeddings","title":"Embeddings","text":"<p>Model Selection:</p> <ul> <li><code>\"text-embedding-3-small\"</code> is a good default for many use cases (balance of quality and cost)</li> <li>Upgrade to stronger models for nuanced or specialized domains</li> <li>Configure via <code>embed_config</code></li> </ul>"},{"location":"tools_mcp/RAG/#vector-store-options","title":"Vector Store Options","text":"<p>Storage Recommendations:</p> <ul> <li>In-memory by default (perfect for development and tests)</li> <li>For larger corpora: Consider FAISS/Qdrant or other backends supported by <code>create_store</code></li> <li>Production: Use persistent storage for better performance</li> </ul>"},{"location":"tools_mcp/RAG/#top-k-retrieval","title":"Top-k Retrieval","text":"<p>Finding the Right Balance:</p> <ul> <li>Typical values: 3\u20135 chunks</li> <li>Increase if your content is highly fragmented or diverse</li> <li>Monitor token usage - larger chunk sizes and higher <code>top_k</code> values increase memory and token consumption</li> </ul>"},{"location":"tools_mcp/RAG/#related-documentation","title":"Related Documentation","text":""},{"location":"tools_mcp/RAG/#features-concepts","title":"Features &amp; Concepts","text":"<ul> <li>Tool Usage Patterns</li> <li>Advanced Context Management</li> </ul>"},{"location":"tools_mcp/RAG/#external-libraries","title":"External Libraries","text":"<p>Powered By:</p> <ul> <li>LiteLLM - Embeddings and chat transport</li> </ul> <p>Optional Vector Store Backends:</p> <ul> <li>FAISS - Fast similarity search</li> <li>Qdrant - Vector database</li> </ul>"},{"location":"tools_mcp/tools_mcp/","title":"Railtracks Tools","text":"<p>Extend Your Agents' Capabilities</p> <p>Tools transform LLMs from chatbots into true agents that can take action in the world.</p> <p>Railtracks provides a comprehensive suite of tools that extend the capabilities of your agents, allowing them to interact with external systems and services. These tools are the \"hands and eyes\" of your agents, enabling them to perform actions beyond just generating text.</p>"},{"location":"tools_mcp/tools_mcp/#ways-to-get-tools","title":"Ways to Get Tools","text":"<p>Railtracks offers multiple ways to access and create tools:</p> <ol> <li>Built-in Tools - Use our pre-built tools for common tasks</li> <li>MCP Tools - Connect to Model Context Protocol servers for additional capabilities</li> <li>Create Your Own - Build custom tools for your specific needs</li> </ol> <p>For a conceptual overview of tools in Railtracks, see the Tools Guide.</p>"},{"location":"tools_mcp/tools_mcp/#available-tools","title":"Available Tools","text":""},{"location":"tools_mcp/tools_mcp/#built-in-tools","title":"Built-in Tools","text":"Tool Description Use Case Python Execution Write and run Python code Data analysis, calculations, algorithmic tasks Local Shell Execute commands in your local environment File operations, system management, running scripts"},{"location":"tools_mcp/tools_mcp/#mcp-based-tools","title":"MCP-Based Tools","text":"Tool Description Use Case GitHub Interact with GitHub repositories Code management, issue tracking, PR reviews Notion Create and manage Notion pages Knowledge management, documentation, project planning Slack Send and receive Slack messages Team communication, notifications, updates Web Search Search the web and retrieve information Research, fact-checking, data gathering"},{"location":"tools_mcp/tools_mcp/#getting-started","title":"Getting Started","text":"<p>To use tools in your Railtracks agents, you'll typically follow these steps:</p> <ol> <li>Import the tools you want to use</li> <li>Create an agent that can access these tools</li> <li>Run the agent with your desired input</li> </ol> <pre><code>import railtracks as rt\nfrom railtracks.nodes.library import connect_mcp\nfrom railtracks.rt_mcp import MCPHttpParams\n\n# Get tools from an MCP server\nserver = connect_mcp(MCPHttpParams(url=\"https://remote.mcpservers.org/fetch/mcp\"))\ntools = server.tools\n\n# Create an agent with access to these tools\nAgent = rt.agent_node(\n    tool_nodes=tools,\n    name=\"Research Agent\",\n    system_message=\"Use the tools to find information.\",\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n)\n\n# Run the agent\nwith rt.Session():\n    result = await rt.call(\n        Agent,\n        \"Find information about Railtracks\"\n    )\n</code></pre>"},{"location":"tools_mcp/tools_mcp/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Model Context Protocol (MCP) for accessing even more tools</li> <li>Check out the tool-specific guides for detailed usage instructions</li> </ul>"},{"location":"tools_mcp/guides/github/","title":"Using GitHub MCP Server with RequestCompletion","text":"<p>To use the GitHub MCP server with RT, use the <code>from_mcp_server</code> utility to load tools directly from the MCP server. A valid GitHub Personal Access Token (PAT) is required, which in this example is provided via an environment variable.</p> <pre><code>import os\nfrom railtracks.rt_mcp import MCPHttpParams\nfrom railtracks import connect_mcp\n\nserver = connect_mcp(\n    MCPHttpParams(\n        url=\"https://api.githubcopilot.com/mcp/\",\n        headers={\n            \"Authorization\": f\"Bearer {os.getenv('GITHUB_PAT_TOKEN')}\",\n        },\n    )\n)\ntools = server.tools\n</code></pre> <p>At this point, the tools can be used the same as any other RT tool. See the following code as a simple example.</p> <pre><code>import railtracks as rt\nimport asyncio\n\nagent = rt.agent_node(\n    # tool_nodes={*tools},    # Uncomment this line to use the tools\n    system_message=\"\"\"You are a GitHub Copilot agent that can interact with GitHub repositories.\"\"\",\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n)\n\nuser_prompt = \"\"\"Tell me about the RailtownAI/rc repository on GitHub.\"\"\"\n\nasync def call_node():\n    with rt.Session():\n        result = await rt.call(agent, user_prompt)\n\n    print(result.content)\n\n# asyncio.run(call_node())\n</code></pre>"},{"location":"tools_mcp/guides/notion/","title":"Using Notion MCP Server with RT","text":"<p>To use Notion tools with RT, use the <code>from_mcp_server</code> utility to load tools directly from the MCP server. For this example, ensure you have a valid Notion API token set in the environment variables. To get the token, in Notion, go to Settings &gt; Connections &gt; Develop or manage integrations, and create a new integration, or get the token from an existing one.</p> <pre><code>import json\nimport os\nfrom railtracks import MCPStdioParams, connect_mcp\n\nMCP_COMMAND = \"npx\"\nMCP_ARGS = [\"-y\", \"@notionhq/notion-mcp-server\"]\nNOTION_VERSION = \"2022-06-28\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {os.environ['NOTION_API_TOKEN']}\",\n    \"Notion-Version\": NOTION_VERSION\n}\n\nnotion_env = {\n    \"OPENAPI_MCP_HEADERS\": json.dumps(headers)\n}\n\nserver = connect_mcp(\n    MCPStdioParams(\n        command=MCP_COMMAND,\n        args=MCP_ARGS,\n        env=notion_env,\n    )\n)\ntools = server.tools\n</code></pre> <p>At this point, the tools can be used the same as any other RT tool. See the following code as a simple example.</p> <pre><code>import railtracks as rt\nimport asyncio\nagent = rt.agent_node(\n    # tool_nodes={*tools},    # Uncomment this line to use the tools\n    system_message=\"\"\"You are a master Notion page designer. You love creating beautiful\n     and well-structured Notion pages and make sure that everything is correctly formatted.\"\"\",\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n)\n\nuser_prompt = \"\"\"Create a new page in Notion called 'Jokes' under the parent page \"Welcome to Notion!\" with a small joke at the top of the page.\"\"\"\nmessage_history = rt.llm.MessageHistory()\nmessage_history.append(rt.llm.UserMessage(user_prompt))\n\nasync def call_notion():\n    with rt.Session():\n        result = await rt.call(agent, message_history)\n\n    print(result.content)\n\n# asyncio.run(call_node())\n</code></pre>"},{"location":"tools_mcp/guides/python_sandbox/","title":"Running python code with Railtracks","text":"<p>This is a simple guide to running Python code with Railtracks, using a Docker container as a sandboxed environment. Before running the code, make sure you have Docker installed and running on your machine.</p> <pre><code>import subprocess\nimport railtracks as rt\n\ndef create_sandbox_container():\n    subprocess.run([\n        \"docker\", \"run\", \"-dit\", \"--rm\",\n        \"--name\", \"sandbox_chatbot_session\",\n        \"--memory\", \"512m\", \"--cpus\", \"0.5\",\n        \"python:3.12-slim\", \"python3\"\n    ])\n\n\ndef kill_sandbox():\n    subprocess.run([\"docker\", \"rm\", \"-f\", \"sandbox_chatbot_session\"])\n\n\ndef execute_code(code: str) -&gt; str:\n    exec_result = subprocess.run([\n        \"docker\", \"exec\", \"sandbox_chatbot_session\",\n        \"python3\", \"-c\", code\n    ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    return exec_result.stdout.decode() + exec_result.stderr.decode()\n\n\nCodeAgent = rt.agent_node(\n    tool_nodes={execute_code},\n    system_message=\"\"\"You are a master python programmer. To execute code, you have access to a sandboxed Python environment.\n    You can execute code in it using run_in_sandbox.\n    You can only see the output of the code if it is printed to stdout or stderr, so anything you want to see must be printed.\n    You can install packages with code like 'import os; os.system('pip install numpy')'\"\"\",\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n)\n</code></pre> <p>When running the agent, use the <code>create_sandbox_container</code> function to start the Docker container before running the agent, and the <code>kill_sandbox</code> function to stop and remove the container after you're done. The following example shows how to use the agent to execute Python code in the sandboxed environment:</p> <pre><code>user_prompt = \"\"\"Create a 3x3 array of random numbers using numpy, and print the array and its mean\"\"\"\n\nasync def call_code_agent():\n    with rt.Session(logging_setting=\"VERBOSE\"):\n        create_sandbox_container()\n        try:\n            result = await rt.call(CodeAgent, user_prompt)\n        finally:\n            kill_sandbox()\n\n    print(result.content)\n\n# asyncio.run(call_node())\n</code></pre>"},{"location":"tools_mcp/guides/shell_bash/","title":"Using Shell as a tool with RequestCompletion","text":"<p>To allow for usage of shell as a tool, we can create a simple tool using <code>from_fuction</code>. The function could be modified to suit your needs, such as adding error handling or specific command restrictions. Below is a basic example of how to create a shell command execution tool using <code>subprocess</code> in Python.</p> <pre><code>import subprocess\n\ndef run_shell(command: str) -&gt; str:\n    \"\"\"Run a bash command and return its output or error.\"\"\"\n    try:\n        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n        if result.returncode == 0:\n            return result.stdout.strip()\n        else:\n            return f\"Error: {result.stderr.strip()}\"\n    except Exception as e:\n        return f\"Exception: {str(e)}\"\n\n\nbash_tool = rt.function_node(run_shell)\n</code></pre> <p>At this point, the tool can be used the same as any other RT tool. See the following code as a simple example.</p> <pre><code>import platform\nimport railtracks as rt\nimport asyncio\n\nBashAgent = rt.agent_node(\n    tool_nodes={bash_tool},\n    system_message=f\"You are a useful helper that can run local shell commands. \"\n                   f\"You are on a {platform.system()} machine. Use appropriate shell commands to answer the user's questions.\",\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n)\n\nuser_prompt = \"\"\"What directories are in the current directory?\"\"\"\nmessage_history = rt.llm.MessageHistory()\nmessage_history.append(rt.llm.UserMessage(user_prompt))\n\nasync def call_bash_agent():\n    with rt.Session(logging_setting=\"VERBOSE\"):\n        result = await rt.call(BashAgent, message_history)\n\n    print(result.content)\n\n# asyncio.run(call_node())\n</code></pre>"},{"location":"tools_mcp/guides/slack/","title":"Adding Slack integration with RT","text":"<p>To allow for Slack integration with RT, you need to first create a Slack app and at it to your Slack workspace - https://api.slack.com/apps.  Next, get the Slack team ID (It starts with T, such as \"T12345678\"). You can also optionally specify the Slack channel IDs you want to restrict interaction to (ex. \"C87654321, C87654322\"). Finally, use the <code>from_mcp_server</code> utility to load tools directly from the MCP server.</p> <pre><code>import os\nfrom railtracks import connect_mcp, MCPStdioParams\n\nMCP_COMMAND = \"npx\"\nMCP_ARGS = [\"-y\", \"@modelcontextprotocol/server-slack\"]\n\nslack_env = {\n    \"SLACK_BOT_TOKEN\": os.environ['SLACK_BOT_TOKEN'],\n    \"SLACK_TEAM_ID\": os.environ['SLACK_TEAM_ID'],\n    \"SLACK_CHANNEL_IDS\": os.environ['SLACK_CHANNEL_IDS'],\n}\n\nserver = connect_mcp(\n    MCPStdioParams(\n        command=MCP_COMMAND,\n        args=MCP_ARGS,\n        env=slack_env,\n    )\n)\ntools = server.tools\n</code></pre> <p>At this point, the tools can be used the same as any other RT tool. See the following code as a simple example.</p> <pre><code>import railtracks as rt\nimport asyncio\n\nSlackAgent = rt.agent_node(\n    # tool_nodes={*tools},    # Uncomment this line to use the tools\n    system_message=\"\"\"You are a Slack agent that can interact with Slack channels.\"\"\",\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n)\n\nuser_prompt = \"\"\"Send a message to general saying \"Hello!\".\"\"\"\n\nasync def call_slack_agent():\n    with rt.Session(logging_setting=\"VERBOSE\"):\n        result = await rt.call(SlackAgent, user_prompt)\n\n    print(result.content)\n\n# asyncio.run(call_node())\n</code></pre>"},{"location":"tools_mcp/guides/websearch_integration/","title":"Web Search Integration with RequestCompletion","text":"<p>This guide demonstrates how to create a web search integration for RequestCompletion (RC) using both MCP (Model Context Protocol) servers and custom Google API tools. This setup allows your AI agent to search the web and fetch content from URLs.</p>"},{"location":"tools_mcp/guides/websearch_integration/#prerequisites","title":"Prerequisites","text":"<p>Before implementing this integration, you'll need:</p> <ol> <li>Google Custom Search API credentials:</li> <li>Visit the Google Cloud Console</li> <li>Enable the Custom Search API</li> <li> <p>Create API credentials and a Custom Search Engine ID</p> </li> <li> <p>Environment variables:  <pre><code>GOOGLE_SEARCH_API_KEY=your_api_key_here\nGOOGLE_SEARCH_ENGINE_ID=your_search_engine_id_here\n</code></pre></p> </li> <li> <p>Required packages:  <pre><code>pip install railtracks python-dotenv aiohttp\n</code></pre></p> </li> </ol>"},{"location":"tools_mcp/guides/websearch_integration/#implementation","title":"Implementation","text":""},{"location":"tools_mcp/guides/websearch_integration/#step-1-import-dependencies-and-load-environment","title":"Step 1: Import Dependencies and Load Environment","text":"<pre><code>from dotenv import load_dotenv\nimport os\nfrom railtracks import connect_mcp\nimport railtracks as rt\nimport asyncio\nfrom railtracks.rt_mcp import MCPHttpParams\nimport aiohttp\nfrom typing import Dict, Any\n\nload_dotenv()\n</code></pre>"},{"location":"tools_mcp/guides/websearch_integration/#step-2-set-up-mcp-tools-for-url-fetching","title":"Step 2: Set Up MCP Tools for URL Fetching","text":"<p>The MCP server provides tools that can fetch and process content from URLs:</p> <p><pre><code># MCP Tools that can fetch data from URLs\nfetch_mcp_server = connect_mcp(MCPHttpParams(url=\"https://remote.mcpservers.org/fetch/mcp\"))\nfetch_mcp_tools = fetch_mcp_server.tools\n</code></pre> Read more about the <code>from_mcp_server</code> utility TODO: change this link.  This connects to a remote MCP server that provides URL fetching capabilities.</p>"},{"location":"tools_mcp/guides/websearch_integration/#step-3-create-custom-google-search-tool","title":"Step 3: Create Custom Google Search Tool","text":"<pre><code>def _format_results(data: Dict[str, Any]) -&gt; Dict[str, Any]:\n    return data\n\n\n@rt.function_node\nasync def google_search(query: str, num_results: int = 3) -&gt; Dict[str, Any]:\n   \"\"\"\n   Tool for searching using Google Custom Search API\n\n   Args:\n       query (str): The search query\n       num_results (int): The number of results to return (max 5)\n\n   Returns:\n       Dict[str, Any]: Formatted search results\n   \"\"\"\n   params = {\n      'key': os.environ['GOOGLE_SEARCH_API_KEY'],\n      'cx': os.environ['GOOGLE_SEARCH_ENGINE_ID'],\n      'q': query,\n      'num': min(num_results, 5)  # Google API maximum is 5\n   }\n\n   async with aiohttp.ClientSession() as session:\n      try:\n         async with session.get(\"https://www.googleapis.com/customsearch/v1\", params=params) as response:\n            if response.status == 200:\n               data = await response.json()\n               return _format_results(data)\n            else:\n               error_text = await response.text()\n               raise Exception(f\"Google API error {response.status}: {error_text}\")\n      except Exception as e:\n         raise Exception(f\"Search failed: {str(e)}\")\n</code></pre>"},{"location":"tools_mcp/guides/websearch_integration/#step-4-create-and-use-the-search-agent","title":"Step 4: Create and Use the Search Agent","text":"<pre><code># Combine all tools\ntools = fetch_mcp_tools + [google_search]\n\n# Create the agent with search capabilities\nWebSearchAgent = rt.agent_node(\n    # tool_nodes={*tools},    # Uncomment this line to use the tools\n    system_message=\"\"\"You are an information gathering agent that can search the web.\"\"\",\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n)\n\n# Example usage\nuser_prompt = \"\"\"Tell me about Railtown AI.\"\"\"\nasync def call_web_search():\n    result = await rt.call(agent, message_history)\n    print(result)\n\n# asyncio.run(call_node())\n</code></pre>"},{"location":"tools_mcp/guides/websearch_integration/#how-it-works","title":"How It Works","text":"<ol> <li>Google Search Tool: Uses the Google Custom Search API to find relevant web pages based on user queries</li> <li>MCP Fetch Tools: Retrieves and processes content from the URLs found in search results</li> <li>Agent Integration: Combines both tools to create a comprehensive web search and content analysis system</li> </ol>"},{"location":"tools_mcp/mcp/MCP_tools_in_RT/","title":"Using MCP Tools in Railtracks","text":""},{"location":"tools_mcp/mcp/MCP_tools_in_RT/#overview","title":"Overview","text":"<p>Quick Summary</p> <p>Railtracks makes it easy to use any MCP-compatible tool with your agents. Just connect to an MCP server, get the tools, and start using them!</p> <p>Railtracks supports seamless integration with Model Context Protocol (MCP), allowing you to use any MCP-compatible tool as a native Railtracks Tool. This means you can connect your agents to a wide variety of external tools and data sources\u2014without having to implement the tool logic yourself. </p> <p>Railtracks handles the discovery and invocation of MCP tools, so you can focus on building intelligent agents.</p>"},{"location":"tools_mcp/mcp/MCP_tools_in_RT/#prerequisites","title":"Prerequisites","text":"<p>Before You Begin</p> <p>Make sure you have the following set up before using MCP tools:</p> <ul> <li>Railtracks Framework installed (<code>pip install railtracks[core]</code>)</li> <li>MCP package set up - Every MCP tool has different requirements (see specific tool documentation)</li> <li>Authentication credentials - Many MCP tools require API keys or OAuth tokens</li> </ul>"},{"location":"tools_mcp/mcp/MCP_tools_in_RT/#connecting-to-mcp-server-types","title":"Connecting to MCP Server Types","text":"<p>Railtracks supports two types of MCP servers</p> <p>Remote HTTP Servers</p> <p>Use <code>MCPHttpParams</code> for connecting to remote MCP servers:</p> <pre><code>import railtracks as rt\n\n\n# Connect to a remote MCP server\nfetch_server = rt.connect_mcp(\n    rt.MCPHttpParams(\n        url=\"https://remote.mcpservers.org/fetch/mcp\",\n        # Optional: Add authentication headers if needed\n        headers={\"Authorization\": f\"Bearer {'&lt;API_KEY&gt;'}\"},\n    )\n)\n</code></pre> <p>Local Stdio Servers</p> <p>Use <code>MCPStdioParams</code> for running local MCP servers:</p> <pre><code>import railtracks as rt\n\n# Run a local MCP server (Time server example)\ntime_server = rt.connect_mcp(\n    rt.MCPStdioParams(\n        command=\"npx\",\n        args=[\"mcp-server-time\"]  # or other command to run the server\n    )\n)\n</code></pre>"},{"location":"tools_mcp/mcp/MCP_tools_in_RT/#using-mcp-tools-with-railtracks-agents","title":"Using MCP Tools with Railtracks Agents","text":"<p>Once you've connected to an MCP server, you can use the tools with your Railtracks agents:</p> <pre><code>import railtracks as rt\n\n# Run a local MCP server (Time server example)\ntime_server = rt.connect_mcp(\n    rt.MCPStdioParams(\n        command=\"npx\",\n        args=[\"mcp-server-time\"]  # or other command to run the server\n    )\n)\n</code></pre>"},{"location":"tools_mcp/mcp/MCP_tools_in_RT/#common-mcp-server-examples","title":"Common MCP Server Examples","text":"Fetch Server (URL Content Retrieval) <p><pre><code>fetch_server = rt.connect_mcp(\n    rt.MCPHttpParams(url=\"https://remote.mcpservers.org/fetch/mcp\")\n)\n</code></pre> Guide: Websearch Server</p> GitHub Server <pre><code>github_server = rt.connect_mcp(\n    rt.MCPHttpParams(\n        url=\"https://api.githubcopilot.com/mcp/\",\n        headers={\n            \"Authorization\": f\"Bearer {'&lt;GITHUB_PAT_TOKEN&gt;'}\",\n        },\n    )\n)\n</code></pre> <p>Guide: Github Server</p> <p>Warning</p> <p>If you fail to provde the correct PAT you will see the following error:</p> <pre><code>Exception in thread Thread-1 (_thread_main):\n\nTraceback (most recent call last):\n\nFile \"C:\\Users\\rc\\.venv\\lib\\site-packages\\anyio\\streams\\memory.py\", line 111, in receive\n</code></pre> Notion Server <p><pre><code>import json\n\nnotion_server = rt.connect_mcp(\n    rt.MCPStdioParams(\n        command=\"npx\",\n        args=[\"-y\", \"@notionhq/notion-mcp-server\"],\n        env={\n            \"OPENAPI_MCP_HEADERS\": json.dumps({\n                \"Authorization\": f\"Bearer {'&lt;NOTION_API_TOKEN&gt;'}\",\n                \"Notion-Version\": \"2022-06-28\"\n            })\n        },\n    )\n)\n</code></pre> Guide: Notion Server</p>"},{"location":"tools_mcp/mcp/MCP_tools_in_RT/#combining-multiple-mcp-tools","title":"Combining Multiple MCP Tools","text":"<p>You can combine tools from different MCP's into one single agent. </p> <pre><code># You can combine the tools from multiple MCP servers\nall_tools = notion_server.tools + github_server.tools + fetch_server.tools\n\n# Create an agent that can use all tools\nsuper_agent = rt.agent_node(\n    tool_nodes=all_tools,\n    name=\"Multi-Tool Agent\",\n    system_message=\"Use the appropriate tools to complete tasks.\",\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n)\n</code></pre>"},{"location":"tools_mcp/mcp/MCP_tools_in_RT/#tool-specific-guides","title":"Tool-Specific Guides","text":"<p>For detailed setup and usage instructions for specific MCP tools:</p> <ul> <li>GitHub Tool Guide</li> <li>Notion Tool Guide</li> <li>Slack Tool Guide</li> <li>Web Search Integration Guide</li> </ul>"},{"location":"tools_mcp/mcp/MCP_tools_in_RT/#related-topics","title":"Related Topics","text":"<ul> <li>What is MCP?</li> <li>Railtracks to MCP: Exposing RT Tools as MCP Tools</li> </ul>"},{"location":"tools_mcp/mcp/RTtoMCP/","title":"Exposing RT Tools as MCP Tools","text":"<p>Warning</p> <p>This area of RT is under construction. We would love some contributions to support this effort on our Github</p>"},{"location":"tools_mcp/mcp/RTtoMCP/#overview","title":"Overview","text":"<p>You can expose any RT Tool as an MCP-compatible tool, making it accessible to any MCP client or LLM agent that supports the Model Context Protocol (MCP). This allows you to share your custom RT logic with other frameworks, agents, or applications that use MCP.</p> <p>RC provides utilities to convert your Nodes into MCP tools and run a FastMCP server, so your tools are discoverable and callable via standard MCP transports (HTTP, SSE, stdio).</p>"},{"location":"tools_mcp/mcp/RTtoMCP/#prerequisites","title":"Prerequisites","text":"<ul> <li>RC Framework installed (<code>pip install railtracks[core]</code>)</li> </ul>"},{"location":"tools_mcp/mcp/RTtoMCP/#basic-usage","title":"Basic Usage","text":""},{"location":"tools_mcp/mcp/RTtoMCP/#1-convert-rt-nodes-to-mcp-tools","title":"1. Convert RT Nodes to MCP Tools","text":"<p>Use the <code>create_mcp_server</code> utility to expose your RT nodes as MCP tools:</p> <pre><code>import railtracks as rt\n\n# Start by creating your tools\n@rt.function_node\ndef add_nums_plus_ten(num1: int, num2: int):\n    \"\"\"Simple tool example.\"\"\"\n    return num1 + num2 + 10\n\n# Create your MCP server with the function node\nmcp = rt.create_mcp_server([add_nums_plus_ten], server_name=\"My MCP Server\")\n\n# Now run the MCP server\nmcp.run(transport=\"streamable-http\", host=\"127.0.0.1\", port=8000)\n</code></pre> <p>This exposes your RT tool at <code>http://127.0.0.1:8000/mcp</code> for any MCP client.</p>"},{"location":"tools_mcp/mcp/RTtoMCP/#2-accessing-your-mcp-tools","title":"2. Accessing Your MCP Tools","text":"<p>Any MCP-compatible client or LLM agent can now discover and invoke your tool. As an example, you can use Railtracks itself to try your tool:</p> <pre><code>server = rt.connect_mcp(rt.MCPHttpParams(url=\"http://127.0.0.1:8000/mcp\"))\ntools = server.tools\n</code></pre>"},{"location":"tools_mcp/mcp/RTtoMCP/#advanced-topics","title":"Advanced Topics","text":"<ul> <li>Multiple Tools: Pass a list of Node classes to <code>create_mcp_server</code> to expose several tools.</li> <li>Transport Options: Use <code>streamable-http</code>, <code>sse</code>, or <code>stdio</code> as needed.</li> </ul>"},{"location":"tools_mcp/mcp/RTtoMCP/#related-topics","title":"Related Topics","text":"<ul> <li>What is MCP?</li> <li>Using MCP Tools in RT</li> </ul>"},{"location":"tools_mcp/mcp/mcp/","title":"Model Context Protocol (MCP)","text":""},{"location":"tools_mcp/mcp/mcp/#what-is-model-context-protocol-mcp","title":"What is Model Context Protocol (MCP)?","text":"<p>MCP in a Nutshell</p> <p>Model Context Protocol (MCP) is an open standard that enables seamless integration between Large Language Models (LLMs) and external systems, including applications, data sources, and tools.</p> <p>Model Context Protocol (MCP) provides a unified interface, making it easy for LLMs to access context, perform actions, and interact with diverse environments. It standardizes how AI models communicate with external tools and services, similar to how REST APIs standardized web service communication.</p>"},{"location":"tools_mcp/mcp/mcp/#key-benefits-of-mcp","title":"Key Benefits of MCP","text":"<ul> <li>Standardized Integration: Connect to any MCP-compatible tool using the same interface</li> <li>Reduced Development Time: Use pre-built MCP tools instead of creating custom integrations</li> <li>Ecosystem Compatibility: Access a growing ecosystem of MCP-compatible tools and services</li> <li>Simplified Architecture: Uniform approach to tool integration reduces complexity</li> </ul>"},{"location":"tools_mcp/mcp/mcp/#using-mcp-tools-in-railtracks","title":"Using MCP Tools in Railtracks","text":"<p>Railtracks allows you to convert MCP tools into Tools that can be used by Railtracks agents just like any other Tool. We handle the conversion and server setup for you, so you can focus on building your agents without worrying about the underlying complexities of MCP.</p> <p>Quick Example</p> <pre><code>import railtracks as rt\n\n\n# Connect to a remote MCP server\nfetch_server = rt.connect_mcp(\n    rt.MCPHttpParams(\n        url=\"https://remote.mcpservers.org/fetch/mcp\",\n        # Optional: Add authentication headers if needed\n        headers={\"Authorization\": f\"Bearer {'&lt;API_KEY&gt;'}\"},\n    )\n)\n</code></pre> <p>For a complete guide and more examples, see Using MCP Tools in Railtracks.</p>"},{"location":"tools_mcp/mcp/mcp/#railtracks-to-mcp","title":"Railtracks to MCP","text":"<p>Railtracks also provides a way to convert Railtracks Tools into MCP tools using FastMCP, allowing you to use your existing Railtracks tools in any MCP-compatible environment.</p> <p>This enables you to:</p> <ul> <li>Share your custom tools with the broader MCP ecosystem</li> <li>Use your tools in other MCP-compatible frameworks</li> <li>Create a unified toolset across different AI systems</li> </ul> <p>See the Railtracks to MCP page for more details on how to set this up.</p>"},{"location":"tools_mcp/mcp/mcp/#available-mcp-servers","title":"Available MCP Servers","text":"<p>Railtracks supports pre-built integrations with various MCP servers, including:</p> MCP Server Description Setup Guide Websearch Retrieve and process content from URLs Guide GitHub Interact with GitHub repositories Guide Notion Create and manage Notion pages Guide Slack Send and receive Slack messages Guide"},{"location":"tools_mcp/tools/agents_as_tools/","title":"Agents as Tools","text":"<p>In Railtracks, you can use any Agent as a tool that other agents can use. This allows you to create complex agents that can be composed of smaller, reusable components. </p> <p>What are Nodes?</p> <p>Nodes are the building blocks of Railtracks. They are responsible for executing a single task and returning a result. Read more about Nodes.</p> <p>How to build an Agent?</p> <p>Read more about how to build an agent Build your First Agent.</p>"},{"location":"tools_mcp/tools/agents_as_tools/#understanding-toolmanifest","title":"Understanding <code>ToolManifest</code>","text":"<p>Before diving into examples, it's important to understand what a <code>ToolManifest</code> is and why it's essential when creating agents that can be used as tools - it's a specification that describes how an agent should be used when called as a tool by other agents and defines:</p> <ul> <li><code>description</code>: What the tool does and how it should be used</li> <li><code>parameters</code>: What inputs the tool expects, including their names, types, and descriptions</li> </ul> <p><code>ToolManifest</code></p> <p>For complete details on <code>ToolManifest</code>, see the API Reference.</p> <p><code>Parameter</code></p> <p>For complete details on <code>Parameter</code>, see the API Reference.</p> <pre><code>calculator_manifest = rt.ToolManifest(\n    description=\"A calculator agent that can perform mathematical calculations and solve math problems.\",\n    parameters=[\n        rt.llm.Parameter(\n            name=\"math_problem\",\n            description=\"The mathematical problem or calculation to solve.\",\n            param_type=\"string\",\n        ),\n    ],\n)\n</code></pre> <p>The <code>ToolManifest</code> acts as a contract that tells other agents exactly how to interact with your agent when using it as a tool. Without it, other agents wouldn't know what parameters to pass or what to expect from your agent.</p>"},{"location":"tools_mcp/tools/agents_as_tools/#working-example-shopping-assistant","title":"Working Example: Shopping Assistant","text":"<p>One powerful pattern is creating agents that can call multiple tools and then making those agents available as tools themselves. This creates a hierarchical structure where complex behaviors can be encapsulated and reused. In this example, we will create a <code>ShoppingAssistant</code> agent that can perform calculations and look up product prices using a calculator agent and a price lookup function.</p> <pre><code>graph TB\n    CA[CalculatorAgent]:::agent\n    CA --&gt; add[add]:::tool\n    CA --&gt; multiply[multiply]:::tool\n    CA --&gt; divide[divide]:::tool\n\n    SA[ShoppingAssistant]:::agent\n    SA --&gt; CA\n    SA --&gt; GPD[get_price_data]:::tool\n\n    classDef agent fill:#e1f5fe\n    classDef tool fill:#fff3e0\n</code></pre>"},{"location":"tools_mcp/tools/agents_as_tools/#step-1-calculatoragent","title":"Step 1: <code>CalculatorAgent</code>","text":"<p>Let's create a math calculator agent that has access to basic math operations and can be reused as a tool:</p> <pre><code>CalculatorAgent = rt.agent_node(\n    name=\"Calculator Agent\",\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n    system_message=\"You are a helpful calculator. Solve math problems step by step using the available math operations.\",\n    tool_nodes=[add, multiply, divide],\n    manifest=calculator_manifest,  # This makes the agent usable as a tool\n)\n</code></pre> Calculation Tools <p>Although these functions are simple, they demonstrate how to encapsulate functionality for an agent. <pre><code>@rt.function_node\ndef add(a: float, b: float) -&gt; float:\n    \"\"\"Add two numbers together.\n\n    Args:\n        a (float): The first number to add.\n        b (float): The second number to add.\n\n    Returns:\n        float: The sum of a and b.\n    \"\"\"\n    return a + b\n\n@rt.function_node\ndef multiply(a: float, b: float) -&gt; float:\n    \"\"\"Multiply two numbers together.\n\n    Args:\n        a (float): The first number to multiply.\n        b (float): The second number to multiply.\n\n    Returns:\n        float: The product of a and b.\n    \"\"\"\n    return a * b\n\n@rt.function_node\ndef divide(a: float, b: float) -&gt; float:\n    \"\"\"Divide one number by another.\n\n    Args:\n        a (float): The dividend (number to be divided).\n        b (float): The divisor (number to divide by).\n\n    Returns:\n        float: The quotient of a divided by b.\n\n    Raises:\n        ZeroDivisionError: When b is zero, returns an error message string instead.\n    \"\"\"\n    if b == 0:\n        raise ZeroDivisionError(\"Error: Cannot divide by zero\")\n    return a / b\n</code></pre></p>"},{"location":"tools_mcp/tools/agents_as_tools/#step-11-invoking-the-agent-independently","title":"Step 1.1 Invoking the Agent Independently","text":"<p>This calculator can be invoked independently:</p> <pre><code>async def top_level():\n    result = await rt.call(\n        CalculatorAgent, \n        \"What is 3 + 4?\"\n        )\n    return result\nresult = asyncio.run(top_level())\n</code></pre>"},{"location":"tools_mcp/tools/agents_as_tools/#step-12-using-the-agent-as-a-tool","title":"Step 1.2 Using the Agent as a Tool","text":"<p>This calculator can be used as a tool in other agents:</p> <pre><code>ShoppingAssistant = rt.agent_node(\n    name=\"Shopping Assistant\",\n    tool_nodes=[get_price_data, CalculatorAgent],  # Use the calculator agent as a tool\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n    system_message=(\n        \"You are a shopping assistant.\" \n        \"Help users with pricing calculations including taxes, discounts, and totals.\"\n        )\n)\n</code></pre> Pricing Tool <p>You might have noticed we also provided an extra tool to the agent in addition to the <code>CalculatorAgent</code> called <code>get_price_data</code>. This is a simple function that simulates looking up product prices. <pre><code>@rt.function_node\ndef get_price_data(item: str) -&gt; dict:\n    \"\"\"\n    Retrieves price and tax rate information for common electronic items.\n    Returns default values for unknown items.\n\n    Args:\n        item (str): The name of the item to look up (e.g., \"laptop\", \"phone\", \"tablet\").\n    \"\"\"\n    # Mock pricing data\n    prices = {\n        \"laptop\": {\"price\": 999.99, \"tax_rate\": 0.08},\n        \"phone\": {\"price\": 699.99, \"tax_rate\": 0.08},\n        \"tablet\": {\"price\": 449.99, \"tax_rate\": 0.08}\n    }\n    return prices.get(item, {\"price\": 0, \"tax_rate\": 0})\n</code></pre></p> <p>In this example:  1. The <code>CalculatorAgent</code> encapsulates math operations and can solve complex calculations  2. The <code>ShoppingAssistant</code> uses both the price lookup function and the calculator agent  3. When asked about laptop costs, it fetches the price data and delegates the math to the calculator agent  <pre><code>async def shopping_assistant():\n    response = await rt.call(\n        ShoppingAssistant,\n        \"I want to buy 3 laptops. Can you calculate the total cost including tax?\",\n    )\n    return response\nresponse = asyncio.run(top_level())\n</code></pre></p> Sample Output <p>Here is a sample out of the running the <code>ShoppingAssistant</code> agent for the above user query. Please note that the output may vary based on the current training date cutoff of the LLM you use. <pre><code>LLMResponse(The total cost for 3 laptops, including tax, is approximately $3,239.97.)\n</code></pre></p> <p>Congratulations! You have created an agent that can be used as a tool by other agents. This pattern allows you to build complex behaviors by composing smaller, reusable components.</p>"},{"location":"tools_mcp/tools/agents_as_tools/#related","title":"Related","text":"<p>Want to go further with tools in Railtracks?</p> <ul> <li> <p>What are tools?    Learn how tools fit into the bigger picture of Railtracks and agent orchestration.</p> </li> <li> <p>Functions as Tools    Learn how to turn Python functions into tools.</p> </li> <li> <p>How to build your first agent    Start with the basics of agent creation.</p> </li> </ul>"},{"location":"tools_mcp/tools/functions_as_tools/","title":"Functions as Tools","text":"<p>In Railtracks, you can turn any Python function into a tool that agents can call, no special boilerplate needed. The key is to provide a Google-style docstring which acts as the tool's description and schema.  </p> <p>Function Nodes</p> <p><code>rt.function_node</code> is a convenience function that wraps a function into a Railtrack node. Read more about this DynamicFunctionNode.</p>"},{"location":"tools_mcp/tools/functions_as_tools/#creating-a-function-tool","title":"Creating a Function Tool","text":""},{"location":"tools_mcp/tools/functions_as_tools/#1-using-an-rt-function","title":"1. Using an RT Function","text":"<p>Let's start with a simple function that takes two arguments and returns their sum:</p> <pre><code>def add(a: int, b: int) -&gt; int:\n    \"\"\"\n    Adds two numbers together.\n    Args:\n        a (int): The first number.\n        b (int): The second number.\n\n    Returns:\n        int: The sum of the two numbers.\n    \"\"\"\n    return a + b\n</code></pre> <p>To turn this function into a tool, we need to provide a docstring that describes the function's parameters. Then we can pass the function to <code>rt.function_node</code> to create a tool:</p> <pre><code>AddNode = rt.function_node(add)\n</code></pre>"},{"location":"tools_mcp/tools/functions_as_tools/#2-using-a-decorator","title":"2. Using a decorator","text":"<p>Let's make another tool that we can use in our agent, this time using the <code>@rt.function_node</code> decorator:</p> <pre><code>@rt.function_node\ndef solve_expression(equation: str, solving_for: str):\n    \"\"\"\n    Solves the given equation (assumed to be equal to 0) for the specified variable.\n\n    Args:\n        equation (str): The equation to solve, written in valid Python syntax.\n        solving_for (str): The variable to solve for.\n    \"\"\"\n    # Convert the string into a symbolic expression\n    eq = sympify(equation, evaluate=True)\n\n    # Solve the equation for the given variable\n    return solve(eq, solving_for)\n</code></pre>"},{"location":"tools_mcp/tools/functions_as_tools/#using-the-tools","title":"Using the tools","text":"<p>Now that we have our tool, we can use it in our agent:</p> <pre><code>MathAgent = rt.agent_node(\n                name=\"MathAgent\",\n                tool_nodes=[\n                  solve_expression, \n                  AddNode,\n                ],    # the agent has access to these tools\n                llm = rt.llm.OpenAILLM(\"gpt-4o\"),\n            )\n\n# run the agent\nresult = asyncio.run(rt.call(MathAgent, \"What is 3 + 4?\"))\n</code></pre>"},{"location":"tools_mcp/tools/functions_as_tools/#related","title":"Related","text":"<p>Want to go further with tools in Railtracks?</p> <ul> <li> <p>What are tools?    Learn how tools fit into the bigger picture of Railtracks and agent orchestration.</p> </li> <li> <p>How to build your first agent    Follow along with a tutorial to build your first agent.</p> </li> <li> <p>Agents as Tools    Discover how you can turn entire agents into callable tools inside other agents.</p> </li> </ul>"},{"location":"tools_mcp/tools/tools/","title":"Tools","text":"<p>In Railtracks, tools are capabilities that agents can call; functions, other agents, or external systems. This makes them composable, flexible, and powerful in multi-agent workflows.</p> <p>Whether you're looking to expose your Python functions to agents, wrap other agents as tools, or explore advanced use cases like dynamic tool routing and function schemas, this section has you covered.</p>"},{"location":"tools_mcp/tools/tools/#contents","title":"Contents","text":"Page Description Functions as Tools Make Python functions callable by agents. Agents as Tools Let one agent act as a tool for another, enabling nested orchestration."},{"location":"tutorials/byfa/","title":"Build Your First Agent","text":"<p>In the quickstart, you ran a ready-made agent. Now let\u2019s build your own step by step, starting from the simplest form and gradually adding more abilities.</p>"},{"location":"tutorials/byfa/#simple-llm-agent","title":"Simple LLM Agent","text":"<p>Start with minimal ingredients: a model + a system message</p> <pre><code>SimpleLLM = rt.agent_node(\n    llm=rt.llm.AnthropicLLM(\"claude-sonnet-4-20250514\"),\n    system_message=\"You are a helpful AI assistant.\"\n)\n</code></pre> Supported LLMs <p>Check out our full list of supported providers</p>"},{"location":"tutorials/byfa/#adding-tool-calling","title":"Adding Tool Calling","text":"<p>What if your agent needs real-world data? You will need to give it tools. This allows your agent to go beyond static responses and actually interact with the real world.</p> Creating a Tool <p>All you need is a Python function with docstring and the <code>rt.function_node</code> decorator <pre><code># Use @rt.function_node decorator to convert your function into a RT tool\n@rt.function_node\ndef your_function(example_input: str):\n    \"\"\"\n    Your function description here.\n\n    Args:\n      example_input (str): The input string to process.\n\n    \"\"\"\n    pass\n</code></pre> Learn more about tools</p> <pre><code>@rt.function_node\ndef weather_tool(city: str):\n    \"\"\"\n    Returns the current weather for a given city.\n\n    Args:\n      city (str): The name of the city to get the weather for.\n    \"\"\"\n    # Simulate a weather API call\n    return f\"{city} is sunny with a temperature of 25\u00b0C.\"\n\nWeatherAgent = rt.agent_node(\n    name=\"Weather Agent\",\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n    system_message=\"You are a helpful assistant that answers weather-related questions.\",\n    tool_nodes=[weather_tool],\n)\n</code></pre> Using MCP servers <p>MCP servers can be used as tools in the RT framework. </p> <p>To connect to an MCP, please refer to our guide</p>"},{"location":"tutorials/byfa/#adding-a-structured-output","title":"Adding a Structured Output","text":"<p>Now that you've seen how to add tools. Let's look at your agent can respond with reliable typed outputs. Schemas give you reliable, machine-checked outputs you can safely consume in code, rather than brittle strings.</p> Defining a Schema <p>We use the Pydantic library to define structured data models. <pre><code>from pydantic import BaseModel, Field\n\nclass YourModel(BaseModel):\n    # Use the Field parameter for more control. \n    parameter: str = Field(default=\"default_value\", description=\"Your description of the parameter\")\n</code></pre> Visit the pydantic docs to learn about what you can do with <code>BaseModel</code>'s</p> <pre><code>class WeatherResponse(BaseModel):\n    temperature: float\n    condition: str\n\nStructuredWeatherAgent = rt.agent_node(\n    name=\"Weather Agent\",\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n    system_message=\"You are a helpful assistant that answers weather-related questions.\",\n    output_schema=WeatherResponse,\n)\n</code></pre>"},{"location":"tutorials/byfa/#structured-tool-calling","title":"Structured + Tool Calling","text":"<p>Often you will want the best of both worlds, an agent capable of both tool calling and responding in a structured format. </p> <pre><code>StructuredToolCallWeatherAgent = rt.agent_node(\n    name=\"Weather Agent\",\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n    system_message=\"You are a helpful assistant that answers weather-related questions.\",\n    tool_nodes=[weather_tool],\n    output_schema=WeatherResponse,\n)\n</code></pre>"},{"location":"tutorials/byfa/#running-agents","title":"Running Agents","text":"<p>Congratulations, you\u2019ve now built agents that call tools, return structured outputs, and even combine both. Next, let\u2019s actually run them and see them in action -&gt; Running your First Agent.</p>"},{"location":"tutorials/flows/","title":"Flows","text":"<p>Railtracks makes it easy to create custom agents with access to tools they can call to complete tasks. But what if you want to use agents themselves as tools? In this section, we\u2019ll explore more complex flows and how Railtracks gives you control over them.</p> <p>To start, let\u2019s look at the simplest case: an agent that uses another agent as a tool.</p>"},{"location":"tutorials/flows/#example","title":"Example","text":"<p><pre><code>import railtracks as rt\nfrom pydantic import BaseModel\n\nclass WeatherResponse(BaseModel):\n    temperature: float\n    condition: str\n\ndef weather_tool(city: str):\n    \"\"\"\n    Returns the current weather for a given city.\n\n    Args:\n      city (str): The name of the city to get the weather for.\n    \"\"\"\n    # Simulate a weather API call\n    return f\"{city} is sunny with a temperature of 25\u00b0C.\"\n\nweather_manifest = rt.ToolManifest(\ndescription=\"A tool you can call to see what the weather in a specified city\",\n    parameters=[rt.llm.Parameter(name=\"prompt\", param_type=\"string\", description=\"This is the prompt that you should provide that tells the CodeAgent what you would like to code.\")]\n)\n\n#As before, we will create our Weather Agent with the additional tool manifest so that other agents know how to use it\nWeatherAgent = rt.agent_node(\n    name=\"Weather Agent\",\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n    system_message=\"You are a helpful assistant that answers weather-related questions.\",\n    tool_nodes=[rt.function_node(weather_tool)],\n    output_schema=WeatherResponse,\n    manifest=weather_manifest\n)\n\n#Now lets create a hiking planner agent\nHikingAgent = rt.agent_node(\n    name=\"Hiking Agent\",\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n    system_message=\"You are a helpful assistant that answers questions about which cities have the best conditions for hiking. The user should specify multiple cities near them.\",\n    tool_nodes=[WeatherAgent],\n)\n</code></pre> You can see here that the flow will look like this</p> <pre><code>graph LR\n    A[Hiking Agent] --&gt; B[Weather Agent]\n    B --&gt; C[Weather API]\n    C --&gt; B\n    B --&gt; A\n\n    A -.-&gt;|\"Calls for all cities\"| B\n</code></pre>"},{"location":"tutorials/flows/#another-simple-flow-but-using-context-to-ensure-precise-outputs","title":"Another Simple Flow but Using Context to Ensure Precise Outputs","text":"<p>Specialized agents perform better than generalist ones. For the simplest of coding projects, you might use a Top Level Agent for ideation and dialogue, a Coding Agent for the code itself, and a Static Checker for validation. It would be important that once the Static Checker approves code, no agents modify it further though. </p> <p>One important aspect of Railtracks is that it handles these complex flows through wrappers. All functions and flows can become nodes that you can run by wrapping them with <code>function_node</code>.</p> <p>In the following example you'll see an example of how Railtracks deals with mid-flow validation.</p>"},{"location":"tutorials/flows/#example_1","title":"Example","text":"<pre><code>import ast\n\n#Static checking function\ndef static_check(code: str) -&gt; tuple[bool, str]:\n    \"\"\"\n    Checks the syntax validity of Python code stored in the variable `code`.\n\n    Attempts to parse the code using Python's AST module. Returns a tuple indicating whether the syntax is valid and a message describing the result.\n\n    Returns:\n        tuple[bool, str]:\n            - True and a success message if the syntax is valid.\n            - False and an error message if a SyntaxError is encountered.\n    \"\"\"\n    try:\n        ast.parse(code)\n        return True, \"Syntax is valid\"\n    except SyntaxError as e:\n        return False, f\"Syntax error: {e}\"\n\nCodeManifest = rt.ToolManifest(\n    \"\"\"This is an agent that is an python coder and can write any\n     code for you if you specify what you would like.\"\"\",\n    set([rt.llm.Parameter(\n        name='prompt',\n        param_type='string',\n        description=\"\"\"This is the prompt that you should provide that \n        tells the CodeAgent what you would like to code.\"\"\",\n        )])\n    )\n\nCodingMessage = \"\"\"You are a master python agent that helps users by \nproviding elite python code for their requests. You will output valid python code that can be directly used without any further editing. Do not add anything other than the python code and python comments if you see fit.\"\"\"\n\nCoordinatorMessage = \"\"\"You are a helpful assistant that will talk to users about the type of code they want. You have access to a CodeAgent tool to generate the code the user is looking for. Your job is to clarify with users to ensure that they have provided all details required to write the code and then effectively communicate that to the CodeAgent. Do not write any code and strictly refer to the CodeAgent for this.\"\"\"\n\n#Create our Coding Agent as usual\nCodingAgent = rt.agent_node(\n    name=\"Code Tool\",\n    system_message=CodingMessage,\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n    )\n\n#Wrap our Validation and file writing flow in a function\nasync def code_agent(prompt : str):\n    valid = False\n    problem = \"There were no problems last time\"\n    while not valid:\n        response = await rt.call(\n        CodingAgent,\n        user_input=prompt + \" Your Problem Last Time: \" + problem\n        )\n\n        valid, problem = static_check(response.text)\n\n    with open(\"new_script.py\", \"w\") as file:\n        file.write(response.text)\n\n    return \"Success\"\n\ntool_nodes = {rt.function_node(code_agent, manifest=CodeManifest)}\nCoordinatorAgent = rt.agent_node(\n    system_message=CoordinatorMessage,\n    tool_nodes=tool_nodes,\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n    )\n\nasync def run():\n    resp = await rt.call(\n            CoordinatorAgent,\n            user_input=\"Would you be able to generate me code that takes 2 numbers as input and returns the sum?\"\n        )\n    print(resp)\n</code></pre>"},{"location":"tutorials/flows/#what-this-flow-would-look-like","title":"What this flow would look like","text":"<pre><code>graph TD\n    Coordinator --&gt;|1| CodeAgentWrapper[\"Code Agent Wrapper\"]\n    CodeAgentWrapper --&gt;|2| CodeAgent[\"Code Agent\"]\n    CodeAgent --&gt;|3| StaticCheck[\"Static Check\"]\n    StaticCheck --&gt;|4| C{\"Valid?\"}\n    C --&gt;|No| CodeAgent\n    C --&gt;|Yes| D[\"Write To File\"]\n    D --&gt; CodeAgentWrapper\n    CodeAgentWrapper --&gt;|5| Coordinator\n</code></pre> <p>Structuring Flows</p> <p>When possible, you should try to keep your flows linear. Notice above that it would also be possible to give the coordinator access to both the static checker as well as the coding agent. In such a simple example, likely this would have been fine but two problems arise with this approach. Firstly, this is a simple validation step that should happen every time code is generated. Leaving it up to the coordinator to call the static checker adds unnecessary complexity to the agent and creates the possibility for the the validation step to be skipped. It can sometimes be easier to think about a more flexible flow but you try to linearize your flow as much as possible. The second problem we will discuss below.</p>"},{"location":"tutorials/flows/#handling-more-complex-flows","title":"Handling More Complex Flows","text":"<p>While <code>function_node</code> works well for linear flows, some scenarios require transferring between different agents like moving from technical support to billing in a customer service system. In these cases, you need to pass data directly between agents without mutations or the \"telephone game\" effect of traditional handoffs. Railtracks solves this with context, a mechanism for sharing data across agent transfers while preserving integrity. Let's see how context enables reliable multi-agent workflows.</p>"},{"location":"tutorials/flows/#customer-service-agents","title":"Customer Service Agents","text":"<pre><code>#Initialize all your system messages, schemas, and tools here.\n\nQualityAssuranceAgent = rt.agent_node(\n    name=\"Quality Assurance Agent\",\n    output_schema=StructuredResponse,\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n    #adding all other arguments as needed\n    )\n\nProductExpertAgent = rt.agent_node(\n    name=\"Product Expert Agent\",\n    output_schema=StructuredResponse,\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n    #adding all other arguments as needed\n    )\n\nBillingAgent = rt.agent_node(\n    name=\"Billing Agent\",\n    output_schema=StructuredResponse,\n    llm=rt.llm.OpenAILLM(\"gpt-4o\", stream=False),\n    #adding all other arguments as needed\n    )\n\nTechnicalAgent = rt.agent_node(\n    name=\"Technical Support Agent\",\n    output_schema=StructuredResponse,\n    llm=rt.llm.OpenAILLM(\"gpt-4o\", stream=False),\n    #adding all other arguments as needed\n    )\n\nasync def billing_tool(prompt : str):\n    try:\n        prompt = prompt + \"Previously the User had this interaction \" + rt.context.get(\"info_from_other_agents\")\n        has_context = True\n    except KeyError:\n        has_context = False\n    response = await rt.call(\n        BillingAgent,\n        user_input=prompt\n        )\n    if has_context:\n        previous = rt.context.get(\"info_from_other_agents\")\n        new = previous + response.structured.info\n    else:\n        new = response.structured.info\n    rt.context.put(\"info_from_other_agents\", new)\n\nasync def technical_tool(prompt : str):\n    try:\n        prompt = prompt + \"Previously the User had this interaction \" + rt.context.get(\"info_from_other_agents\")\n        has_context = True\n    except KeyError:\n        has_context = False\n    response = await rt.call(\n        TechnicalAgent,\n        user_input=prompt\n        )\n    if has_context:\n        previous = rt.context.get(\"info_from_other_agents\")\n        new = previous + response.structured.info\n    else:\n        new = response.structured.info\n    rt.context.put(\"info_from_other_agents\", new)\n\n#This would be similar to functions above\ndef qa_tool():\n    ...\n#This would be similar to functions above\ndef pe_tool():\n    ...\n\ntools = {rt.function_node(billing_tool), rt.function_node(technical_tool), rt.function_node(qa_tool), rt.function_node(pe_tool)}\n\nCoordinator = rt.agent_node(\n    name=\"Coordinator Agent\",\n    tool_nodes=tools,\n    llm=rt.llm.OpenAILLM(\"gpt-4o\"),\n    system_message=CoordinatorMessage,\n)\n\nasync def main():\n    response = await rt.call(\n            CoordinatorAgent,\n            user_input=\"\"\n        )\n</code></pre>"},{"location":"tutorials/flows/#what-an-example-flow-would-look-like","title":"What an example flow would look like","text":"<pre><code>graph TD\n    Coordinator[\"Coordinator\"] --&gt;|1| Technical[\"Technical Support Agent\"]\n    Technical --&gt; |2| Tools[\"Tool Calls\"]\n    Tools --&gt; |3| Technical\n    Technical --&gt; |Puts| Context[\"Context\"]\n    Technical --&gt; |4| Coordinator[\"Coordinator\"]\n    Coordinator --&gt;|5| Billing[\"Billing Agent\"]\n    Context --&gt; |Gets| Billing\n    Billing --&gt; |6| BillingTools[\"Tool Calls\"]\n    BillingTools --&gt; |7| Billing\n    Billing --&gt; |8| Coordinator\n</code></pre>"},{"location":"tutorials/rag_tutorial/","title":"RAG Tutorial","text":"<p>This tutorial will get you from zero to RAG-powered agent in just a few minutes. We'll cover how Railtracks can help you build the perfect RAG Agent for your needs.</p> <p>Don't know what RAG is or why you might want to use it? Check out our brief explainer here</p> <p>When you create a RAG node, Railtracks automatically take care of chunking, embedding, and searching for you making it easy to use. All you need to do is provide a text file and let Railtracks take care of it for you from there. Let's look at an example of what this could look like.</p>"},{"location":"tutorials/rag_tutorial/#quickstart-prebuilt-rag-node","title":"Quickstart: Prebuilt RAG Node","text":"<p>This is the easiest way to add RAG to your app. Let's build a simple knowledge base in under 10 lines of code:</p> <pre><code>import asyncio\nimport railtracks as rt\nfrom railtracks.prebuilt import rag_node\n\nretriever = rt.prebuilt.rag_node([\n    \"Steve likes apples and enjoys them as snacks\",\n    \"John prefers bananas for their potassium content\",\n    \"Alice loves oranges for vitamin C\",\n])\n\nquestion = \"What does Steve like?\"\nresults = asyncio.run(rt.call(retriever, question, top_k=3))\n\ncontext = \"\\n\".join(\n    f\"Document {i+1} (score: {r.score:.4f}): {r.record.text}\"\n    for i, r in enumerate(results)\n)\n\nprint(f\"Question: {question}\")\nprint(f\"Retrieved context:\\n{context}\")\n</code></pre> <p>Example Output</p> <p>Your RAG node can now answer questions based on your documents:</p> <p>Query: <code>\"Who is Steve?\"</code></p> <p>Response: <code>\"In our company, Steve is the lead engineer and ...\"</code></p> <p>Next Steps</p> <p>Ready to build with RAG</p> <ul> <li>RAG Reference Documentation to learn how to build RAG applications in RT.</li> <li>Tools Documentation for integrating any type of tool.</li> </ul>"},{"location":"tutorials/ryfa/","title":"How to Run Your First Agent","text":""},{"location":"tutorials/ryfa/#calling-the-agent-directly","title":"Calling the Agent directly","text":"<p>Once you have defined your agent class (Build Your First Agent) you can then run your workflow and see results!</p> <p>To begin you just have to use <code>call</code> method from Railtracks. This is an asynchronous method so you will need to run it in an async context.</p> Asynchronous <pre><code>async def weather_agent():\n    response = await rt.call(\n        StructuredToolCallWeatherAgent, \n        \"What is the forecast for Vancouver today?\"\n        )\n    return response\n</code></pre> <p>Agent input options</p> <p>There are multiple ways to provide input to your agent.</p> single user message <p>If you'd like to simply provide a single user message, you can pass it as a string directly to the <code>call</code> </p> few-shot prompting <p>If you want to provide a few-shot prompt, you can pass a list of messages to the <code>call</code> functions, with the specific message for each role being passed as an input to its specific role ie (<code>rt.llm.UserMessage</code> for user, <code>rt.llm.AssistantMessage</code> for assistant):  <pre><code>async def few_shot():\n    response = await rt.call(\n        WeatherAgent,\n        [\n            rt.llm.UserMessage(\"What is the forecast for BC today?\"),\n            rt.llm.AssistantMessage(\"Please specify the specific city in BC you're interested in\"),\n            rt.llm.UserMessage(\"Vancouver\"),\n        ]\n    )\n    return response\n</code></pre></p> <p>Asynchronous Execution</p> <p>Since the <code>call</code> function is asynchronous and needs to be awaited, you should ensure that you are running this code within an asynchronous context like the <code>main</code> function in the code snippet above.</p> <p>Jupyter Notebooks: If you are using in a notebook, you can run the code in a cell with <code>await</code> directly.</p> <p>For more info on using <code>async/await</code> in RT, see Async/Await in Python.</p> <p>Dynamic Runtime Configuration</p> <p>If you pass <code>llm</code> to <code>agent_node</code> and then a different llm model to <code>call</code> function, Railtracks will use the latter one. If you pass <code>system_message</code> to <code>agent_node</code> and then another <code>system_message</code> to <code>call</code>, the system messages will be stacked.</p> Example <p><pre><code>    import railtracks as rt\n    from pydantic import BaseModel\n\n    class WeatherResponse(BaseModel):\n        temperature: float\n        condition: str\n\n\n    system_message = rt.llm.SystemMessage(\n        \"You can also geolocate the user\"\n    )\n    user_message = rt.llm.UserMessage(\n        \"Would you please be able to tell me the forecast for the next week?\"\n    )\n    async def main():\n\n        response = await rt.call(\n            StructuredToolCallWeatherAgent,\n            user_input=rt.llm.MessageHistory([system_message, user_message]),\n            llm=rt.llm.AnthropicLLM(\"claude-3-5-sonnet-20241022\"),\n        )\n\n        return response\n</code></pre> In this example Railtracks will use claude rather than chatgpt and the <code>system_message</code> will become <code>\"You are a helpful assistant that answers weather-related questions. If not specified, the user is talking about Vancouver.\"</code></p> <p>Just like that you have run your first agent!</p>"},{"location":"tutorials/ryfa/#calling-the-agent-within-a-session","title":"Calling the Agent within a Session","text":"<p>Alternatively, you can run your agent within a session using the <code>rt.Session</code> context manager. This allows you to manage the session state and run multiple agents or workflows within the same session and providing various options such as setting a timeout, a shared context (Context), and more.</p> <pre><code>async def session_based():\n    with rt.Session(\n        context=weather_context,\n        timeout=60  # seconds\n    ):\n        response = await rt.call(\n            WeatherAgent,\n            \"What is the weather like in Vancouver?\"\n        )\n</code></pre> <p>For more details on how to use sessions, please refer to the Sessions documentation.</p>"},{"location":"tutorials/ryfa/#retrieving-the-results-of-a-run","title":"Retrieving the Results of a Run","text":"<p>All agents return a response object which you can use to get the last message or the entire message history if you would prefer.</p> <p>Reponse of a Run</p> Unstructured ResponseStructured Response <p>In the unstructured response example, the last message from the agent and the entire message history can be accessed using the <code>text</code> and <code>message_history</code> attributes of the response object, respectively.</p> <pre><code>print(f\"Last Message: {response.text}\")\nprint(f\"Message History: {response.message_history}\")\n</code></pre> <p>WeatherResponse</p> <pre><code>class WeatherResponse(BaseModel):\n    temperature: float\n    condition: str\n</code></pre> <p>In the structured response example, the <code>output_schema</code> parameter is used to define the expected output structure. The response can then be accessed using the <code>structured</code> attribute.</p> <pre><code>print(f\"Condition: {response.structured.condition}\")\nprint(f\"Temperature: {response.structured.temperature}\")\n</code></pre>"}]}