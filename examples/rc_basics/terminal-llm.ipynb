{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c626569d33a13ec",
   "metadata": {},
   "source": [
    "# Terminal LLM\n",
    "In the following document we will outline how to use the terminal LLM. It is a common use case for a simple LLM with a system prompt and a text output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T21:30:36.906266Z",
     "start_time": "2025-05-30T21:30:34.940791Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requestcompletion as rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f8ed72aa6cfc093",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T21:30:38.070161Z",
     "start_time": "2025-05-30T21:30:38.054298Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aryan\\dev\\rc\\src\\requestcompletion\\exceptions\\node_creation\\validation.py:219: UserWarning: You have set max_tool_calls to None. The llm will be able to make unlimited tool calls.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "system_message = rc.llm.SystemMessage(\"You are a helpful AI writing assistant. You should provide suggestions and corrections to the user's writing. Keep your advice concise and to the point.\")\n",
    "\n",
    "WritingEditor = rc.library.terminal_llm(\"Writing Editor\", system_message=system_message, model=rc.llm.GeminiLLM(\"gemini-2.5-flash\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "372f4fbc62f01588",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T21:30:43.367902Z",
     "start_time": "2025-05-30T21:30:39.401240Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[+3053.963s] RC          : INFO     - START CREATED Writing Editor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[+3054.160s] RC          : ERROR    - Writing Editor FAILED\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\litellm\\llms\\vertex_ai\\gemini\\vertex_and_google_ai_studio_gemini.py\", line 1605, in async_completion\n",
      "    response = await client.post(\n",
      "  File \"c:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\litellm\\litellm_core_utils\\logging_utils.py\", line 135, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py\", line 278, in post\n",
      "    raise e\n",
      "  File \"c:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py\", line 234, in post\n",
      "    response.raise_for_status()\n",
      "  File \"c:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\httpx\\_models.py\", line 829, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '404 Not Found' for url 'https://generativelanguage.googleapis.com/:generateContent'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\litellm\\main.py\", line 525, in acompletion\n",
      "    response = await init_response\n",
      "  File \"c:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\litellm\\llms\\vertex_ai\\gemini\\vertex_and_google_ai_studio_gemini.py\", line 1611, in async_completion\n",
      "    raise VertexAIError(\n",
      "litellm.llms.vertex_ai.common_utils.VertexAIError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Aryan\\dev\\rc\\src\\requestcompletion\\llm\\model.py\", line 129, in achat\n",
      "    response = await self._achat(messages, **kwargs)\n",
      "  File \"C:\\Users\\Aryan\\dev\\rc\\src\\requestcompletion\\llm\\models\\_litellm_wrapper.py\", line 235, in _achat\n",
      "    raw = await self._ainvoke(messages=messages, **kwargs)\n",
      "  File \"C:\\Users\\Aryan\\dev\\rc\\src\\requestcompletion\\llm\\models\\_litellm_wrapper.py\", line 218, in _ainvoke\n",
      "    completion = await litellm.acompletion(\n",
      "  File \"c:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\litellm\\utils.py\", line 1494, in wrapper_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\litellm\\utils.py\", line 1355, in wrapper_async\n",
      "    result = await original_function(*args, **kwargs)\n",
      "  File \"c:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\litellm\\main.py\", line 544, in acompletion\n",
      "    raise exception_type(\n",
      "  File \"c:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2271, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 1293, in exception_type\n",
      "    raise NotFoundError(\n",
      "litellm.exceptions.NotFoundError: litellm.NotFoundError: VertexAIException - \n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Aryan\\dev\\rc\\src\\requestcompletion\\nodes\\library\\terminal_llm.py\", line 25, in invoke\n",
      "    returned_mess = await self.model.achat(self.message_hist)\n",
      "  File \"C:\\Users\\Aryan\\dev\\rc\\src\\requestcompletion\\llm\\model.py\", line 131, in achat\n",
      "    self._run_exception_hooks(messages, Exception(\"Error during async chat\"))\n",
      "  File \"C:\\Users\\Aryan\\dev\\rc\\src\\requestcompletion\\llm\\model.py\", line 108, in _run_exception_hooks\n",
      "    hook(message_history, exception)\n",
      "  File \"C:\\Users\\Aryan\\dev\\rc\\src\\requestcompletion\\nodes\\library\\_llm_base.py\", line 116, in _exception_llm_hook\n",
      "    raise exception\n",
      "Exception: Error during async chat\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Aryan\\dev\\rc\\src\\requestcompletion\\execution\\execution_strategy.py\", line 54, in execute\n",
      "    result = await invoke_func()\n",
      "  File \"C:\\Users\\Aryan\\dev\\rc\\src\\requestcompletion\\execution\\task.py\", line 23, in invoke\n",
      "    return await self.node.tracked_invoke()\n",
      "  File \"C:\\Users\\Aryan\\dev\\rc\\src\\requestcompletion\\nodes\\nodes.py\", line 165, in tracked_invoke\n",
      "    raise e\n",
      "  File \"C:\\Users\\Aryan\\dev\\rc\\src\\requestcompletion\\nodes\\nodes.py\", line 163, in tracked_invoke\n",
      "    return await self.invoke()\n",
      "  File \"C:\\Users\\Aryan\\dev\\rc\\src\\requestcompletion\\nodes\\nodes.py\", line 44, in async_wrapper\n",
      "    return await method(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\Aryan\\dev\\rc\\src\\requestcompletion\\nodes\\library\\terminal_llm.py\", line 27, in invoke\n",
      "    raise LLMError(\n",
      "requestcompletion.exceptions.errors.LLMError: \n",
      "LLM Error: Exception during model chat: Error during async chat\n",
      "Details:\n",
      "  Message History:\n",
      "    system: You are a helpful AI writing assistant. You should provide suggestions and corrections to the user's writing. Keep your advice concise and to the point.\n",
      "    user: I am a writer who is working on a novel. Here is my first paragraph: 'The sun was setting over the horizon, casting a warm glow over the landscape. The birds were chirping in the trees, and a gentle breeze rustled the leaves.'\n"
     ]
    },
    {
     "ename": "LLMError",
     "evalue": "\n\u001b[91m\u001b[1m\u001b[91mLLM Error: \u001b[0m\u001b[91mException during model chat: Error during async chat\u001b[0m\u001b[0m\n\u001b[1m\u001b[92mDetails:\n\u001b[0m  \u001b[1m\u001b[92mMessage History:\n\u001b[0m\u001b[92m    system: You are a helpful AI writing assistant. You should provide suggestions and corrections to the user's writing. Keep your advice concise and to the point.\n    user: I am a writer who is working on a novel. Here is my first paragraph: 'The sun was setting over the horizon, casting a warm glow over the landscape. The birds were chirping in the trees, and a gentle breeze rustled the leaves.'\u001b[0m",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\litellm\\llms\\vertex_ai\\gemini\\vertex_and_google_ai_studio_gemini.py:1605\u001b[0m, in \u001b[0;36mVertexLLM.async_completion\u001b[1;34m(self, model, messages, model_response, print_verbose, data, custom_llm_provider, timeout, encoding, logging_obj, stream, optional_params, litellm_params, logger_fn, api_base, client, vertex_project, vertex_location, vertex_credentials, gemini_api_key, extra_headers)\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1605\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m   1606\u001b[0m         api_base, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mcast(\u001b[38;5;28mdict\u001b[39m, request_body)\n\u001b[0;32m   1607\u001b[0m     )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1608\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[1;32mc:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\litellm\\litellm_core_utils\\logging_utils.py:135\u001b[0m, in \u001b[0;36mtrack_llm_api_timing.<locals>.decorator.<locals>.async_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:278\u001b[0m, in \u001b[0;36mAsyncHTTPHandler.post\u001b[1;34m(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code)\n\u001b[1;32m--> 278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:234\u001b[0m, in \u001b[0;36mAsyncHTTPHandler.post\u001b[1;34m(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\u001b[0m\n\u001b[0;32m    233\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39msend(req, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[1;32m--> 234\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\httpx\\_models.py:829\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    828\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[1;32m--> 829\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPStatusError\u001b[0m: Client error '404 Not Found' for url 'https://generativelanguage.googleapis.com/:generateContent'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mVertexAIError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\litellm\\main.py:525\u001b[0m, in \u001b[0;36macompletion\u001b[1;34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, **kwargs)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39miscoroutine(init_response):\n\u001b[1;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m init_response\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\litellm\\llms\\vertex_ai\\gemini\\vertex_and_google_ai_studio_gemini.py:1611\u001b[0m, in \u001b[0;36mVertexLLM.async_completion\u001b[1;34m(self, model, messages, model_response, print_verbose, data, custom_llm_provider, timeout, encoding, logging_obj, stream, optional_params, litellm_params, logger_fn, api_base, client, vertex_project, vertex_location, vertex_credentials, gemini_api_key, extra_headers)\u001b[0m\n\u001b[0;32m   1610\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code\n\u001b[1;32m-> 1611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m VertexAIError(\n\u001b[0;32m   1612\u001b[0m         status_code\u001b[38;5;241m=\u001b[39merror_code,\n\u001b[0;32m   1613\u001b[0m         message\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m   1614\u001b[0m         headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1615\u001b[0m     )\n\u001b[0;32m   1616\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException:\n",
      "\u001b[1;31mVertexAIError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\dev\\rc\\src\\requestcompletion\\llm\\model.py:129\u001b[0m, in \u001b[0;36mModelBase.achat\u001b[1;34m(self, messages, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 129\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_achat(messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32m~\\dev\\rc\\src\\requestcompletion\\llm\\models\\_litellm_wrapper.py:235\u001b[0m, in \u001b[0;36mLiteLLMWrapper._achat\u001b[1;34m(self, messages, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_achat\u001b[39m(\u001b[38;5;28mself\u001b[39m, messages: MessageHistory, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[1;32m--> 235\u001b[0m     raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ainvoke(messages\u001b[38;5;241m=\u001b[39mmessages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_handle_base(\u001b[38;5;241m*\u001b[39mraw)\n",
      "File \u001b[1;32m~\\dev\\rc\\src\\requestcompletion\\llm\\models\\_litellm_wrapper.py:218\u001b[0m, in \u001b[0;36mLiteLLMWrapper._ainvoke\u001b[1;34m(self, messages, stream, response_format, **call_kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mUserWarning\u001b[39;00m, module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpydantic.*\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m )  \u001b[38;5;66;03m# Supress pydantic warnings. See issue #204 for more deatils.\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39macompletion(\n\u001b[0;32m    219\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_name, messages\u001b[38;5;241m=\u001b[39mlitellm_messages, stream\u001b[38;5;241m=\u001b[39mstream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmerged\n\u001b[0;32m    220\u001b[0m )\n\u001b[0;32m    222\u001b[0m mess_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_message_info(completion, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time)\n",
      "File \u001b[1;32mc:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\litellm\\utils.py:1494\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper_async\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout)\n\u001b[1;32m-> 1494\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\litellm\\utils.py:1355\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper_async\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1354\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[1;32m-> 1355\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m original_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1356\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[1;32mc:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\litellm\\main.py:544\u001b[0m, in \u001b[0;36macompletion\u001b[1;34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, **kwargs)\u001b[0m\n\u001b[0;32m    543\u001b[0m custom_llm_provider \u001b[38;5;241m=\u001b[39m custom_llm_provider \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 544\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:2271\u001b[0m, in \u001b[0;36mexception_type\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[0;32m   2270\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, litellm_response_headers)\n\u001b[1;32m-> 2271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   2272\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Aryan\\dev\\rc\\.venv\\lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:1293\u001b[0m, in \u001b[0;36mexception_type\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFoundError(\n\u001b[0;32m   1294\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVertexAIException - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_exception\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1295\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[0;32m   1296\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   1297\u001b[0m     )\n\u001b[0;32m   1298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_exception\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m408\u001b[39m:\n",
      "\u001b[1;31mNotFoundError\u001b[0m: litellm.NotFoundError: VertexAIException - ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\dev\\rc\\src\\requestcompletion\\nodes\\library\\terminal_llm.py:25\u001b[0m, in \u001b[0;36mTerminalLLM.invoke\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     returned_mess \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39machat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage_hist)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\dev\\rc\\src\\requestcompletion\\llm\\model.py:131\u001b[0m, in \u001b[0;36mModelBase.achat\u001b[1;34m(self, messages, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m--> 131\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_exception_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;167;43;01mException\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mError during async chat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\dev\\rc\\src\\requestcompletion\\llm\\model.py:108\u001b[0m, in \u001b[0;36mModelBase._run_exception_hooks\u001b[1;34m(self, message_history, exception)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_hook:\n\u001b[1;32m--> 108\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\dev\\rc\\src\\requestcompletion\\nodes\\library\\_llm_base.py:116\u001b[0m, in \u001b[0;36mLLMBase._exception_llm_hook\u001b[1;34m(self, message_history, exception)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_details[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_details\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    109\u001b[0m     RequestDetails(\n\u001b[0;32m    110\u001b[0m         message_input\u001b[38;5;241m=\u001b[39mdeepcopy(message_history),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m     )\n\u001b[0;32m    115\u001b[0m )\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[1;31mException\u001b[0m: Error during async chat",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLLMError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m mess_hist \u001b[38;5;241m=\u001b[39m rc\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mMessageHistory([\n\u001b[0;32m      3\u001b[0m     rc\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mUserMessage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am a writer who is working on a novel. Here is my first paragraph: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe sun was setting over the horizon, casting a warm glow over the landscape. The birds were chirping in the trees, and a gentle breeze rustled the leaves.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      4\u001b[0m ])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m rc\u001b[38;5;241m.\u001b[39mRunner() \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[1;32m----> 7\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(WritingEditor, message_history\u001b[38;5;241m=\u001b[39mmess_hist)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(info\u001b[38;5;241m.\u001b[39manswer)\n",
      "File \u001b[1;32m~\\dev\\rc\\src\\requestcompletion\\run.py:175\u001b[0m, in \u001b[0;36mRunner.run\u001b[1;34m(self, start_node, *args, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    169\u001b[0m     start_node: Callable[_P, Node] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs,\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[0;32m    172\u001b[0m ):\n\u001b[0;32m    173\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs the rc framework with the given start node and provided arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall(start_node, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrc_state\u001b[38;5;241m.\u001b[39minfo\n",
      "File \u001b[1;32m~\\dev\\rc\\src\\requestcompletion\\run.py:164\u001b[0m, in \u001b[0;36mRunner.call\u001b[1;34m(self, node, *args, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    159\u001b[0m     node: Callable[_P, Node[_TOutput]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[0;32m    163\u001b[0m ):\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m call(node, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\dev\\rc\\src\\requestcompletion\\interaction\\call.py:72\u001b[0m, in \u001b[0;36mcall\u001b[1;34m(node, *args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# if the context is not active then we know this is the top level request\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_context_active():\n\u001b[1;32m---> 72\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _start(node, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# if the context is active then we can just run the node\u001b[39;00m\n",
      "File \u001b[1;32m~\\dev\\rc\\src\\requestcompletion\\interaction\\call.py:135\u001b[0m, in \u001b[0;36m_start\u001b[1;34m(node, args, kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# Here we wait the completion of the future with timeouts.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait_for(wrapped_fut(fut), timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# if the internal flag is set then the coroutine itself raised a timeout error and it was not the wait\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;66;03m#  for function.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout_exception_flag[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py:445\u001b[0m, in \u001b[0;36mwait_for\u001b[1;34m(fut, timeout)\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fut\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m--> 445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m     fut\u001b[38;5;241m.\u001b[39mremove_done_callback(cb)\n",
      "File \u001b[1;32m~\\dev\\rc\\src\\requestcompletion\\interaction\\call.py:124\u001b[0m, in \u001b[0;36m_start.<locals>.wrapped_fut\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped_fut\u001b[39m(f: Coroutine):\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m f\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mTimeoutError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m    126\u001b[0m         timeout_exception_flag[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\dev\\rc\\src\\requestcompletion\\interaction\\call.py:186\u001b[0m, in \u001b[0;36m_execute\u001b[1;34m(node, args, kwargs, message_filter)\u001b[0m\n\u001b[0;32m    173\u001b[0m f \u001b[38;5;241m=\u001b[39m publisher\u001b[38;5;241m.\u001b[39mlistener(message_filter(request_id), output_mapping)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m publisher\u001b[38;5;241m.\u001b[39mpublish(\n\u001b[0;32m    176\u001b[0m     RequestCreation(\n\u001b[0;32m    177\u001b[0m         current_node_id\u001b[38;5;241m=\u001b[39mget_parent_id(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    184\u001b[0m )\n\u001b[1;32m--> 186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m f\n",
      "File \u001b[1;32m~\\dev\\rc\\src\\requestcompletion\\pubsub\\publisher.py:212\u001b[0m, in \u001b[0;36mPublisher.listener\u001b[1;34m(self, message_filter, result_mapping, listener_name)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munsubscribe(sub_id)\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result_mapping(returnable_result)\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m single_listener()\n",
      "File \u001b[1;32m~\\dev\\rc\\src\\requestcompletion\\pubsub\\publisher.py:210\u001b[0m, in \u001b[0;36mPublisher.listener.<locals>.single_listener\u001b[1;34m()\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m returnable_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mListener should have received a message before returning.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m )\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munsubscribe(sub_id)\n\u001b[1;32m--> 210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresult_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturnable_result\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\dev\\rc\\src\\requestcompletion\\pubsub\\utils.py:22\u001b[0m, in \u001b[0;36moutput_mapping\u001b[1;34m(result)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, RequestFailure):\n\u001b[0;32m     21\u001b[0m     result: RequestFailure\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m result\u001b[38;5;241m.\u001b[39merror\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, FatalFailure):\n\u001b[0;32m     24\u001b[0m     result: FatalFailure\n",
      "File \u001b[1;32m~\\dev\\rc\\src\\requestcompletion\\execution\\execution_strategy.py:54\u001b[0m, in \u001b[0;36mAsyncioExecutionStrategy.execute\u001b[1;34m(self, task)\u001b[0m\n\u001b[0;32m     51\u001b[0m publisher \u001b[38;5;241m=\u001b[39m get_publisher()\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m invoke_func()\n\u001b[0;32m     55\u001b[0m     response \u001b[38;5;241m=\u001b[39m RequestSuccess(\n\u001b[0;32m     56\u001b[0m         request_id\u001b[38;5;241m=\u001b[39mtask\u001b[38;5;241m.\u001b[39mrequest_id,\n\u001b[0;32m     57\u001b[0m         node_state\u001b[38;5;241m=\u001b[39mNodeState(task\u001b[38;5;241m.\u001b[39mnode),\n\u001b[0;32m     58\u001b[0m         result\u001b[38;5;241m=\u001b[39mresult,\n\u001b[0;32m     59\u001b[0m     )\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\dev\\rc\\src\\requestcompletion\\execution\\task.py:23\u001b[0m, in \u001b[0;36mTask.invoke\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The callable that this task is representing.\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m update_parent_id(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39muuid)\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mtracked_invoke()\n",
      "File \u001b[1;32m~\\dev\\rc\\src\\requestcompletion\\nodes\\nodes.py:165\u001b[0m, in \u001b[0;36mNode.tracked_invoke\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke()\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    167\u001b[0m     latency \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\dev\\rc\\src\\requestcompletion\\nodes\\nodes.py:163\u001b[0m, in \u001b[0;36mNode.tracked_invoke\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    161\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke()\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\dev\\rc\\src\\requestcompletion\\nodes\\nodes.py:44\u001b[0m, in \u001b[0;36mNodeCreationMeta.__init__.<locals>.async_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21masync_wrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39miscoroutinefunction(\n\u001b[0;32m     42\u001b[0m         method\n\u001b[0;32m     43\u001b[0m     ):  \u001b[38;5;66;03m# check if the method is a coroutine\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mto_thread(method, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\dev\\rc\\src\\requestcompletion\\nodes\\library\\terminal_llm.py:27\u001b[0m, in \u001b[0;36mTerminalLLM.invoke\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     25\u001b[0m     returned_mess \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39machat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage_hist)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LLMError(\n\u001b[0;32m     28\u001b[0m         reason\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException during model chat: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     29\u001b[0m         message_history\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage_hist,\n\u001b[0;32m     30\u001b[0m     )\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage_hist\u001b[38;5;241m.\u001b[39mappend(returned_mess\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m returned_mess\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mrole \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mLLMError\u001b[0m: \n\u001b[91m\u001b[1m\u001b[91mLLM Error: \u001b[0m\u001b[91mException during model chat: Error during async chat\u001b[0m\u001b[0m\n\u001b[1m\u001b[92mDetails:\n\u001b[0m  \u001b[1m\u001b[92mMessage History:\n\u001b[0m\u001b[92m    system: You are a helpful AI writing assistant. You should provide suggestions and corrections to the user's writing. Keep your advice concise and to the point.\n    user: I am a writer who is working on a novel. Here is my first paragraph: 'The sun was setting over the horizon, casting a warm glow over the landscape. The birds were chirping in the trees, and a gentle breeze rustled the leaves.'\u001b[0m"
     ]
    }
   ],
   "source": [
    "# note the system message will be automatically added so don't include it in the message history\n",
    "mess_hist = rc.llm.MessageHistory([\n",
    "    rc.llm.UserMessage(\"I am a writer who is working on a novel. Here is my first paragraph: 'The sun was setting over the horizon, casting a warm glow over the landscape. The birds were chirping in the trees, and a gentle breeze rustled the leaves.'\"),\n",
    "])\n",
    "\n",
    "with rc.Runner() as runner:\n",
    "    info = await runner.run(WritingEditor, message_history=mess_hist)\n",
    "\n",
    "print(info.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dace52aef3f2f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
