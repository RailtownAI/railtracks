{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import railtracks as rt\n",
    "from railtracks.RAG import get_rag_node, RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"Apple is deep red\",\n",
    "    \"Pear is light yellow\",\n",
    "    \"Watermelon is green on the outside and red on the inside\",\n",
    "]\n",
    "query = \"What is the color of watermelon?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great—here’s a concise review of the sigmoid (logistic) function, with a few complementary derivations and key identities.\n",
    "\n",
    "1) Logistic ODE derivation (growth with saturation)\n",
    "Assume a quantity y(x) grows proportionally to both its current size and the remaining capacity to a normalized maximum of 1:\n",
    "$$\n",
    "\\frac{dy}{dx} = y(1-y).\n",
    "$$\n",
    "Separate variables:\n",
    "$$\n",
    "\\frac{dy}{y(1-y)} = dx.\n",
    "$$\n",
    "Use partial fractions:\n",
    "$$\n",
    "\\frac{1}{y(1-y)} = \\frac{1}{y} + \\frac{1}{1-y},\n",
    "$$\n",
    "so\n",
    "$$\n",
    "\\int \\left(\\frac{1}{y} + \\frac{1}{1-y}\\right) dy = \\int dx\n",
    "\\quad\\Rightarrow\\quad\n",
    "\\ln|y| - \\ln|1-y| = x + C.\n",
    "$$\n",
    "Exponentiate:\n",
    "$$\n",
    "\\frac{y}{1-y} = C e^{x}.\n",
    "$$\n",
    "Solve for y:\n",
    "$$\n",
    "y(x) = \\frac{1}{1 + C^{-1} e^{-x}} = \\frac{1}{1 + e^{-(x+b)}},\n",
    "$$\n",
    "where b = \\ln C. With an intercept shift b and optional slope a > 0, the general form is\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}} \\quad\\text{or}\\quad \\sigma_{a,b}(x) = \\frac{1}{1 + e^{-a(x-b)}}.\n",
    "$$\n",
    "For population dynamics with carrying capacity K and rate r:\n",
    "$$\n",
    "y(t) = \\frac{K}{1 + A e^{-rt}}.\n",
    "$$\n",
    "\n",
    "2) Odds/log-odds derivation (logistic regression link)\n",
    "If the log-odds (logit) of an event is linear in x:\n",
    "$$\n",
    "\\log\\frac{p}{1-p} = w^\\top x + b,\n",
    "$$\n",
    "then exponentiate:\n",
    "$$\n",
    "\\frac{p}{1-p} = e^{w^\\top x + b}\n",
    "\\quad\\Rightarrow\\quad\n",
    "p = \\frac{e^{w^\\top x + b}}{1 + e^{w^\\top x + b}}\n",
    "= \\frac{1}{1 + e^{-(w^\\top x + b)}}.\n",
    "$$\n",
    "Thus the probability is the sigmoid of a linear score.\n",
    "\n",
    "3) As the CDF of the logistic distribution\n",
    "The standard logistic CDF is\n",
    "$$\n",
    "F(x) = \\frac{1}{1 + e^{-x}} = \\sigma(x),\n",
    "$$\n",
    "with PDF\n",
    "$$\n",
    "f(x) = F'(x) = \\sigma(x)\\bigl(1-\\sigma(x)\\bigr).\n",
    "$$\n",
    "More generally, with scale s > 0 and location μ:\n",
    "$$\n",
    "F(x) = \\frac{1}{1 + e^{-(x-\\mu)/s}}.\n",
    "$$\n",
    "\n",
    "Key identities and properties\n",
    "- Range and symmetry:\n",
    "$$\n",
    "0 < \\sigma(x) < 1, \\quad \\sigma(0) = \\tfrac12, \\quad \\sigma(-x) = 1 - \\sigma(x).\n",
    "$$\n",
    "- Derivative:\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x)\\bigl(1 - \\sigma(x)\\bigr).\n",
    "$$\n",
    "- Inverse (logit):\n",
    "$$\n",
    "\\text{logit}(p) = \\log\\frac{p}{1-p}, \\quad \\sigma(\\text{logit}(p)) = p.\n",
    "$$\n",
    "- Relation to tanh:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{2}\\bigl(1 + \\tanh(\\tfrac{x}{2})\\bigr).\n",
    "$$\n",
    "- Small-x approximation:\n",
    "$$\n",
    "\\sigma(x) \\approx \\tfrac12 + \\tfrac{x}{4} \\quad \\text{for } |x|\\ll 1.\n",
    "$$\n",
    "\n",
    "Would you like quick practice problems (e.g., solving the ODE with different initial conditions, or deriving p from given log-odds)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of “soft-max” as just a mathematical **function**;  \n",
    "“soft-max regression” (multinomial logistic regression) is a **statistical model / learning algorithm** that *uses* that function as its last step.\n",
    "\n",
    "------------------------------------------------\n",
    "1. Soft-max function (nothing but a mapping)  \n",
    "\n",
    "Given any real-valued vector  \n",
    "$$\\mathbf{z}=(z_1,\\dots , z_K)\\in\\mathbb R^{K},$$  \n",
    "soft-max produces a probability vector  \n",
    "\n",
    "$$\n",
    "\\text{softmax}(\\mathbf{z})_k\n",
    "    \\;=\\;\n",
    "    \\frac{e^{z_k}}{\\sum_{j=1}^{K}e^{z_j}},\\qquad k=1,\\dots ,K .\n",
    "$$\n",
    "\n",
    "Properties  \n",
    "• Each component is in $(0,1)$.  \n",
    "• They sum to $1$.  \n",
    "• No training, no data, no parameters—just evaluate the formula.\n",
    "\n",
    "------------------------------------------------\n",
    "2. Soft-max regression (a learning model)  \n",
    "\n",
    "Goal: classify an input feature vector $\\mathbf x\\!\\in\\!\\mathbb R^{d}$ into one of $K$ classes.\n",
    "\n",
    "Step-by-step model  \n",
    "a) Affine scores (“logits”):\n",
    "\n",
    "$$\n",
    "\\mathbf z = W\\mathbf x + \\mathbf b,\n",
    "$$\n",
    "\n",
    "with parameters  \n",
    "$W\\in\\mathbb R^{K\\times d}$  (weights) and  \n",
    "$\\mathbf b\\in\\mathbb R^{K}$    (biases).\n",
    "\n",
    "b) Convert those scores into class probabilities with the **soft-max function**:\n",
    "\n",
    "$$\n",
    "P(y=k\\mid\\mathbf x;\\,W,\\mathbf b)\n",
    "  =\\frac{e^{z_k}}{\\sum_{j=1}^{K}e^{z_j}}\n",
    "  =\\text{softmax}(\\mathbf z)_k .\n",
    "$$\n",
    "\n",
    "c) Fit $(W,\\mathbf b)$ to data by maximizing the multinomial likelihood (or, equivalently, minimizing cross-entropy loss).\n",
    "\n",
    "So soft-max regression = “linear logits” + “soft-max function” + “parameter learning”.\n",
    "\n",
    "------------------------------------------------\n",
    "Analogy\n",
    "\n",
    "logistic function  ↔  logistic regression  \n",
    "soft-max function  ↔  soft-max (multinomial logistic) regression\n",
    "\n",
    "Function alone: deterministic mapping, 0 parameters.  \n",
    "Regression: statistical model that embeds the function inside it and supplies *trainable* parameters.\n",
    "\n",
    "------------------------------------------------\n",
    "Quick comparison table\n",
    "\n",
    "• Soft-max  \n",
    "  – Input: any $K$-vector of real numbers.  \n",
    "  – Output: $K$ probabilities.  \n",
    "  – Role: normalization / squashing.\n",
    "\n",
    "• Soft-max regression  \n",
    "  – Input: feature vector $\\mathbf x$.  \n",
    "  – Output: predicted class probabilities.  \n",
    "  – Contains: weight matrix $W$, bias vector $\\mathbf b$, loss, an optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[+251.235s] RT          : INFO     - START CREATED query Node\n",
      "[+251.609s] RT          : INFO     - query Node DONE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  Color of pear?\n",
      "choice [0]:  Pear is light yellow\n",
      "choice [1]:  Apple is deep red\n"
     ]
    }
   ],
   "source": [
    "rag_node:rt.Node = get_rag_node(\n",
    "    documents=docs,\n",
    ")\n",
    "\n",
    "query = \"Color of pear?\"\n",
    "# You should not use _sync in notebook, as it is natively using event loop\n",
    "with rt.Runner() as runner:\n",
    "    result = await runner.call(rag_node, query)\n",
    "print(\"Query: \", query)\n",
    "print(\"choice [0]: \", result[0].record.text)\n",
    "print(\"choice [1]: \", result[1].record.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
